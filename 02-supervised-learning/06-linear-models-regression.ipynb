{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: center, middle\n",
    "\n",
    "#### W4995 Applied Machine Learning\n",
    "\n",
    "## Linear models for Regression\n",
    "\n",
    "02/10/20\n",
    "\n",
    "Andreas C. Müller\n",
    "\n",
    "\n",
    "\n",
    "So today we'll talk about linear models for regression.\n",
    "You're probably familiar with some of the basics, and that's\n",
    "where we'll start. But we'll also look into how to tune\n",
    "these models, what the trade-offs are, and some more complex\n",
    "techniques.\n",
    "But first we'll start with some more preprocessing that we didn't get\n",
    "to last week, in particular dealing with missing values, which you'll need for the homework.\n",
    "\n",
    "\n",
    "FIXME show how to get feature names out of complex pipelines\n",
    "FIXME remove isocontour slides\n",
    "FIXME sync with workshop\n",
    "FIXME imputation for categorical data\n",
    "FIXME notebook needs sorting\n",
    "FIXME better motivation for log\n",
    "FIXME add missing value indicator\n",
    "FIXME better explanations for imputation, in particular iterative! too short, this fixes it\n",
    "FIXME L1 explanation unclear\n",
    "FIXME adding features: use pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: center, middle\n",
    "## Sidebar: Dealing with Missing Values\n",
    "\n",
    "\n",
    "This is for the homework. I didn't realize there were missing values in the continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: spacious\n",
    "\n",
    "## Dealing with missing values\n",
    "\n",
    "- Missing values can be encoded in many ways\n",
    "- Numpy has no standard format for it (often np.NaN) - pandas does\n",
    "- Sometimes: 999, \n",
    ", ?, np.inf, “N/A”, “Unknown“ …\n",
    "- Not discussing “missing output” - that’s semi-supervised learning.\n",
    "- Often missingness is informative (Use `MissingIndicator`)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So first we're going to talk about imputation, which means\n",
    "basically dealing with missing values. Before that, we're\n",
    "going to talk about different methods to deal with missing\n",
    "values. The first thing about missing values is that you\n",
    "need to figure out whether your dataset has them and how\n",
    "they encode it. If you're on Python, numpy has no standard\n",
    "way to represent missing values while Pandas does and it's\n",
    "usually nan. But the problem is usually more in the data\n",
    "source. So depending on where your data comes from, missing\n",
    "values might be encoded as anything. They might be\n",
    "differently encoded for different parts of the data set. So\n",
    "if you see some question marks somewhere, it doesn't mean\n",
    "that all missing values are encoded as question marks. There\n",
    "might be different reasons why data is missing. If you look\n",
    "into like theoretical analysis missingness, often there you\n",
    "can see something that's called missing at random or missing\n",
    "completely at random, meaning that data was retracted\n",
    "randomly from the dataset. That's not usually what happens.\n",
    "Usually, if the data is missing, it's missing because\n",
    "something when differently in a process, like someone didn't\n",
    "fill out the form correctly, or someone didn't reply in a\n",
    "survey. And very often the fact that someone didn't reply or\n",
    "that something was not measured is actually informative.\n",
    "Whenever you have missing values, it's often a good idea to\n",
    "keep the information about whether the value was missing or\n",
    "not. If a feature was missing, while we're going to do\n",
    "imputation and we're going to try to fill in the missing\n",
    "values. It's often really useful information that's\n",
    "something was missing and you should record the fact and\n",
    "represent it in the dataset somehow. We're only going to\n",
    "talk about missing input today. You can also have missing\n",
    "values in the outputs that are usually called\n",
    "semi-supervised learning where you have the true target or\n",
    "the true class only for some data points, but not for all of\n",
    "them. So there's the first method which is very obvious.\n",
    "Let's say your data looks like this. All my illustrations\n",
    "today we'll be adding randomness to the iris data set.\n",
    "\n",
    "\n",
    "FIXME: Better digram for feature selection. What are the\n",
    "hypothesises for the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 80%](images/row_nan_col_nan.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "So if my dataset looks like something on the left-hand side\n",
    "here, then you can see that there are only missing values in\n",
    "the first feature and it's mostly missing. One of the ways\n",
    "to deal with this is just completely dropped the first\n",
    "feature, and that might be a reasonable thing to do. If\n",
    "there are so few values here that you don't think there's\n",
    "any information in this just drop it I always like to\n",
    "compare everything to a baseline approach. So your baseline\n",
    "approach should be if there's only missing values in some of\n",
    "the columns, just drop these columns, see what happens. You\n",
    "can always iterate and improve on that but that should be\n",
    "the baseline. A little bit trickier situation is that there\n",
    "might be some missing values only for a few rows, the rows\n",
    "are data points. You can kick out these data points and\n",
    "train the model on the rest and that might be fine. There's\n",
    "a bit of a problem with this though, that if you want to\n",
    "make predictions on new data and the data that arrives has\n",
    "missing values you'll not be able to make predictions\n",
    "because you don't have a way to deal with missing values. If\n",
    "you're guaranteed that any new test point that comes in will\n",
    "not have missing values then doing this might make sense.\n",
    "Another issue with dropping the rows with missing values is\n",
    "that if this was related to the actual outcome then it might\n",
    "be that you biased how well you think you're doing. Maybe\n",
    "all the hard data points are the ones that have missing\n",
    "values. And so by excluding them from your training data,\n",
    "you're also excluding them from the validation. So it means\n",
    "you think you're doing very well because you discarded all\n",
    "the hard data points. Discarding data points is a little bit\n",
    "trickier and it depends on your situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 100%](images/imputation-schema.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "The other solution is to impute them. So the idea is that\n",
    "you have your training data set, you build some model of the\n",
    "training data set, and then you fill in the missing values\n",
    "using the information from the other rows and columns. And\n",
    "you built a model for this and then you can also apply the\n",
    "same imputation model on the test data set if you want to\n",
    "make predictions. Question is what if it has all missing\n",
    "values? Then you have no choice but drop that. If in the\n",
    "dataset that happens, you need to figure out what you are\n",
    "going to do. Like, if you have a production system, and\n",
    "something comes in with all missing values, you need to\n",
    "decide what you're going to do. But you probably cannot use\n",
    "this data point for training model. You could like use the\n",
    "outputs and train to find the mean outcome of all the values\n",
    "that are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: spacious\n",
    "\n",
    "## Imputation Methods\n",
    "\n",
    "- Mean / Median\n",
    "- kNN\n",
    "- Regression models\n",
    "- Matrix factorization (not in this lecture)\n",
    "\n",
    "\n",
    "\n",
    "So let's talk about the general methods for data imputation.\n",
    "So these are the ones that we are going to talk through. The\n",
    "easiest one is me doing a constant value per column.\n",
    "Imputing the mean or the medium of the column that we're\n",
    "trying to compute. kNN means taking the average of KNearest\n",
    "Neighbors. Regression means I build a regression model from\n",
    "some of the features trying to rip the missing future. And\n",
    "finally, elaborate probabilistic models. They try to build a\n",
    "probabilistic model of the dataset and complete the missing\n",
    "values based on this probabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Dropping Columns\n",
    "\n",
    ".smaller[\n",
    "```python\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, stratify=y)\n",
    "\n",
    "nan_columns = np.any(np.isnan(X_train), axis=0)\n",
    "X_drop_columns = X_train[:, ~nan_columns]\n",
    "scores = cross_val_score(LogisticRegressionCV(v=5), X_drop_columns, y_train, cv=10)\n",
    "np.mean(scores)\n",
    "```\n",
    "0.772\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Here’s my baseline, which is dropping the columns. I used\n",
    "iris dataset where I had to put some missing values into the\n",
    "second and third column. Here, x_score has like some missing\n",
    "values, I split into test and training test data set and\n",
    "then I drop all columns that have missing values and then I\n",
    "can train logistic regression, which is sort of the model\n",
    "I'm going to use to tell me how well does this imputation\n",
    "work for classification. For my baseline, using a logistic\n",
    "regression model and 10 fold baseline, I get 77.2% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and Median\n",
    "\n",
    ".center[\n",
    "![:scale 100%](images/imputation-median-schema.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "The simplest imputation is mean or medium. Unfortunately,\n",
    "only one is implemented in scikit-learn right now. If this\n",
    "is our dataset, the imputation will look like this. For a\n",
    "column, each missing value is replaced by the mean of the\n",
    "column. The imputer is the only transformer in scikit-learn\n",
    "that can handle missing data. Using the imputer you can\n",
    "specify the strategy, mean or median or constant and then\n",
    "you can call the method transform and that imputes missing\n",
    "values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 90%](images/median_imputation.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Here is a graphical illustration of what does this. So the\n",
    "iris dataset is four-dimensional, and I'm plotting it in two\n",
    "dimensions in which I added missing values. So, there are\n",
    "two other dimensions which had no missing values, which you\n",
    "can't see. The original data set is basically blue points\n",
    "here, orange points here, green points here, and it's\n",
    "relatively easy to separate. But here you can see that the\n",
    "green points they have a lot of missingness, so they were\n",
    "replaced by the mean of the whole dataset.\n",
    "\n",
    "So there are two things about this. One, I kind of lost the\n",
    "class information a lot. Two, the data is now in places\n",
    "where there was no data before, which is not so great. You\n",
    "could do smart things like doing the mean per class, but\n",
    "that's actually not something that I've seen a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScalar\n",
    "\n",
    "nan_columns = np.any(np.isnan(X_train), axis = 0)\n",
    "X_drop_columns = X_train[:,~nan_columns]\n",
    "logreg = make_pipeline(StandardScalar(), LogisticRegression())\n",
    "scores = cross_val_score(logreg, X_drop_columns, y_train, cv=10)\n",
    "np.mean(scores)\n",
    "```\n",
    "0.794\n",
    "```python\n",
    "mean_pipe = make_pipeline(SimpleImputer(strategy='median'),\n",
    "                          StandardScalar(),\n",
    "                          LogisticRegression())\n",
    "scores = cross_val_score(mean_pipe, X_train, y_train, cv=10)\n",
    "np.mean(scores)\n",
    "```\n",
    "0.729\n",
    "\n",
    "\n",
    "\n",
    "Here's the comparison of dropping the columns versus doing\n",
    "the mean amputation. We actually see that the mean\n",
    "imputation is worse. In general, if you have very few\n",
    "missing values, it might actually not be so bad and putting\n",
    "in the mean might work.\n",
    "Here I basically designed the dataset so that mean imputation fails.\n",
    "This is not very realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: spacious\n",
    "\n",
    "## KNN Imputation\n",
    "\n",
    "- Find k nearest neighbors that have non-missing values.\n",
    "- Fill in all missing values using the average of the\n",
    "neighbors.\n",
    "\n",
    "\n",
    "\n",
    "FIXME illustration\n",
    "\n",
    "In terms of complexity is using Nearest Neighbors. So the\n",
    "idea is for each data point, we find the kNearest neighbors\n",
    "and then fill in the missing values as the average of the\n",
    "features of these neighbors. The difficulty here is\n",
    "measuring distances if there are missing values. If there\n",
    "are missing values, I can't just compute Euclidean\n",
    "distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".smaller[\n",
    "```python\n",
    "\n",
    "knn_pipe = make_pipeline(KNNImputer(), StandardScaler(), LogisticRegression())\n",
    "np.mean(scores)\n",
    "```\n",
    "```\n",
    "0.849\n",
    "```\n",
    "]\n",
    "\n",
    ".center[\n",
    "![:scale 60%](images/knn_imputation.png)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Driven Imputation\n",
    "\n",
    "- Train regression model for missing values\n",
    "- Iterate: retrain after filling in\n",
    "- IterativeImputer in next sklearn release\n",
    "\n",
    ".smaller[\n",
    "```python\n",
    "rf_imp = IterativeImputer(predictor=RandomForestRegressor())\n",
    "rf_pipe = make_pipeline(rf_imp, StandardScaler(), LogisticRegression())\n",
    "\n",
    "scores = cross_val_score(rf_pipe, X_rf_imp, y_train, cv=10)\n",
    "np.mean(scores)\n",
    "```\n",
    "```\n",
    "0.845\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "The next step up in complexity is using an arbitrary\n",
    "regression model for imputation. I mean, arguably, the kNN\n",
    "is also a regression model, but there's like some\n",
    "intricacies that make it a little bit different. So the idea\n",
    "with using a regression model is you do the first pass and\n",
    "impute data using the mean. And then you try to predict the\n",
    "missing features using a regression model trained on the\n",
    "non-missing features and then you iterate this until stuff\n",
    "doesn't change anymore. You can use any model you like, and\n",
    "this is very flexible, and it can be fast if you have a fast\n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: spacious\n",
    "\n",
    "## Comparision of Imputation Methods\n",
    "\n",
    ".center[\n",
    "![:scale 100%](images/med_knn_rf_comparison.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "FIXME better illustration/ graph\n",
    "Here's a comparison on the iris dataset again.\n",
    "This is a very artificial example and is only meant to illustrate the ideas.\n",
    "In practice, using the mean or median is actually quite decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: center, middle\n",
    "\n",
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class:center\n",
    "\n",
    "## Linear Models for Regression\n",
    "\n",
    "![:scale 50%](images/linear_regression_1d.png)\n",
    "\n",
    "$$\\hat{y} = w^T \\mathbf{x} + b = \\sum_{i=1}^p w_i x_i +b$$\n",
    "\n",
    "\n",
    "\n",
    "Predictions in all linear models for regression are of the\n",
    "form shown here: It's an inner product of the features with\n",
    "some coefficient or weight vector w, and some bias or\n",
    "intercept b. In other words, the output is a weighted sum of\n",
    "the inputs, possibly with a shift. here i runs over the\n",
    "features and x_i is one feature of the data point x. These\n",
    "models are called linear models because they are linear in\n",
    "the parameters w. The way I wrote it down here they are also\n",
    "linear in the features x_i. However, you can replace the\n",
    "features by any non-linear function of the inputs, and it'll\n",
    "still be a linear model.\n",
    "\n",
    "There are many differnt linear models for regression, and\n",
    "they all share this formula for making predictions. The\n",
    "difference between them is in how they find w and b based on\n",
    "the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "\n",
    "$$\\hat{y} = w^T \\mathbf{x} + b = \\sum_{i=1}^p w_i x_i +b $$\n",
    "\n",
    "`$$\\min_{w \\in \\mathbb{R}^p, b\\in\\mathbb{R}} \\sum_{i=1}^n (w^T\\mathbf{x}_i + b - y_i)^2$$`\n",
    "\n",
    "Unique solution if $\\mathbf{X} = (\\mathbf{x}_1, ... \\mathbf{x}_n)^T$ has full column rank.\n",
    "\n",
    "\n",
    "The most straight-forward solution that goes back to Gauss\n",
    "is ordinary least squares. In ordinary least squares, find w\n",
    "and b such that the predictions on the training set are as\n",
    "accurate as possible according the the squared error. That\n",
    "intuitively makes sense: we want the predictions to be good\n",
    "on the training set. If there is more samples than features\n",
    "(and the samples span the whole feature space), then there\n",
    "is a unique solution. The problem is what's called a least\n",
    "squares problem, which is particularly easy to optimize and\n",
    "get the unique solution to.\n",
    "\n",
    "However, if there are more features than samples, there are\n",
    "usually many perfect solutions that lead to 0 error on the\n",
    "training set. Then it's not clear which solution to pick.\n",
    "Even if there are more samples than features, if there are\n",
    "strong correlations among features the results might be\n",
    "unstable, and we'll see some examples of that soon.\n",
    "\n",
    "Before we look at examples, I want to introduce a popular\n",
    "alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "`$$ \\min_{w \\in \\mathbb{R}^p, b\\in\\mathbb{R}} \\sum_{i=1}^n (w^T\\mathbf{x}_i + b - y_i)^2 + \\alpha ||w||^2 $$`\n",
    "\n",
    "Always has a unique solution.\n",
    "\n",
    "Tuning parameter alpha.\n",
    "\n",
    "\n",
    "\n",
    "In Ridge regression we add another term to the optimization\n",
    "problem. Not only do we want to fit the training data well,\n",
    "we also want w to have a small squared l2 norm or squared\n",
    "euclidean norm. The idea here is that we're decreasing the\n",
    "\"slope\" along each of the feature by pushing the\n",
    "coefficients towards zero. This constraings the model to be\n",
    "more simple.\n",
    "\n",
    "So there are two terms in this optimization problem, which\n",
    "is also called the objective function of the model: the data\n",
    "fitting term here that wants to be close to the training\n",
    "data according to the squared norm, and the prenalty or\n",
    "regularization term here that wants w to have small norm,\n",
    "and that doesn't depend on the data.\n",
    "\n",
    "Usually these two goals are somewhat opposing. If we made w\n",
    "zero, the second term would be zero, but the predictions\n",
    "would be bad. So we need to trade off between these two. The\n",
    "trade off is problem specific and is specified by the user.\n",
    "If we set alpha to zero, we get linear regression, if we set\n",
    "alpha to infinity we get a constant model. Obviously usually\n",
    "we want something in between.\n",
    "\n",
    "This is a very typical example of a general principle in\n",
    "machine learning, called regularized empirical risk\n",
    "minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (regularized) Empirical Risk Minimization\n",
    "\n",
    "`$$ min_{f \\in F} \\sum_{i=1}^n L(f(\\mathbf{x}_i), y_i) + \\alpha R(f) $$`\n",
    "\n",
    "\n",
    "\n",
    "FIXME pointers data fitting / regularization!\n",
    "\n",
    "Many models in machine learning, like linear models, SVMs\n",
    "and neural networks follow the general framework of\n",
    "empirical risk minimization, which you can see here. We\n",
    "formulate the machine learning problem as an optimization\n",
    "problem over a family of functions. In our case that was the\n",
    "family of linear functions parametrized by w and b. The\n",
    "minimization problem consists of two parts, the data fitting\n",
    "part and the model complexity part. The data fitting part\n",
    "says that the predictions mad eby our functions should be\n",
    "accurate according to some loss L. For our regression\n",
    "problems that was the squared loss. The model complexity\n",
    "part says that we prefer simple models and penalizes\n",
    "complicated f. Most machine learning algorithms can be cast\n",
    "into this, with a particular choice of family of functions\n",
    "f, loss function L and regularizer R. And most of machine\n",
    "learning theory is build around this framework. People proof\n",
    "for differnt choices of F and L and R that if you minimize\n",
    "this, you'll be able to generalize well. And that makes\n",
    "intuitive sense. To do well on the test set, we definitely\n",
    "want to do reasonably well on the training set. We don't\n",
    "expect that we can do better on a test set than the training\n",
    "set. But we also want to minimize the performance difference\n",
    "between training and test set. If we restrict our model to\n",
    "be simple via the regularizer R, we have better chances of\n",
    "the model generalizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder on model complexity\n",
    "\n",
    "![:scale 80%](images/overfitting_underfitting_cartoon_full.png)\n",
    "\n",
    "\n",
    "\n",
    "I hope this sounds familiar from what we talked about last\n",
    "time. This is a particular way of dealing with overfitting\n",
    "and underfitting. For this framework in general, or for\n",
    "ridge regression in particular, trading off the data fitting\n",
    "and the regularization changes the model complexity. If we\n",
    "set alpha high we restrict the model, and we will be on the\n",
    "left side of the graph. If we make alpha small, we allow the\n",
    "model to fit the data more, and we're on the right side of\n",
    "the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ames Housing Dataset\n",
    "\n",
    "![:scale 80%](images/ames_housing_scatter.png)\n",
    ".tiny[\n",
    "```python\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "```\n",
    "```\n",
    "(2195, 79)\n",
    "(2195,)\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "Ok after all this pretty abstract talk, let's make this\n",
    "concrete. Let's do some regression on the boston housing\n",
    "dataset. After the last homework you're hopefully familiar\n",
    "with it. The idea is to predict prices of property in the\n",
    "boston area in different neighborhoods. This is a dataset\n",
    "from the 70s I think, so everything is pretty cheap. Most of\n",
    "the features you can see are continuous, with the exception\n",
    "of the charlston river variable which says whether the\n",
    "neighborhood is on the river.\n",
    "\n",
    "Keep in mind that this data lives in a 13 dimensional space\n",
    "and these univariate plots only look at 13 different\n",
    "projections of the data, and can't capture any of the\n",
    "interactions.\n",
    "\n",
    "But still we can see that the price clearly depends on some\n",
    "of these variables. It's also pretty clear that the\n",
    "dependency is non-linear for some of the variables. We'll\n",
    "still start with a linear model, because its a very simple\n",
    "class of models, and I'd always star approaching any model\n",
    "from the simplest baseline. In this case it's linear\n",
    "regression. We're having 506 samples and 13 features. We\n",
    "have much more samples than features. Linear regression\n",
    "should work just fine. Also it's a tiny dataset, so\n",
    "basically anything we'll try will run instantaneously, which\n",
    "is also good to keep in mind.\n",
    "\n",
    "Another thing that you can see in this graph is that the\n",
    "features have very different scales. Here's a box plot that\n",
    "shows that even more clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 100%](images/ames_scaling.png)\n",
    "\n",
    "\n",
    "\n",
    "That's something that will trip up the distance based models\n",
    "models we talked about last time, as well as the linear\n",
    "models we're talking about today.  For the penalized models\n",
    "the different scales mean that different features are\n",
    "penalized differently, which you usually want to avoid.\n",
    "Usually there is no particular semantics attached to the\n",
    "fact that one feature has different magnitutes than another.\n",
    "We could measure something in inches instead of miles, and\n",
    "that would change the outcome of the model. That's certainly\n",
    "not something we want. A good idea is to scale the data to\n",
    "get rid of this effect. We'll talk about that and other\n",
    "preprocessing methods in-depth on Wednesday next week. Today\n",
    "I'm mostly gonna ignore this. But let's get started with\n",
    "Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient of determination R^2\n",
    "\n",
    "`$$ R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n - 1} (y_i - \\hat{y}_i)^2}{\\sum_{i=0}^{n - 1} (y_i - \\bar{y})^2} $$`\n",
    "\n",
    "`$$ \\bar{y} =  \\frac{1}{n} \\sum_{i=0}^{n - 1} y_i$$`\n",
    "\n",
    "Can be negative for biased estimators - or the test set!\n",
    "\n",
    "\n",
    "\n",
    "The scores are R squared or coefficient of determination.\n",
    "This is basically a score that's usually between zero and\n",
    "one, where one means perfect prediction or perfect\n",
    "correlation and zero means a random prediction. What it does\n",
    "is it computes the mean of the targets over the data you're\n",
    "evaluating it on and then it looks at the distance between\n",
    "the prediction and the ground truth relative to the mean. If\n",
    "it's negative, it means you do a worse job at predicting and\n",
    "just predicting the mean. It can happen if your model was\n",
    "really bad and bias. The other reason is if you use a test\n",
    "set. This is guaranteed to be positive on the data it was\n",
    "fit on with an unbiased linear model, which will nearly\n",
    "never apply to what we’re doing.\n",
    "\n",
    "The R^2 can be misleading if there's outliers in the training\n",
    "data and some consider it a bad metric. Max Kuhn, author of APM\n",
    "thinks it's a bad metric. It's not clear to me that MSE is much\n",
    "better in general, though. Reducing anything to a single number\n",
    "is tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: smaller\n",
    "## Preprocessing\n",
    "```python\n",
    "cat_preprocessing = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='NA'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "cont_preprocessing = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    StandardScaler())\n",
    "\n",
    "preprocess = make_column_transformer(\n",
    "    (cat_preprocessing, make_column_selector(dtype_include='object')),\n",
    "    remainder=cont_preprocessing)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)\n",
    "cross_val_score(\n",
    "    make_pipeline(preprocess, LinearRegression()),\n",
    "    X_train, y_train, cv=5)\n",
    "```\n",
    "```\n",
    "array([0.928, 0.927, 0.932, 0.898, 0.884])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: smaller\n",
    "## Skewed targets\n",
    ".left-column[\n",
    "```python\n",
    "y_train.hist(bins='auto')\n",
    "```\n",
    "![:scale 100%](images/ames_housing_price_hist.png)\n",
    "\n",
    "]\n",
    ".right-column[\n",
    "```python\n",
    "np.log(y_train).hist(bins='auto')\n",
    "```\n",
    "![:scale 100%](images/ames_housing_price_hist_log.png)\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: middle, spacious\n",
    "```python\n",
    "cross_val_score(make_pipeline(preprocess, LinearRegression()),\n",
    "                X_train, y_train, cv=5)\n",
    "```\n",
    "```\n",
    "array([0.928, 0.927, 0.932, 0.898, 0.884])\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "log_regressor = TransformedTargetRegressor(\n",
    "    LinearRegression(), func=np.log, inverse_func=np.exp)\n",
    "cross_val_score(make_pipeline(preprocess, log_regressor),\n",
    "                X_train, y_train, cv=5)\n",
    "```\n",
    "```\n",
    "array([0.95 , 0.943, 0.941, 0.913, 0.922])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression vs Ridge\n",
    "```python\n",
    "log_regressor = TransformedTargetRegressor(\n",
    "    LinearRegression(), func=np.log, inverse_func=np.exp)\n",
    "cross_val_score(make_pipeline(preprocess, log_regressor),\n",
    "                X_train, y_train, cv=5)\n",
    "\n",
    "```\n",
    "```\n",
    "array([0.95 , 0.943, 0.941, 0.913, 0.922])\n",
    "```\n",
    "```python\n",
    "log_ridge = TransformedTargetRegressor(\n",
    "    Ridge(), func=np.log, inverse_func=np.exp)\n",
    "cross_val_score(make_pipeline(preprocess, log_ridge),\n",
    "                X_train, y_train, cv=5)\n",
    "\n",
    "```\n",
    "```\n",
    "array([0.948, 0.95 , 0.941, 0.915, 0.931])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Let’s look at two simple models. Linear regression and Ridge\n",
    "regression. What I've done is I’ve split the data into\n",
    "training and test set and used 10 fold cross-validation to\n",
    "evaluate them. Here I use cross_val_score together with the\n",
    "model, the training data, training labels, and 10 fold\n",
    "cross-validation. This will return 10 scores and I'm going\n",
    "to compute the mean of them. I'm doing this for both linear\n",
    "regression and Ridge regression. Here is ridge regression\n",
    "uses a default value of alpha of 1. Here these two scores\n",
    "are quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".smallest[\n",
    "```python\n",
    "pipe = Pipeline([('preprocessing', preprocess), ('ridge', log_ridge)])\n",
    "param_grid = {'ridge__regressor__alpha': np.logspace(-3, 3, 13)}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=RepeatedKFold(10, 5),\n",
    "                    return_train_score=True)\n",
    "```\n",
    "```\n",
    "{'ridge__regressor__alpha': array([ 0.001,  0.003, 0.01, 0.032, 0.1, 0.316, 1., 3.162,\n",
    "           10., 31.623, 100., 316.228, 1000.])}\n",
    "```\n",
    "\n",
    "```python\n",
    "grid.fit(X_train, y_train)\n",
    "```\n",
    "]\n",
    ".center[\n",
    "![:scale 50%](images/ridge_alpha_search.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Coming back to the ridge regression we used the standard\n",
    "alpha of one which is a reasonable default, but by no means,\n",
    "this is guaranteed to make sense in this particular problem.\n",
    "Here I’ve done the grid search. As we talked about on\n",
    "Monday, I defined a parameter grid where the key is the\n",
    "parameter I want to search (alpha in Ridge) and the\n",
    "parameters I want to try. For regularization parameters,\n",
    "like alpha, it’s usually good to do them on the logarithmic\n",
    "grid. I do a relatively fine grid here with 13 different\n",
    "points mostly because I wanted to have a nice plot. In\n",
    "reality, I would use a three or six or something like that.\n",
    "I’ve instantiated GridSearchCV with Ridge(), the parameter\n",
    "grid and do 10 fold cross-validation and then I called\n",
    "grid.fit. I’ve reported the mean training accuracy and mean\n",
    "test accuracy over 10 cross-validation folds for each of the\n",
    "parameter settings. Okay, you can see a couple things here.\n",
    "A) There's a lot of uncertainty B) The training set is\n",
    "always better than the test set. C) The most important thing\n",
    "that you can see here is that regularization didn't help.\n",
    "Making alpha as small as possible is the best. What I'm\n",
    "going to do next is I'm going to modify this dataset a\n",
    "little bit so that we can see the effect of the\n",
    "regularization. I’m going to modifying by using a polynomial\n",
    "expansion, again we're going to talk about a little bit more\n",
    "on Wednesday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".padding-top[\n",
    ".left-column[\n",
    "![:scale 100%](images/ridge_alpha_search.png)\n",
    "]\n",
    ".right-column[\n",
    "![:scale 100%](images/ridge_alpha_search_cv_runs.png)\n",
    "]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triazine Dataset\n",
    "\n",
    "```python\n",
    "triazines = fetch_openml('triazines')\n",
    "triazines.data.shape\n",
    "```\n",
    "```\n",
    "(186, 60)\n",
    "```\n",
    "```python\n",
    "pd.Series(triazines.target).hist()\n",
    "```\n",
    ".center[\n",
    "![:scale 40%](images/triazine_bar.png)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(triazines.data, triazines.target, random_state=0)\n",
    "\n",
    "cross_val_score(LinearRegression(), X_train, y_train, cv=5)\n",
    "```\n",
    "```\n",
    "array([-4.749e+24, -9.224e+24, -7.317e+23, -2.318e+23, -2.733e+22])\n",
    "```\n",
    "```python\n",
    "cross_val_score(Ridge(), X_train, y_train, cv=5)\n",
    "```\n",
    "```\n",
    "array([0.263, 0.455, 0.024, 0.23 , 0.036])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "param_grid = {'alpha': np.logspace(-3, 3, 13)}\n",
    "\n",
    "grid = GridSearchCV(Ridge(), param_grid, cv=RepeatedKFold(10, 5),\n",
    "                    return_train_score=True)\n",
    "grid.fit(X_train, y_train)\n",
    "```\n",
    ".center[\n",
    "![:scale 40%](images/ridge_alpha_triazine.png)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting coefficient values (LR)\n",
    "\n",
    "\n",
    "```python\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "plt.scatter(range(X_train.shape[1]), lr.coef_,\n",
    "            c=np.sign(lr.coef_), cmap='bwr_r')\n",
    "\n",
    "```\n",
    ".center[\n",
    "![:scale 55%](images/lr_coefficients_large.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "In the previous slide, linear regression did nearly as well\n",
    "as the Ridge Regression with the default parameters but\n",
    "let's look at the coefficients of a linear model. These are\n",
    "the coefficients of the linear model trained on the\n",
    "polynomial futures. I plot them adding a color to represent\n",
    "positive and negative. The magnitude here is 1 and then 13\n",
    "zeros. We have two features, one is like, probably more than\n",
    "trillions times two and then there's another one that's very\n",
    "negative. What I conclude from this is, these 2 features are\n",
    "very highly correlated. The model makes both of them really,\n",
    "really big and then they cancel each other out. This is not\n",
    "a very nice model, because it relates to American stability\n",
    "and also it tells me that these 2 features are like\n",
    "extremely important, but they might not be important at all.\n",
    "Like the other features might be more important, but they're\n",
    "nullified by this cancellation effect. They (0.2 and -0.8)\n",
    "need to cancel each other out because the predictions are\n",
    "reasonable and all the houses only cost like $70,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Coefficients\n",
    "\n",
    "```python\n",
    "ridge = grid.best_estimator_\n",
    "plt.scatter(range(X_train.shape[1]), ridge.coef_,\n",
    "            c=np.sign(ridge.coef_), cmap=\"bwr_r\")\n",
    "```\n",
    ".center[\n",
    "![:scale 55%](images/ridge_coefficients.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Let's look at the Ridge model. This is the best estimator,\n",
    "which is the model that was found in a grid search with the\n",
    "best parameter settings. This looks much more reasonable.\n",
    "This feature, which was a very negative one, still is very\n",
    "negative. But now this is actually three and minus three. So\n",
    "this is a much more reasonable range. We can also look at\n",
    "the features and the effect of different values of alpha.\n",
    "Here is a Ridge with 3 different values of alpha. In the\n",
    "previous random seat, alpha equal to 14 was the best now and\n",
    "now we have it equal to 30 something. The green one is more\n",
    "or less the best setting and then there's a smaller and a\n",
    "bigger one. You can see that basically what alpha does is,\n",
    "on average, it pushes all the coefficients toward zero. So\n",
    "here you can see this coefficient shrank going from 1 to 14\n",
    "going to 0 and the same here. So basically, they all push\n",
    "the different features towards 0. If you look at this long\n",
    "enough, you can see things that are interesting, the first\n",
    "one with alpha equal to one it's positive, and with alpha\n",
    "equal to 100, it's negative. That means depending on how\n",
    "much you regularize the direction of effect goes in opposite\n",
    "directions, what that tells me is don't interpret your\n",
    "models too much because clearly, either it has a positive or\n",
    "negative effect, it can't have both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ridge100 = Ridge(alpha=100).fit(X_train, y_train)\n",
    "ridge1 = Ridge(alpha=.1).fit(X_train, y_train)\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(ridge1.coef_, 'o', label=\"alpha=.1\")\n",
    "plt.plot(ridge.coef_, 'o', label=f\"alpha={ridge.alpha:.2f}\")\n",
    "plt.plot(ridge100.coef_, 'o', label=\"alpha=100\")\n",
    "plt.legend()\n",
    "```\n",
    "\n",
    ".center[\n",
    "![:scale 60%](images/ridge_coefficients_alpha.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "One other way to visualize the coefficient is to look at the\n",
    "coefficient path or regularization path. On the x-axis is\n",
    "the alpha, and on the y-axis, the coefficient magnitude.\n",
    "Basically, I looped over all of the different alphas and you\n",
    "can see how they shrink towards zero to increase alpha.\n",
    "There are some very big coefficients that go to zero very\n",
    "quickly and some coefficients here that stay the same for a\n",
    "long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "![:scale 90%](images/ridge_learning_curve.png)\n",
    "\n",
    "\n",
    "\n",
    "The thing I want to illustrate is if you have less data,\n",
    "regularization helps more. You can see the effects here\n",
    "between the orange and the red are very drastic. The green\n",
    "one probably has too much regularization. If you have very\n",
    "little data, even regularizing way too much is okay, but\n",
    "there's sort of a sweet spot here which is the orange one\n",
    "and the orange one does best. The more data you add, the\n",
    "less important regularization becomes. If you take the blue\n",
    "curve at the beginning it’s much higher than the red curve,\n",
    "but then, in the end, they overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "`$$ \\min_{w \\in \\mathbb{R}^p, b\\in\\mathbb{R}} \\sum_{i=1}^n (w^T\\mathbf{x}_i + b - y_i)^2 + \\alpha ||w||_1 $$`\n",
    "\n",
    "- Shrinks w towards zero like Ridge\n",
    "\n",
    "- Sets some w exactly to zero - automatic feature selection!\n",
    "\n",
    "\n",
    "\n",
    "Lasso Regression looks very similar to Ridge Regression. The\n",
    "only thing that is changed is we use the L1 norm instead of\n",
    "the L2 norm. L2 norm is the sum of squares, the L1 norm is\n",
    "the sum of the absolute values. So again, we are shrinking w\n",
    "towards 0, but we're shrinking it in a different way. The L2\n",
    "norm penalizes very large coefficients more, the L1 norm\n",
    "penalizes all coefficients equally. What this does in\n",
    "practice is its sets some entries of W to exactly 0. It does\n",
    "automatic feature selection if the coefficient of zero means\n",
    "it doesn't influence the prediction and so you can just drop\n",
    "it out of the model. This model does features selection\n",
    "together with prediction. Ideally what you would want is,\n",
    "let's say you want a model that does features selections.\n",
    "The goal is to make our model automatically select the\n",
    "features that are good. What you would want to penalize the\n",
    "number of features that it uses, that would be L0 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding L1 and L2 Penalties\n",
    ".wide-left-column[\n",
    "![:scale 100%](images/l2_l1_l0.png)\n",
    "]\n",
    "\n",
    ".narrow-right-column[\n",
    ".smaller[\n",
    "`$$ \\ell_2(w) = \\sqrt{\\sum_i w_i ^ 2}$$`\n",
    "`$$ \\ell_1(w) = \\sum_i |w_i|$$`\n",
    "`$$ \\ell_0(w) = \\sum_i 1_{w_i != 0}$$`\n",
    "]]\n",
    "\n",
    "\n",
    "The L0 norm penalizes the number of features that are not\n",
    "zero. The L1 norm penalizes the absolute values. L2 norm\n",
    "penalizes the squared norm. The problem with L0 norm is it's\n",
    "one everywhere except at zero. And that's not\n",
    "differentiable, it's not even continuous, so this is very\n",
    "hard to optimize. Basically, we just gave up on this and we\n",
    "use the next best thing we can do, which is the L1 norm. The\n",
    "L1 norm is not differentiable, there’s like this kink in\n",
    "zero, but we can still optimize it pretty well. The L2 norm\n",
    "penalizes large values much more than small values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding L1 and L2 Penalties\n",
    "\n",
    ".wide-left-column[\n",
    "![:scale 100%](images/l1_kink.png)\n",
    "]\n",
    ".narrow-right-column[\n",
    ".padding-top[\n",
    ".smaller[\n",
    "`$ f(x) = (2 x - 1)^2 $`\n",
    "\n",
    "`$ f(x) + L2 = (2 x - 1)^2 + \\alpha x^2$`\n",
    "\n",
    "`$ f(x) + L1= (2 x - 1)^2 + \\alpha |x|$`\n",
    "]]]\n",
    "\n",
    "\n",
    "Let's say I want to do a one-dimensional linear regression\n",
    "problem. The problem to solve is something of this form. I\n",
    "want my coefficients to be such that, let's say x, the one\n",
    "coefficient I'm looking for and this is sort of the squared\n",
    "loss function that I have. F is the daycare fitting term\n",
    "here, and this is would be like some quadratic. The loss\n",
    "function is in blue, its quadratic. The loss function plus\n",
    "our L2 regularization, the ridge is in orange. The data\n",
    "fitting plus L1 regularization is in green. The blue is a\n",
    "squared quadratic function, the orange is a quadratic\n",
    "function that sort of moved a little bit toward zero and the\n",
    "green one also moved towards zero but there's a kink here.\n",
    "Basically, this is just sort of the kink at the absolute\n",
    "value of zero and that makes it much more likely that the\n",
    "optimum will actually be at zero. So here the optimum for\n",
    "orange is somewhere here, so it was pushed towards zero but\n",
    "not exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: center\n",
    "## Understanding L1 and L2 Penalties\n",
    "\n",
    "![:scale 70%](images/l1l2ball.png)\n",
    "\n",
    "\n",
    "A different way to visualize this is in two dimensions.\n",
    "Let's say we have 2 coefficients. So one way to look at\n",
    "these norms, let’s say we want to minimize the norm but say\n",
    "we want to fix the norm, like more or less equivalent, these\n",
    "are the L1 and L2 balls. So this is all the 2D vectors that\n",
    "have Euclidean norm 1 or the ball and all the vectors that\n",
    "have L1 norm is the diamond. Let's say we restrict ourselves\n",
    "to a solution that lies on this ball."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: center\n",
    "## Understanding L1 and L2 Penalties\n",
    "\n",
    "![:scale 70%](images/l1l2ball_intersect.png)\n",
    "\n",
    "\n",
    "And then we have a quadratic function like this, this is\n",
    "sort of the data fitting term again and so now if we want a\n",
    "solution with L2 norm equal to zero, we get the solution\n",
    "here, the intersection of the height lines of the data\n",
    "fitting term and constraint to L2 norm. But for this\n",
    "diamond, we're going to hit very likely one of the corners\n",
    "just because of the geometry of the space, we're likely to\n",
    "either of these corners. The corner means one of the\n",
    "coefficients is exactly zero, and the other one is one. So\n",
    "this is another sort of geometric realization of trying to\n",
    "understand why does this lead to exact zeros hop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-Search for Lasso\n",
    "```python\n",
    "param_grid = {'alpha': np.logspace(-5, 0, 10)}\n",
    "grid = GridSearchCV(Lasso(normalize=True), param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "```\n",
    "```\n",
    "{'alpha': 0.0016}\n",
    "0.163\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now we can do Grid Search again, the default parameters\n",
    "usually don't work very well for Lasso. I use alpha on the\n",
    "logarithmic grid. I fitted and then I get the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 90%](images/lasso_alpha_triazine.png)\n",
    "\n",
    "\n",
    "\n",
    "Looking at the training test set performance, you can see\n",
    "that if you increase the regularization, this model gets\n",
    "really bad. The ridge regression didn't go this badly. If\n",
    "you set the realization to one, all coefficients become\n",
    "zero. Other than that there's reasonable performance, which\n",
    "is about as good as the ridge performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 60%](images/lasso_coefficients.png)\n",
    "]\n",
    "\n",
    "```python\n",
    "print(X_train.shape)\n",
    "np.sum(lasso.coef_ != 0)\n",
    "```\n",
    "```\n",
    "(139, 60)\n",
    "13\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "These are the coefficients of the model. Out of the 104\n",
    "features, it only selected 64 that are non-zero, the other\n",
    "ones are exactly zero. You can see this visualized here. The\n",
    "white ones are exactly zero and the other ones on non-zero.\n",
    "If I wanted I could prune the future space a lot and that\n",
    "makes the model possibly more interpretable. There's a\n",
    "slight caveat here if two of the features that are very\n",
    "correlated, Lasso will pick one of them at random and make\n",
    "the other one zero. Just because something's zero doesn't\n",
    "mean it's not important. It means you can drop it out of\n",
    "this model. If you have two features that are identical, one\n",
    "of them will be zero and one of them will be not zero and\n",
    "it's going to be randomly selected. That makes\n",
    "interpretation a little bit harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "- Combines benefits of Ridge and Lasso\n",
    "\n",
    "- two parameters to tune.\n",
    "\n",
    "\n",
    "`$$\\min_{w \\in \\mathbb{R}^p, b\\in\\mathbb{R}} \\sum_{i=1}^n ||w^T\\mathbf{x}_i + b - y_i||^2 + \\alpha_1 ||w||_1 +  \\alpha_2 ||w||^2_2 $$`\n",
    "\n",
    "\n",
    "\n",
    "You can also combine them. This actually what works best in\n",
    "practice. This is what's called the Elastic Net. Elastic Net\n",
    "tries to combine both of these penalizations together. You\n",
    "now have both terms, you have the L1 norm and the L2 norm\n",
    "and you have different values of alpha. Basically, this\n",
    "generalizes both. If you choose both these are alpha, it can\n",
    "become ridge and it can become Lasso, it can become any\n",
    "anything in between. Generally, ridge helps generalization.\n",
    "So it's a good idea to have the ridge penalty in there, but\n",
    "also maybe if there are some features that are really not\n",
    "useful, the L1 penalty helps makes the same exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: center\n",
    "## Comparing unit balls\n",
    "\n",
    "![:scale 70%](images/l1l2_elasticnet.png)\n",
    "\n",
    "\n",
    "\n",
    "Looking at the unit balls, if you zoom in here you would see\n",
    "the Elastic Net ball is kind of round, but also has a\n",
    "corner. Important things are the corners because that allows\n",
    "you to hit the exact zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrization in scikit-learn\n",
    "`$$\\min_{w \\in \\mathbb{R}^p, b\\in\\mathbb{R}} \\sum_{i=1}^n (w^T\\mathbf{x}_i + b - y_i)^2 + \\alpha \\eta ||w||_1 +  \\alpha (1 - \\eta) ||w||^2_2 $$`\n",
    "\n",
    "Where $\\eta$ is the relative amount of l1 penalty (`l1_ratio` in the code).\n",
    "\n",
    "\n",
    "The way this parameterize in scikit-learn is slightly\n",
    "different. In scikit-learn, you have a parameter alpha,\n",
    "which is the amount of regularization and then there's a\n",
    "parameter called l1_ratio, that says how much of the penalty\n",
    "should be L1 and L2. If you make this one, you have Lasso,\n",
    "if you make it zero, you have a Ridge. Don't use Lasso or\n",
    "Ridge and set alpha zero, because the solver will not handle\n",
    "it well. If you actually want alpha equal to zero, use\n",
    "linear regression. Now we have more parameters to tune, but\n",
    "we just have a more general model. This actually works\n",
    "pretty well often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-searching ElasticNet\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "param_grid = {'alpha': np.logspace(-4, -1, 10),\n",
    "              'l1_ratio': [0.01, .1, .5, .8, .9, .95, .98, 1]}\n",
    "\n",
    "grid = GridSearchCV(ElasticNet(), param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "```\n",
    "```\n",
    "{'alpha': 0.001, 'l1_ratio': 0.9}\n",
    "0.100\n",
    "```\n",
    "```python\n",
    "(grid.best_estimator_.coef_!= 0).sum()\n",
    "```\n",
    "```\n",
    "10\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Here is me doing a grid search. If you have two parameters\n",
    "for the grid search it will do all possible combinations.\n",
    "Here I do a logarithmic space for alpha and for the\n",
    "l1_ratio, I use something that’s very close to zero and\n",
    "something that's very close to one and some stuff in\n",
    "between. If you want to analyze the output of a 2D grid\n",
    "search a little bit harder we can’t do the nice curve\n",
    "anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: smaller\n",
    "## Analyzing grid-search results\n",
    "```python\n",
    "import pandas as pd\n",
    "res = pd.pivot_table(pd.DataFrame(grid.cv_results_),\n",
    "    values='mean_test_score', index='param_alpha', columns='param_l1_ratio')\n",
    "```\n",
    ".center[\n",
    "![:scale 60%](images/elasticnet_search.png)\n",
    "]\n",
    "\n",
    "\n",
    "The way that I like to do it is, here's the grip.cv results.\n",
    "And I put it in a data frame and then I'll make a pivot\n",
    "table where the values are test score, the index is one\n",
    "parameter and the columns are the other parameter. It allows\n",
    "me to visualize the grid search nicely. This is alpha and\n",
    "this is l1_ratio and you can see that if the l1_ratio is\n",
    "pretty high, there are some pretty good results, if you set\n",
    "the alpha accordingly. So here's like the diagonal of pretty\n",
    "good things. This is the model that did best. There's a\n",
    "slight caveat here that right now I did this with\n",
    "cross-validation and so this is the cross-validation\n",
    "accuracy. Last time I said, this is not really a good\n",
    "measure of generalization performance. So here, I searched\n",
    "way more parameters, I tried like 5 or 10 times as many\n",
    "models. So it's likely that by chance, I'll get better\n",
    "results. I didn't do this here in particular, because the\n",
    "data set is small and very noisy but in practice, if you\n",
    "want to compare models, you should evaluate it on a test set\n",
    "and see which of the models actually are better on the test.\n",
    "One more thing, why this is helpful is if the best value is\n",
    "on the edge of this graph that means my ranges were too\n",
    "small. Question is why we're using r square instead of the\n",
    "squared loss, one of the answers is that's the default in\n",
    "scikit-learn and the other answer is it's nice to know the\n",
    "range so you know that perfect prediction is one and you\n",
    "have some idea of what 0.5 means, the RMSE (the other norm\n",
    "that you usually use is the RMSE) depends on the scale of\n",
    "the output. So for example for the housing prices, it might\n",
    "be interesting to see what is the standard error in terms of\n",
    "dollars. If you want, like something that is in the units of\n",
    "the output, RMSE is good or mean absolute error might even\n",
    "be better. If you want something that is independent of the\n",
    "units of the output r square is pretty good because you know\n",
    "it's going to be between zero and one and it's measure\n",
    "something like the correlation and so if it's like 0.9, you\n",
    "know it’s a pretty good model. If my RMSE is 10,000 I don't\n",
    "know if have a good model or a bad model depends on what the\n",
    "range of the outputs is. The last thing I want to talk about\n",
    "today is this was basically changing the regularization\n",
    "parts. The two most times regularization we looked at is\n",
    "Ridge which is L2 penalty, Lasso which is an L1 penalty and\n",
    "combining two of them which is Elastic Net. So now I want to\n",
    "talk about changing the first part, which was the squared\n",
    "loss of the predictions, basically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: middle\n",
    "\n",
    "## Questions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "sklearn.set_config(print_changed_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    if i > 12:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    ax.plot(X[:, i], y, 'o', alpha=.5)\n",
    "    ax.set_title(\"{}: {}\".format(i, boston.feature_names[i]))\n",
    "    ax.set_ylabel(\"MEDV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(LinearRegression(),\n",
    "                        X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(\n",
    "        Ridge(), X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'alpha': np.logspace(-3, 3, 14)}\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(Ridge(), param_grid, cv=10, return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "plt.figure(dpi=200)\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.plot('param_alpha', 'mean_train_score', ax=plt.gca())\n",
    "results.plot('param_alpha', 'mean_test_score', ax=plt.gca())\n",
    "\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, scale\n",
    "# being lazy and not really doing things properly whoops\n",
    "X_poly = PolynomialFeatures(include_bias=False).fit_transform(scale(X))\n",
    "print(X_poly.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(LinearRegression(),\n",
    "                        X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(Ridge(),\n",
    "                        X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(Ridge(), param_grid, cv=10, return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "results.plot('param_alpha', 'mean_train_score', ax=plt.gca())\n",
    "results.plot('param_alpha', 'mean_test_score', ax=plt.gca())\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "plt.scatter(range(X_poly.shape[1]), lr.coef_, c=np.sign(lr.coef_), cmap=\"bwr_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = grid.best_estimator_\n",
    "plt.scatter(range(X_poly.shape[1]), ridge.coef_, c=np.sign(ridge.coef_), cmap=\"bwr_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge100 = Ridge(alpha=100).fit(X_train, y_train)\n",
    "ridge1 = Ridge(alpha=1).fit(X_train, y_train)\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(ridge1.coef_, 'o', label=\"alpha=1\")\n",
    "plt.plot(ridge.coef_, 'o', label=\"alpha=14\")\n",
    "plt.plot(ridge100.coef_, 'o', label=\"alpha=100\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used:\", np.sum(lasso.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Load the diabetes dataset using ``sklearn.datasets.load_diabetes``. Apply ``LinearRegression``, ``Ridge`` and ``Lasso`` and visualize the coefficients. Try polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/linear_models_diabetes.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
