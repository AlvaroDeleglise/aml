{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Kernel SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Motivation\n",
    "\n",
    "- Go from linear models to more powerful nonlinear ones.\n",
    "- Keep convexity (ease of optimization).\n",
    "- Generalize the concept of feature engineering.\n",
    "\n",
    "\n",
    "\n",
    "The main motivation for kernel support vector machines is\n",
    "that we want to go from linear models to nonlinear models\n",
    "but we also want to have the same simple kernel optimization\n",
    "to solve. So basically, the optimization problem we have to\n",
    "solve from a kernel SVM is about as hard as a linear SVM. So\n",
    "it's sort of very simple problem to solve. It’s much easier\n",
    "than learning in neural networks for example. But we get\n",
    "nonlinear decision boundaries. The idea behind this is to\n",
    "generalize the concept of feature engineering. We'll see in\n",
    "a little bit how, for example, kernels SVM with polynomial\n",
    "kernels relate to using polynomials explicitly in your\n",
    "feature engineering. Before we talk about kernels, I want to\n",
    "talk a little bit more about linear support vector machines,\n",
    "which we already discussed last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "\n",
    "## Reminder on Linear SVM\n",
    "\n",
    "`$$ \\min_{w \\in \\mathbb{R}^p, b \\in \\mathbf{R}} C \\sum_{i=1}^n \\max(0, 1 - y_i (w^T\\mathbf{x} +b)) + ||w||^2_2 $$`\n",
    "\n",
    "$$ \\hat{y} = \\text{sign}(w^T \\mathbf{x} + b)  $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The idea behind the linear support vector machine is it's a\n",
    "linear classifier, the minimization problem is up a hinge\n",
    "loss, which is basically linear in the decision function\n",
    "w^x. Basically, as long as you have a distance on the right\n",
    "side of the hyper plane, that’s bigger than one, your data\n",
    "point does not contribute to the loss. So you want all of\n",
    "the points outside of this margin of one around the\n",
    "separating hyper plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformulate Linear Models\n",
    "\n",
    ".smaller[\n",
    "- Optimization Theory\n",
    "\n",
    "$$ w = \\sum_{i=1}^n \\alpha_i \\mathbf{x}_i $$\n",
    "\n",
    "(alpha are dual coefficients. Non-zero for support vectors only)\n",
    "\n",
    "\n",
    "$$ \\hat{y} = \\text{sign}(w^T \\mathbf{x})  \\Longrightarrow   \\hat{y} = \\text{sign}\\left(\\sum_i^{n}\\alpha_i (\\mathbf{x}_i^T  \\mathbf{x})\\right) $$\n",
    "\n",
    "$$ \\alpha_i \n",
    "<\n",
    "= C$$\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Now I want to go from this linear model and extended to a\n",
    "nonlinear model. The idea here is that with some\n",
    "improvisation theory, you can find out that the W at the\n",
    "optimum can be expressed as a linear combination of the data\n",
    "points which is relatively straightforward to C. Expressing\n",
    "W as a linear combination, these alphas are called dual\n",
    "coefficients. Basically, you’re expressing the linear\n",
    "weights as a linear combination of the data points with this\n",
    "two coefficients alpha and you can show that these alpha are\n",
    "only non-zero for the points that contribute to this\n",
    "solution. So you can always write W as a linear combination\n",
    "of the support vectors with some alphas. If you want you can\n",
    "do this optimization problem either in trying to find W or\n",
    "you can rewrite the optimization problem and you can try to\n",
    "find these alphas. If you do this in terms of alphas, the\n",
    "decision function can be rewritten like this. Instead of\n",
    "looking at you w^T x, we just replace W transposed by the\n",
    "sum over all the training data points x_i and then basically\n",
    "we move the sum out of the inner products and we can see\n",
    "that it's the sum over all the alpha i in the inner product\n",
    "of the training data points  x_i was a test data point x.\n",
    "Optimization theory also tells us that if I find the alpha w\n",
    "to minimize this problem, then all the alpha i (s) will be\n",
    "smaller than c. This is another way to say what I said last\n",
    "time that basically c limits the influence of each data\n",
    "point. So, if you have a smaller c the alpha i belong to\n",
    "each training data point can only be as big as this c, and\n",
    "so each of the data points has only limited influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Introducing Kernels\n",
    "\n",
    "\n",
    "$$\\hat{y} = \\text{sign}\\left(\\sum_i^{n}\\alpha_i (\\mathbf{x}_i^T  \\mathbf{x})\\right) \\longrightarrow \\hat{y} = \\text{sign}\\left(\\sum_i^{n}\\alpha_i (\\phi(\\mathbf{x}_i)^T  \\phi(\\mathbf{x}))\\right) $$\n",
    "\n",
    "$$ \\phi(\\mathbf{x}_i)^T \\phi( \\mathbf{x}_j) \\longrightarrow k(\\mathbf{x}_i,  \\mathbf{x}_j) $$\n",
    "\n",
    "k positive definite, symmetric $\\Rightarrow$ there exists a $\\phi$! (possilby $\\infty$-dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The idea of this rewrite is that now I observed that the\n",
    "optimization problem and the prediction problem can be\n",
    "written only in terms of these inner products. Let's say I\n",
    "want to use the feature function  ∅, like doing a polynomial\n",
    "expansion with the polynomial feature transformation and\n",
    "while if I use ∅x_i as these inputs then they just replace\n",
    "the x and then I just need the inner products between these\n",
    "feature vectors, but the only thing I really ever need is\n",
    "these inner products. Instead of trying to come up with some\n",
    "feature functions that are good to separate the data points,\n",
    "I can try it to come up with inner products. I can try to\n",
    "engineer this thing here instead of trying to engineer the\n",
    "features. If you write down any positive definite quadratic\n",
    "form that is symmetric, there's always a ∅. So whenever I\n",
    "write down any function that is positive definite and\n",
    "symmetric into vectors, there's always a feature function ∅\n",
    "so that this is the inner product in this feature space,\n",
    "which is kind of interesting. The feature space might be\n",
    "infinite dimensional though. So the idea that it might be\n",
    "much easier to come up with these kinds of inner products\n",
    "than trying to find ∅. So we're now trying to design the k,\n",
    "which makes this whole thing a good nonlinear classifier. So\n",
    "in this case, obviously, the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Kernels\n",
    "\n",
    "\n",
    "$$k_\\text{linear}(\\mathbf{x}, \\mathbf{x}') = \\mathbf{x}^T\\mathbf{x}'$$\n",
    "\n",
    "$$k_\\text{poly}(\\mathbf{x}, \\mathbf{x}') = (\\mathbf{x}^T\\mathbf{x}' + c) ^ d$$\n",
    "\n",
    "$$k_\\text{rbf}(\\mathbf{x}, \\mathbf{x}') = \\exp(-\\gamma||\\mathbf{x} -\\mathbf{x}'||^2)$$\n",
    "\n",
    "$$k_\\text{sigmoid}(\\mathbf{x}, \\mathbf{x}') = \\tanh\\left(\\gamma \\mathbf{x}^T\\mathbf{x}'  + r\\right)$$\n",
    "\n",
    "`$$k_\\cap(\\mathbf{x}, \\mathbf{x}')= \\sum_{i=1}^p \\min(x_i, x'_i)$$`\n",
    "\n",
    "- If $k$ and $k'$ are kernels, so are `$k + k', kk', ck', ...$`\n",
    "\n",
    "\n",
    "\n",
    "There's a couple of kernels that are commonly used. So the\n",
    "linear kernel just means I just use the inner product in the\n",
    "original space. That is just the original linear SVM. Other\n",
    "kernels that are commonly used are like the polynomial\n",
    "kernel, in which I take the inner products, I add some\n",
    "constant c and I raise it to power d. There's the RVF\n",
    "kernel, which is probably the most commonly used kernel,\n",
    "which is just a Gaussian function above the curve around the\n",
    "distance of the two data points. There's a sigmoid kernel.\n",
    "There's the intersection kernel, which computes the minimum.\n",
    "And if you have any kernel, you can create a new kernel by\n",
    "adding two kernels together or multiplying them or\n",
    "multiplying them by constant. The only requirement we have\n",
    "is that they're positive, indefinite and symmetric.\n",
    "\n",
    "\n",
    "What does this look like in practice? So, for example, let's\n",
    "look at this polynomial kernel. It's not very commonly used,\n",
    "but I think it's one of the ones that are relatively easy to\n",
    "understand. So the idea of the polynomial kernel is it does\n",
    "the same thing as computing polynomials. But if you compute\n",
    "polynomials of a very high degree, your feature vector\n",
    "becomes very long. Whereas here, I always just need to\n",
    "compute this inner product and raise them to this power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel vs Features\n",
    "\n",
    "\n",
    "$$ k_\\text{poly}(\\mathbf{x}, \\mathbf{x}') = (\\mathbf{x}^T\\mathbf{x}' + c) ^ d $$\n",
    "\n",
    "Primal vs Dual Optimization\n",
    "\n",
    "\n",
    "Explicit polynomials $\\rightarrow$ compute on `n_samples * n_features ** d`\n",
    "\n",
    "Kernel trick $\\rightarrow$ compute on kernel matrix of shape `n_samples * n_samples`\n",
    "\n",
    "\n",
    "For a single feature:\n",
    "\n",
    "$$ (x^2, \\sqrt{2}x, 1)^T (x'^2, \\sqrt{2}x', 1) = x^2x'^2 + 2xx' + 1 = (xx' + 1)^2 $$\n",
    "\n",
    "\n",
    "\n",
    "So if I compute explicit polynomials, if I end features and\n",
    "then I do all the interactions, my data set becomes number\n",
    "of samples times number of features to the power D. So if I\n",
    "have 1000 features, and I take D to be 5, this will be\n",
    "enormous. If I'm using the kernel trick, which is replacing\n",
    "this inner product in the feature space by the kernel, then\n",
    "I only need to compute the inner product on the training\n",
    "data. So I only need to compute on this inner product matrix\n",
    "which is number of samples times number of samples. So this\n",
    "is much smaller if number of features to the D is smaller\n",
    "than number of samples then I can compute, essentially the\n",
    "same result on something that’s a different representation\n",
    "of the data. You can see that they're not entirely\n",
    "equivalent. So doing the polynomial expansion is not\n",
    "entirely equivalent to the polynomial features but it's\n",
    "about the same. You can see this quite easily for a single\n",
    "feature. If I do this for many features, adding extra\n",
    "features would make a very long feature vector, but the\n",
    "thing on the right-hand side always stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poly kernels vs explicit features\n",
    "\n",
    ".smaller[\n",
    "```python\n",
    "poly = PolynomialFeatures(include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "print(X.shape, X_poly.shape)\n",
    "print(poly.get_feature_names())\n",
    "```\n",
    "```\n",
    "((100, 2), (100, 5))\n",
    "['x0', 'x1', 'x0^2', 'x0 x1', 'x1^2']\n",
    "```\n",
    "]\n",
    "\n",
    ".center[\n",
    "![:scale 70%](images/poly_kernel_features.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "You can see that they're very similar, by just applying them\n",
    "scikit-learn, either the Kernel SVM or the explicit\n",
    "polynomial features. Here I have this dataset consisting of\n",
    "classes orange and blue. These are not linearly separable.\n",
    "So if I use a linear SVM, it wouldn't work. But I can do a\n",
    "polynomial transformation to go from two features to five\n",
    "features. In here I just added degree two features. And then\n",
    "I can learn linear SVM on top of these expanded features\n",
    "space. I get a very similar result if I instead of expanding\n",
    "the features, I use the original dataset. But now instead of\n",
    "learning linear SVM, I use a SVM with the polynomial kernel\n",
    "of degree 2, they're not exactly the same because there was\n",
    "this factor of squared of two that I mentioned, but they're\n",
    "pretty much the same. Question is if we increase the\n",
    "capacity of the model and we overfit and we'll talk about\n",
    "this in a little bit, but so, for now, we want to increase\n",
    "the capacity of a model making more flexible I mean, we\n",
    "definitely don't want to increase it infinitely. But we\n",
    "really making out model more flexible. So here for the\n",
    "polynomial kernel, you could get the same result just by\n",
    "expanding. If you have a very high degree polynomial it\n",
    "would be a large expansion. If you use one of the other\n",
    "kernels, for example, the RVF kernel doing this expansion\n",
    "would actually be resolved in infinite dimensional vector,\n",
    "so you couldn't actually do this with a feature vector. Now\n",
    "I'm looking at this model here on the right-hand side where\n",
    "I'm using the polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "## Understanding Dual Coefficients\n",
    "\n",
    "```python\n",
    "linear_svm.coef_\n",
    "```\n",
    "```\n",
    "array([[0.139, 0.06, -0.201, 0.048, 0.019]])\n",
    "```\n",
    "$$ y = \\text{sign}(0.139 x_0 + 0.06 x_1 - 0.201 x_0^2 + 0.048 x_0 x_1 + 0.019 x_1^2) $$\n",
    "\n",
    "```python\n",
    "linear_svm.dual_coef_\n",
    "#array([[-0.03, -0.003, 0.003, 0.03]])\n",
    "linear_svm.support_\n",
    "#array([1,26,42,62], dtype=int32)\n",
    "```\n",
    ".smallest[\n",
    "`$$ y = \\text{sign}(-0.03 \\phi(\\mathbf{x}_1)^T \\phi(x) - 0.003 \\phi(\\mathbf{x}_{26})^T \\phi(\\mathbf{x})  +0.003 \\phi(\\mathbf{x}_{42})^T \\phi(\\mathbf{x}) + 0.03 \\phi(\\mathbf{x}_{62})^T \\phi(\\mathbf{x})) $$`\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "FIXME formula goes over\n",
    "And so if I look at the linear SVM and I have five\n",
    "coefficients corresponding to the five features. And so if I\n",
    "make a prediction, its first coefficient times the first\n",
    "feature, second coefficient times the second feature and so\n",
    "on, and I look at the sine of it. This is just a linear\n",
    "prediction if I want to look what this looks like in terms\n",
    "of dual coefficients, I can look at the dual coefficients of\n",
    "linear SVM and the support. Out of these many data points,\n",
    "there's four, they were selected as support vectors by the\n",
    "optimization. These are the ones that have these black\n",
    "circles around them. And so the solution can be expressed as\n",
    "a linear combination of these four support vectors where the\n",
    "coefficients are dual coefficients.\n",
    "\n",
    "If I want to express this in terms of dual coefficients, I\n",
    "can say its dual coefficient -0.03 times the inner product\n",
    "of the first data point and my test data point in feature\n",
    "space. I do the same thing for the train each point 26 and\n",
    "42 and 63 with their respective weights. Now I can look at\n",
    "how does this look like for kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Kernel\n",
    "`$$y = \\text{sign}\\left(\\sum_i^{n}\\alpha_i k(\\mathbf{x}_i,  \\mathbf{x})\\right) $$`\n",
    "\n",
    "```python\n",
    "poly_svm.dual_coef_\n",
    "## array([[-0.057, -0. , -0.012, 0.008, 0.062]])\n",
    "poly_svm.support_\n",
    "## array([1,26,41,42,62], dtype=int32)\n",
    "```\n",
    "`$$ y = \\text{sign}(-0.057 (\\mathbf{x}_1^T\\mathbf{x} + 1)^2\n",
    "         -0.012 (\\mathbf{x}_{41}^T \\mathbf{x} + 1)^2 \\\\\n",
    "         +0.008 (\\mathbf{x}_{42}^T \\mathbf{x} + 1)^2 + 0.062 (\\mathbf{x}_{62}, \\mathbf{x} + 1)^2)\n",
    "$$`\n",
    "\n",
    "\n",
    "\n",
    "So for kernel, the prediction is this. For kernel SVM there\n",
    "are no coefficients in the original space, there's only dual\n",
    "coefficients. For each of support vectors, I compute the\n",
    "kernel of support vector with the test point for which I\n",
    "want to make a prediction. This is basically the prediction\n",
    "function of the kernel support vector machine. And you can\n",
    "see it's very similar to this only that I’ve replaced these\n",
    "inner products by the kernels. The original idea behind this\n",
    "kernel trick was to make computation faster. In some cases,\n",
    "it might be faster.\n",
    "\n",
    "$$ y = sign(-0.03 * np.inner(poly(X[1]), poly(x)) – 0.003 *\n",
    "np.inner(poly(X[26]), poly(x)) +0.003 *\n",
    "np.inner(poly(X[42]), poly(x)) + 0.03 *\n",
    "np.inner(poly(X[63]), poly(x)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Considerations\n",
    "\n",
    ".center[\n",
    "![:scale 85%](images/svm_runtime.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "So how does it look like in terms of runtime, here I have\n",
    "the same plot in linear space and log-log space. This is for\n",
    "a fixed number of features, more features make the kernel\n",
    "better. But if I fix the number of features, you can see\n",
    "which of the two is faster. Log-log space is better for\n",
    "this. So you can see that if I have a very small number of\n",
    "samples, then linear kernel and doing explicit polynomials\n",
    "is slower than doing the kernel because of the matrix that's\n",
    "number of samples times the number of samples is very small.\n",
    "But if I have a lot of features, then doing the explicit\n",
    "expansion is faster since the number of samples is large. In\n",
    "the case where we can do explicit feature expansion if we\n",
    "have a lot of samples, maybe that's faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [

    "\n",
    "## Kernels in Practice\n",
    "\n",
    "- Dual coefficients less interpretable\n",
    "- Long runtime for “large” datasets (100k samples)\n",
    "- Real power in infinite-dimensional spaces: rbf!\n",
    "- Rbf is “universal kernel” - can learn (aka overfit)\n",
    "anything.\n",
    "\n",
    "\n",
    "\n",
    "So what does this mean for us in practice? One issue is that\n",
    "the dual coefficients are usually less interpretable because\n",
    "they give you like weightings of inner products with the\n",
    "training data points which is maybe less intuitive than\n",
    "looking at like five times x squared and if you have large\n",
    "data sets they can become very slow. The real power of\n",
    "kernels is when the feature space would actually be\n",
    "infinite-dimensional. The RBF kernel is called the universal\n",
    "kernel, which means you can learn any possible concept or\n",
    "you can overfit any possible concept. Same is true for like\n",
    "neural networks and trees and nearest neighbors, for\n",
    "example, they can learn anything but linear models and even\n",
    "polynomial models of a fixed degree cannot learn everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [

    "\n",
    "#Preprocessing\n",
    "\n",
    "- Kernel use inner products or distances.\n",
    "- StandardScaler or MinMaxScaler ftw\n",
    "- Gamma parameter in RBF directly relates to scaling\n",
    "of data and n_features – the default is `1/(X.var() * n_features)`\n",
    "\n",
    "\n",
    "\n",
    "Nearly all of these kernels use the distance in the original\n",
    "space. These inner products are distances and so scaling is\n",
    "really important. People use a center scale or min-max\n",
    "scalar.\n",
    "\n",
    "For the RBF kernel or for any of the kernels really the\n",
    "default parameters work well for scale data. If you don't\n",
    "scale your data default parameters will not work at all. If\n",
    "you multiply that by five then the default parameters will\n",
    "give you terrible results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for RBF Kernels\n",
    "\n",
    "- Regularization parameter C is limit on alphas\n",
    "(for any kernel)\n",
    "- Gamma is bandwidth: $k_\\text{rbf}(\\mathbf{x}, \\mathbf{x}') = \\exp(-\\gamma||\\mathbf{x} -\\mathbf{x}'||^2)$\n",
    "\n",
    "\n",
    ".center[\n",
    "![:scale 85%](images/svm_gamma.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Support vector machine with RBF kernel has these two\n",
    "parameters you need to tune. These both control complexity\n",
    "in some way. We already said that the C parameter limits the\n",
    "influence of each individual data point and the other one is\n",
    "to kernel bandwidth gamma. A smaller gamma means a wider\n",
    "kernel. So wider kernel means a simpler model because the\n",
    "decision boundary will be smoother. Whereas a larger gamma\n",
    "means a much narrower kernel and that means each data point\n",
    "will have much more local influence, which means it's more\n",
    "like the nearest neighbor algorithm and has more complexity.\n",
    "Usually, you should tune both of these parameters to get a\n",
    "good tradeoff between fitting and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 85%](images/svm_c_gamma.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "This plot tries to illustrate these two parameters in the\n",
    "RBF SVM kernel. On the vertical is C and on the horizontal\n",
    "is gamma and support vectors are marked with circles. So the\n",
    "simplest model is basically the one with the smallest gamma\n",
    "and smallest C. Basically, all data points are support\n",
    "vectors, everything gets averaged and have a very broad\n",
    "kernel. Now, if I increase C, I overfit the data more, each\n",
    "data point can have more influence. And so only the data\n",
    "points that are really close to the boundary have influence\n",
    "now. If you increase gamma, the area of influence of each\n",
    "data point sort of shrinks. So here from it being basically\n",
    "linear or having this very little curvature if you increase\n",
    "gamma, it gets more and more curvature. And in the end, if\n",
    "you make gamma very large, each data point basically has a\n",
    "small sort of isolated island of its class around it giving\n",
    "a very complicated decision boundary. So here, for example,\n",
    "these clearly overfit the training data and so this is\n",
    "probably too large gamma. There's sort of a tradeoff between\n",
    "the two. Usually, they're multiple combinations of C and\n",
    "gamma that will give you similarly good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "```\n",
    ".center[\n",
    "![:scale 65%](images/digits_images.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Looking at the digit dataset. Let's say we want to learn\n",
    "kernel support vector machine on this dataset. So set the\n",
    "two important parameters to tune is C and gamma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Default Params\n",
    "\n",
    ".smaller[\n",
    "```\n",
    "gamma : {‘scale’, ‘auto’} or float, optional (default=’scale’)\n",
    "  Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
    "  if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var())\n",
    "  as value of gamma\n",
    "  if ‘auto’, uses 1 / n_features.\n",
    "```\n",
    "```python\n",
    "print('auto', np.mean(cross_val_score(SVC(gamma='auto'), X_train, y_train, cv=10)))\n",
    "print('scale', np.mean(cross_val_score(SVC(gamma='scale'), X_train, y_train, cv=10)))\n",
    "scaled_svc = make_pipeline(StandardScaler(), SVC())\n",
    "print('pipe', np.mean(cross_val_score(scaled_svc, X_train, y_train, cv=10)))\n",
    "```\n",
    "```\n",
    "auto 0.563\n",
    "scale 0.987\n",
    "pipe 0.977\n",
    "```\n",
    "```python\n",
    "gamma = (1. / (X_train.shape[1] * X_train.var()))\n",
    "print(np.mean(cross_val_score(SVC(gamma=gamma), X_train, y_train, cv=10)))\n",
    "```\n",
    "```\n",
    "0.987\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Gamma by default in scikit-learn is one over the number of\n",
    "features. This really makes sense only if you scale the data\n",
    "to have a standard deviation of one, then this is a pretty\n",
    "good default. Here if I compare cross-validation, kernel SVM\n",
    "with default parameters on this dataset versus kernel SVM\n",
    "with the scaled data, the performance is either 57% or 97%.\n",
    "There's a giant jump between the scaled data and the upscale\n",
    "data. Actually here in this data set, all the pixels are on\n",
    "the same scale more or less, so they all are between 0 and\n",
    "16. So if we change the gamma to be to take into account the\n",
    "standard deviation of the data set, I actually get pretty\n",
    "good results. This is actually just a very peculiar dataset\n",
    "because I basically I know the scale should be from zero to\n",
    "one when it's from 0 to 16. So scaling by the overall\n",
    "standard deviation gives me good results here. But in\n",
    "principle day, I want to convey is that gamma scales with\n",
    "the standard deviation of the features. Usually, each of the\n",
    "features has a different standard deviation or a different\n",
    "scale and so you want to use a centered scale to estimate\n",
    "the scale for each feature and bring it to one. And then the\n",
    "default gamma, which is one over number of features will be\n",
    "sort of reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-Searching Parameters\n",
    "\n",
    "```python\n",
    "param_grid = {'svc__C': np.logspace(-3, 2, 6),\n",
    "              'svc__gamma': np.logspace(-3, 2, 6) / X_train.shape[0]}\n",
    "param_grid\n",
    "```\n",
    "\n",
    "```json\n",
    "{'svc_C': array([0.001, 0.01, 0.1, 1., 10.,  100.]),\n",
    " 'svc_gamma': array([ 0.000001, 0.000007, 0.000074,\n",
    "                      0.000742, 0.007424, 0.074239])}\n",
    "```\n",
    "\n",
    "```python\n",
    "grid = GridSearchCV(scaled_svc, param_grid=param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "\n",
    "To tune them, I use the scaled SVM and both C and gamma\n",
    "usually learn auto log space, so here I have a log space for\n",
    "C and gamma. I actually divide this by the number of\n",
    "features, I could also just change the range, but as I said,\n",
    "this is sort of something that scales with the number of\n",
    "features and standard deviation. Then I can just do my\n",
    "standard grid searchCV with the two-dimensional parameter\n",
    "grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-Searching Parameters\n",
    "\n",
    ".center[\n",
    "![:scale 95%](images/svm_c_gamma_heatmap.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Usually, there's a strong interaction between these two\n",
    "parameters. And the SVM is pretty sensitive to the setting\n",
    "of these two parameters. So you can see here if you look at\n",
    "the scales, balance 10 classification dataset. So chance\n",
    "performance is 10% accuracy. And so if I set the parameters\n",
    "wrong, I get chance accuracy. If I set them right, I get the\n",
    "high 90s. So that's the difference between setting gamma to\n",
    "0.007 and  0.000007 or between setting c to 1 and c to\n",
    "0.0001.\n",
    "\n",
    "So usually they're some correlation between the C and gamma\n",
    "values which are good. So you can decrease or increase C if\n",
    "you decrease gamma and the other way around.\n",
    "\n",
    "So usually I like to look at grid search results as a 2d\n",
    "heat map for this and if your optimum is somewhere on the\n",
    "boundary, you want to extend your search space. For example,\n",
    "you can see that I wouldn't need to search this very small\n",
    "Cs, they don't work at all, but maybe something better might\n",
    "be over here. A C of 100 is already like a very big C so if\n",
    "I want to use even less regularization, learning the model\n",
    "will be even slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Questions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "sklearn.set_config(print_changed_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    if i > 12:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    ax.plot(X[:, i], y, 'o', alpha=.5)\n",
    "    ax.set_title(\"{}: {}\".format(i, boston.feature_names[i]))\n",
    "    ax.set_ylabel(\"MEDV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(LinearRegression(),\n",
    "                        X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(\n",
    "        Ridge(), X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'alpha': np.logspace(-3, 3, 14)}\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(Ridge(), param_grid, cv=10, return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "plt.figure(dpi=200)\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.plot('param_alpha', 'mean_train_score', ax=plt.gca())\n",
    "results.plot('param_alpha', 'mean_test_score', ax=plt.gca())\n",
    "\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, scale\n",
    "# being lazy and not really doing things properly whoops\n",
    "X_poly = PolynomialFeatures(include_bias=False).fit_transform(scale(X))\n",
    "print(X_poly.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(LinearRegression(),\n",
    "                        X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(Ridge(),\n",
    "                        X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(Ridge(), param_grid, cv=10, return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "results.plot('param_alpha', 'mean_train_score', ax=plt.gca())\n",
    "results.plot('param_alpha', 'mean_test_score', ax=plt.gca())\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "plt.scatter(range(X_poly.shape[1]), lr.coef_, c=np.sign(lr.coef_), cmap=\"bwr_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = grid.best_estimator_\n",
    "plt.scatter(range(X_poly.shape[1]), ridge.coef_, c=np.sign(ridge.coef_), cmap=\"bwr_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge100 = Ridge(alpha=100).fit(X_train, y_train)\n",
    "ridge1 = Ridge(alpha=1).fit(X_train, y_train)\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(ridge1.coef_, 'o', label=\"alpha=1\")\n",
    "plt.plot(ridge.coef_, 'o', label=\"alpha=14\")\n",
    "plt.plot(ridge100.coef_, 'o', label=\"alpha=100\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used:\", np.sum(lasso.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Load the diabetes dataset using ``sklearn.datasets.load_diabetes``. Apply ``LinearRegression``, ``Ridge`` and ``Lasso`` and visualize the coefficients. Try polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/linear_models_diabetes.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
