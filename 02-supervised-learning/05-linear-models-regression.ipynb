{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models for Regression\n",
    "\n",
    "One of the oldest and most widely used models are linear regression models. Linear models for regression have been a staple in statistics and engineering long before the invention of the terms Computer Science and Machine Learning TODO cite. In fact, one of the most commonly used approaches goes back to Gauss himself, and you might have seen some variant of it in high school. Linear models for regression also serve as the basis for the linear models for classification that we will see in section TODO.\n",
    "\n",
    "The principle of linear models is illustrated in Figure TODO which shows a linear regression in one dimension, in our terminology we have one feature, shown on the x-axis, and our target is shown on the y-axis.\n",
    "We now try to find a linear relationship between the two, in other words, we want to find a straight line that is as close as possible to all of the points.\n",
    "For a single feature, you can write the prediction given by the line as\n",
    "$$ \\hat{y} = a x + b $$\n",
    "where $a$ and $b$ are fixed numbers, and $x$ is the value of our input feature. Here, $a$ is the slope of the line, and $b$ is known as the intercept or offset. It is the place where the fitted line crossed the y-axis.\n",
    "For more than one feature, the prediction takes the form\n",
    "$$ \\hat{y} = w_1  x_1 + w_2  x_2 + ...  + w_p x_p + b $$\n",
    "where $x_1$ to $x_p$ are the features (i.e. we have p features) and $w_1$ to $w_p$ are the coefficients or slope for each of these features.\n",
    "For more than one features, visualizing the prediction is much trickier, so we'll skip that for now.\n",
    "\n",
    "```{admonition} Mathematical details\n",
    "Often the above equation is written as as an inner product between two vectors $w \\in \\mathbb{R}^p$ and $x\\in \\mathbb{R}^p$:\n",
    "$$\\hat{y} = w^T \\mathbf{x} + b = \\sum_{i=1}^p w_i x_i +b$$\n",
    "As a side note, linear models are named such because they are linear in $w$, not because they are linear in $x$. In fact, we can replace any $x_i$ by an arbitrary fixed function of $x$, say $sin(x_i)$ or $x_i * x_j$, and it would still be considered a linear model. This is one of the powers of this kind of model, and we'll discuss it in more depth in chapter TODO feature engineering. \n",
    "```\n",
    "\n",
    "All linear models for regression share the same formular for prediction shown in TODO. However, how they differ is in how they learn the coefficients $w$ and $b$ from the training data. Here, $w$ and $b$ are *parameters* of the model (in the statistical sense), i.e. they are the parts of the model that are estimated from the training data. Nearly all of the methods for learning linear models are formulated as an optimization problem, where the goal is to find the optimum of some mathematical formula, which is known as objective function. We will provide the objective functions (and therefore the optimizatio problem) that give rise to all the models we will discuss. If you are not familiar with optimization, you can skip these parts as we will also provide some more intuitive explanation of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "The oldest and probably the most widely use method to learn the parameters $w$ and $b$ is ordinary least squares (OLS). This is what people usually refer to when they say \"linear regression\" and it's what's implemented in scikit-learn's ``LinearRegression`` class. The idea behind OLS is to compute the *least squares* solution, which means find the parameters $w$ and $b$ so that the prediction error on the training set has the smallest possible squared norm.\n",
    "This is a classical and well-understood method. However, it requires the training data to be somewhat well-behaved. If there are more features than there are samples, or if there is a linear relationship between the features, then the method can be unstable and will often reduce non-sensical results.\n",
    "TODO image\n",
    "\n",
    "```{admonition} Mathematical details\n",
    "The objective function of ordinary least squares for linear regression is given by\n",
    "$$\\min_{w \\in \\mathbb{R}^p, b\\in\\mathbb{R}} \\sum_{i=1}^n (w^T\\mathbf{x}_i + b - y_i)^2$$\n",
    "Here, $x_i, i=1,...,n $ are the training vectors and $y_i i=1,...,n$ TODO indexing unlike above?! are the corresponding targets.\n",
    "The formula on the right computes the prediction error for each training sample, squares it, and then sums over all the samples.\n",
    "The outcome is a number that measures the euclidean length (aka squared norm) of the error $\\hat{y}_i - y_i$. We want to find a vector $w$ and a number $b$ such that this error is minimized.\n",
    "If the data matrix X has full column rank, then this is a convex optimization problem, and we can easily compute the optimum with a number of standard techniques.\n",
    "If the data matrix X does not have full column rank, for example if there is more feature than samples, then there is no unique optimum; in fact there will be infinitely many solutions that lead to zero error on the training set.\n",
    "In this case, the optimization problem is called *ill conditioned* as no unique solution exists. In this case we can add additional criteria that guarantee a well-behaved solution TODO ref, though\n",
    "they are not always implemented in statistical software.\n",
    "```\n",
    "\n",
    "Before we look at examples of using ``LinearRegression`` let's look at an alternative that is particularly popular for predictive modelling, *Ridge Regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "Ridge regression is an extension to OLS in which we add another demand to our solution. Not only do we want our model to fit the training data well, we also want it to be simple.\n",
    "We already discussed in chapter TODO, that we prefer simple solutions, as they will generalize more readily to new data. TODO this needs work.\n",
    "We can include this directly in our modeling process-however, we have to formalize what we mean by \"simple\". In Ridge regression, simple is encoded as having small slope in all directions.\n",
    "In other words, we want our coefficient vectors $w$ to be as close as possible to 0 (in fact the restriction is on $w^2$). So now we have to goals: we want a simple model (i.e. one with small coefficients) that also fits the training data well. To allow optimizing both, we need to introduce a parameter, often called $\\alpha$ (or sometimes $\\lambda$) that trades off between the two goals. Now $\\alpha$ is a hyper-parameter that we need to tune or pick, say using `GridSearchCV`. Setting $alpha=0$ will disregard the simplicity demand, and is equivalent to using OLS. Setting $\\alpha=\\infty$ will basically disregard the data fitting demand, and will produce a coefficient vector of all zeros, which is the most \"simple\" solution according to the least squares measure used in Ridge regression. The $\\alpha$ parameter is known as *regularization parameter* or *penalty term*, as it regularizes or penalizes overly complex solutions $w$.\n",
    "\n",
    "```{admonition} Mathematical details\n",
    "The objective function of Ridge regression is very similar to OLS; it only has the addition of the penalty term $\\alpha ||w||^2$. Notice that the intercept $b$ is not penalized. TODO explain?\n",
    "$$ \\min_{w \\in \\mathbb{R}^p, b\\in\\mathbb{R}} \\sum_{i=1}^n (w^T\\mathbf{x}_i + b - y_i)^2 + \\alpha ||w||^2 $$\n",
    "The first term in the objective function is also known as the data fitting term, while the second term, the penalty term, is completely independent of the data.\n",
    "For $\\alpha>0$, this objective function is always strongly convex, which leads to simple and robust optimization methods.\n",
    "```\n",
    "\n",
    "The benefit of Ridge regression is that it is much more stable than linear regression with OLS. If $\\alpha>0$, it always has a unique solution, no matter how badly behaved your data is.\n",
    "From a statistical perspective Ridge regression has the disadvantage that it is not an unbiased estimator: we explicitly biased our model to make the coefficients small, so predictions are biased towards zero. In predictive modeling, this is rarely an issue, however, you should keep it in mind.\n",
    "\n",
    "The presence of a hyper-parameter could be seen either as an advantage or a disadvantage: one the one hand, it allows us to control the model fitting more closely, on the other it requires us to use some method to adjust the hyper-parameter, which makes the whole modeling process more complicated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Mathematical details\n",
    "\n",
    "## (regularized) Empirical Risk Minimization\n",
    "The objective function shown in TODO is a typical example of a mathematical framework that's widely used in machine learning,\n",
    "known as regularized empirical risk minimization.\n",
    "As mentioned in TODO we didn't mention it yet, the goal in supervised learning is usually to find a function $f$ from a fixed familiy of fuctions $F$ that minimizes\n",
    "\n",
    "$$\\min_{f\\in F} \\mathbb{E}_{(x, y) \\propto p} \\ell(f(x), y)$$\n",
    "for some loss function $\\ell$. In other words, we want to minimize the expected loss between the prediction made by $f$ and the true outcome $y$,\n",
    "when sampling $x$ and $y$ from the data distribution. However, in practice, it's usually impossible to access the data distribution $p$, and instead,\n",
    "we have an i.i.d. sample from it, the training data.\n",
    "\n",
    "The idea behind empirical risk minimization is that if we minimize the training set error, and the function $f$ is simple, then the error on new samples will also be small.\n",
    "In other words, we replace the expectation in formula TODO by the empirical estimate on the training set:\n",
    "$$\\min_{f\\in F} \\sum_{i=i}^n\\ell(f(x_i), y_i)$$\n",
    "This is now a problem we can solve, and it is exactly the form of the objective found in OLS in equation TODO, where $F$ is the family of all linear models, as parametrized by $w$ and $b$, and $\\ell$ is the squared loss.\n",
    "If we take this approach, it's possible to proof theorems of the form TODO\n",
    "\n",
    "In other words, we are trying to ensure a small error on the training set, and if we achive that, we can show that the test set error will also be small, if $f$ is sufficiently simple.\n",
    "We could now try to optimize the bound in Equation TODO more directly, by optimizing both the training set error and encouraging the model to be simple.\n",
    "This leads to *regularized empirical risk minimization* in which we add an additional *regularization term* $R$:\n",
    "$$ \\min_{f \\in F} \\sum_{i=1}^n L(f(\\mathbf{x}_i), y_i) + \\alpha R(f) $$\n",
    "This is the approach taken by ridge regression, where $R$ is the squared norm of the coefficients.\n",
    "Maybe somewhat surprisingly, equation TODO summarizes most of supervised machine learning, and nearly every supervised algorithm in this book follows this idea.\n",
    "Linear models, support vector machines, neural networks and gradient boosting can all be formulated in this way, using different choices of the family of functions F, the loss $\\ell$ and the regularizer $R$.\n",
    "It is also the basis of nearly all theoretical analysis of machine learning algorithms. Interestingly, it is now thought to not adequately explain the generalization ability of the very large neural networks used today, as they are not \"simple\" in any way we can quantify easily. This has led to a large body of interesting work investigating new theoretical underpinning for deep neural networks TODO reference.\n",
    "Please keep in mind that this is a very birds-eye view, as we are much more concerned with practical issues in this book.\n",
    "Theoretical machine learning is a fascinating topic, and you can learn much, much more in the amazing book TODO.\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of determination\n",
    "To evaluate model fit for regression, scikit-learn by default uses the coefficient of determination, also known as $R^2$, which is defined as follows:\n",
    "$$ R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n - 1} (y_i - \\hat{y}_i)^2}{\\sum_{i=0}^{n - 1} (y_i - \\bar{y})^2} $$\n",
    "where $\\bar{y}$ is the mean response over the training set:\n",
    "$$ \\bar{y} =  \\frac{1}{n} \\sum_{i=0}^{n - 1} y_i$$\n",
    "The intuition of the measure it that it looks at the relative average distance between prediction and ground truth.\n",
    "\n",
    "There are several definitions of this metric, but the one given in Equation TODO works for any model, and is what ``score`` will return for any regressor.\n",
    "The benefit of this metric is that its scale is easily interpretable: a score of 1 means perfect prediction, while a score of 0 means constant or entirely random predictions.\n",
    "Note that with this definition, the $R^2$ score can be negative, which basically means that predictions are worse than just predicting the mean.\n",
    "A disadvantage of this score is that it depends on the variance of the data it is applied to, which has several peculiar implications.\n",
    "In particular, adding a data point with an extreme true response value will improve the overall score. Also, the mean is estimated independently when scoring the test set,\n",
    "which can lead to artifacts if the test set is small. In particular, when computing the score on a single data point, it is undefined.\n",
    "TODO reference Max Kuhn.\n",
    "\n",
    "An alternative metric that has very different properties, is the root mean squared error (RMSE), which we'll discuss in more detail in Chapter TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ames Housing Dataset\n",
    "\n",
    "![:scale 80%](images/ames_housing_scatter.png)\n",
    ".tiny[\n",
    "```python\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "```\n",
    "```\n",
    "(2195, 79)\n",
    "(2195,)\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "Ok after all this pretty abstract talk, let's make this\n",
    "concrete. Let's do some regression on the boston housing\n",
    "dataset. After the last homework you're hopefully familiar\n",
    "with it. The idea is to predict prices of property in the\n",
    "boston area in different neighborhoods. This is a dataset\n",
    "from the 70s I think, so everything is pretty cheap. Most of\n",
    "the features you can see are continuous, with the exception\n",
    "of the charlston river variable which says whether the\n",
    "neighborhood is on the river.\n",
    "\n",
    "Keep in mind that this data lives in a 13 dimensional space\n",
    "and these univariate plots only look at 13 different\n",
    "projections of the data, and can't capture any of the\n",
    "interactions.\n",
    "\n",
    "But still we can see that the price clearly depends on some\n",
    "of these variables. It's also pretty clear that the\n",
    "dependency is non-linear for some of the variables. We'll\n",
    "still start with a linear model, because its a very simple\n",
    "class of models, and I'd always star approaching any model\n",
    "from the simplest baseline. In this case it's linear\n",
    "regression. We're having 506 samples and 13 features. We\n",
    "have much more samples than features. Linear regression\n",
    "should work just fine. Also it's a tiny dataset, so\n",
    "basically anything we'll try will run instantaneously, which\n",
    "is also good to keep in mind.\n",
    "\n",
    "Another thing that you can see in this graph is that the\n",
    "features have very different scales. Here's a box plot that\n",
    "shows that even more clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 100%](images/ames_scaling.png)\n",
    "\n",
    "\n",
    "\n",
    "That's something that will trip up the distance based models\n",
    "models we talked about last time, as well as the linear\n",
    "models we're talking about today.  For the penalized models\n",
    "the different scales mean that different features are\n",
    "penalized differently, which you usually want to avoid.\n",
    "Usually there is no particular semantics attached to the\n",
    "fact that one feature has different magnitutes than another.\n",
    "We could measure something in inches instead of miles, and\n",
    "that would change the outcome of the model. That's certainly\n",
    "not something we want. A good idea is to scale the data to\n",
    "get rid of this effect. We'll talk about that and other\n",
    "preprocessing methods in-depth on Wednesday next week. Today\n",
    "I'm mostly gonna ignore this. But let's get started with\n",
    "Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "```python\n",
    "cat_preprocessing = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='NA'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "cont_preprocessing = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    StandardScaler())\n",
    "\n",
    "preprocess = make_column_transformer(\n",
    "    (cat_preprocessing, make_column_selector(dtype_include='object')),\n",
    "    remainder=cont_preprocessing)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)\n",
    "cross_val_score(\n",
    "    make_pipeline(preprocess, LinearRegression()),\n",
    "    X_train, y_train, cv=5)\n",
    "```\n",
    "```\n",
    "array([0.928, 0.927, 0.932, 0.898, 0.884])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewed targets\n",
    ".left-column[\n",
    "```python\n",
    "y_train.hist(bins='auto')\n",
    "```\n",
    "![:scale 100%](images/ames_housing_price_hist.png)\n",
    "\n",
    "]\n",
    ".right-column[\n",
    "```python\n",
    "np.log(y_train).hist(bins='auto')\n",
    "```\n",
    "![:scale 100%](images/ames_housing_price_hist_log.png)\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "cross_val_score(make_pipeline(preprocess, LinearRegression()),\n",
    "                X_train, y_train, cv=5)\n",
    "```\n",
    "```\n",
    "array([0.928, 0.927, 0.932, 0.898, 0.884])\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "log_regressor = TransformedTargetRegressor(\n",
    "    LinearRegression(), func=np.log, inverse_func=np.exp)\n",
    "cross_val_score(make_pipeline(preprocess, log_regressor),\n",
    "                X_train, y_train, cv=5)\n",
    "```\n",
    "```\n",
    "array([0.95 , 0.943, 0.941, 0.913, 0.922])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression vs Ridge\n",
    "```python\n",
    "log_regressor = TransformedTargetRegressor(\n",
    "    LinearRegression(), func=np.log, inverse_func=np.exp)\n",
    "cross_val_score(make_pipeline(preprocess, log_regressor),\n",
    "                X_train, y_train, cv=5)\n",
    "\n",
    "```\n",
    "```\n",
    "array([0.95 , 0.943, 0.941, 0.913, 0.922])\n",
    "```\n",
    "```python\n",
    "log_ridge = TransformedTargetRegressor(\n",
    "    Ridge(), func=np.log, inverse_func=np.exp)\n",
    "cross_val_score(make_pipeline(preprocess, log_ridge),\n",
    "                X_train, y_train, cv=5)\n",
    "\n",
    "```\n",
    "```\n",
    "array([0.948, 0.95 , 0.941, 0.915, 0.931])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Let’s look at two simple models. Linear regression and Ridge\n",
    "regression. What I've done is I’ve split the data into\n",
    "training and test set and used 10 fold cross-validation to\n",
    "evaluate them. Here I use cross_val_score together with the\n",
    "model, the training data, training labels, and 10 fold\n",
    "cross-validation. This will return 10 scores and I'm going\n",
    "to compute the mean of them. I'm doing this for both linear\n",
    "regression and Ridge regression. Here is ridge regression\n",
    "uses a default value of alpha of 1. Here these two scores\n",
    "are quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".smallest[\n",
    "```python\n",
    "pipe = Pipeline([('preprocessing', preprocess), ('ridge', log_ridge)])\n",
    "param_grid = {'ridge__regressor__alpha': np.logspace(-3, 3, 13)}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=RepeatedKFold(10, 5),\n",
    "                    return_train_score=True)\n",
    "```\n",
    "```\n",
    "{'ridge__regressor__alpha': array([ 0.001,  0.003, 0.01, 0.032, 0.1, 0.316, 1., 3.162,\n",
    "           10., 31.623, 100., 316.228, 1000.])}\n",
    "```\n",
    "\n",
    "```python\n",
    "grid.fit(X_train, y_train)\n",
    "```\n",
    "]\n",
    ".center[\n",
    "![:scale 50%](images/ridge_alpha_search.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Coming back to the ridge regression we used the standard\n",
    "alpha of one which is a reasonable default, but by no means,\n",
    "this is guaranteed to make sense in this particular problem.\n",
    "Here I’ve done the grid search. As we talked about on\n",
    "Monday, I defined a parameter grid where the key is the\n",
    "parameter I want to search (alpha in Ridge) and the\n",
    "parameters I want to try. For regularization parameters,\n",
    "like alpha, it’s usually good to do them on the logarithmic\n",
    "grid. I do a relatively fine grid here with 13 different\n",
    "points mostly because I wanted to have a nice plot. In\n",
    "reality, I would use a three or six or something like that.\n",
    "I’ve instantiated GridSearchCV with Ridge(), the parameter\n",
    "grid and do 10 fold cross-validation and then I called\n",
    "grid.fit. I’ve reported the mean training accuracy and mean\n",
    "test accuracy over 10 cross-validation folds for each of the\n",
    "parameter settings. Okay, you can see a couple things here.\n",
    "A) There's a lot of uncertainty B) The training set is\n",
    "always better than the test set. C) The most important thing\n",
    "that you can see here is that regularization didn't help.\n",
    "Making alpha as small as possible is the best. What I'm\n",
    "going to do next is I'm going to modify this dataset a\n",
    "little bit so that we can see the effect of the\n",
    "regularization. I’m going to modifying by using a polynomial\n",
    "expansion, again we're going to talk about a little bit more\n",
    "on Wednesday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".padding-top[\n",
    ".left-column[\n",
    "![:scale 100%](images/ridge_alpha_search.png)\n",
    "]\n",
    ".right-column[\n",
    "![:scale 100%](images/ridge_alpha_search_cv_runs.png)\n",
    "]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triazine Dataset\n",
    "\n",
    "```python\n",
    "triazines = fetch_openml('triazines')\n",
    "triazines.data.shape\n",
    "```\n",
    "```\n",
    "(186, 60)\n",
    "```\n",
    "```python\n",
    "pd.Series(triazines.target).hist()\n",
    "```\n",
    ".center[\n",
    "![:scale 40%](images/triazine_bar.png)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(triazines.data, triazines.target, random_state=0)\n",
    "\n",
    "cross_val_score(LinearRegression(), X_train, y_train, cv=5)\n",
    "```\n",
    "```\n",
    "array([-4.749e+24, -9.224e+24, -7.317e+23, -2.318e+23, -2.733e+22])\n",
    "```\n",
    "```python\n",
    "cross_val_score(Ridge(), X_train, y_train, cv=5)\n",
    "```\n",
    "```\n",
    "array([0.263, 0.455, 0.024, 0.23 , 0.036])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "param_grid = {'alpha': np.logspace(-3, 3, 13)}\n",
    "\n",
    "grid = GridSearchCV(Ridge(), param_grid, cv=RepeatedKFold(10, 5),\n",
    "                    return_train_score=True)\n",
    "grid.fit(X_train, y_train)\n",
    "```\n",
    ".center[\n",
    "![:scale 40%](images/ridge_alpha_triazine.png)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting coefficient values (LR)\n",
    "\n",
    "\n",
    "```python\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "plt.scatter(range(X_train.shape[1]), lr.coef_,\n",
    "            c=np.sign(lr.coef_), cmap='bwr_r')\n",
    "\n",
    "```\n",
    ".center[\n",
    "![:scale 55%](images/lr_coefficients_large.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "In the previous slide, linear regression did nearly as well\n",
    "as the Ridge Regression with the default parameters but\n",
    "let's look at the coefficients of a linear model. These are\n",
    "the coefficients of the linear model trained on the\n",
    "polynomial futures. I plot them adding a color to represent\n",
    "positive and negative. The magnitude here is 1 and then 13\n",
    "zeros. We have two features, one is like, probably more than\n",
    "trillions times two and then there's another one that's very\n",
    "negative. What I conclude from this is, these 2 features are\n",
    "very highly correlated. The model makes both of them really,\n",
    "really big and then they cancel each other out. This is not\n",
    "a very nice model, because it relates to American stability\n",
    "and also it tells me that these 2 features are like\n",
    "extremely important, but they might not be important at all.\n",
    "Like the other features might be more important, but they're\n",
    "nullified by this cancellation effect. They (0.2 and -0.8)\n",
    "need to cancel each other out because the predictions are\n",
    "reasonable and all the houses only cost like $70,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Coefficients\n",
    "\n",
    "```python\n",
    "ridge = grid.best_estimator_\n",
    "plt.scatter(range(X_train.shape[1]), ridge.coef_,\n",
    "            c=np.sign(ridge.coef_), cmap=\"bwr_r\")\n",
    "```\n",
    ".center[\n",
    "![:scale 55%](images/ridge_coefficients.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Let's look at the Ridge model. This is the best estimator,\n",
    "which is the model that was found in a grid search with the\n",
    "best parameter settings. This looks much more reasonable.\n",
    "This feature, which was a very negative one, still is very\n",
    "negative. But now this is actually three and minus three. So\n",
    "this is a much more reasonable range. We can also look at\n",
    "the features and the effect of different values of alpha.\n",
    "Here is a Ridge with 3 different values of alpha. In the\n",
    "previous random seat, alpha equal to 14 was the best now and\n",
    "now we have it equal to 30 something. The green one is more\n",
    "or less the best setting and then there's a smaller and a\n",
    "bigger one. You can see that basically what alpha does is,\n",
    "on average, it pushes all the coefficients toward zero. So\n",
    "here you can see this coefficient shrank going from 1 to 14\n",
    "going to 0 and the same here. So basically, they all push\n",
    "the different features towards 0. If you look at this long\n",
    "enough, you can see things that are interesting, the first\n",
    "one with alpha equal to one it's positive, and with alpha\n",
    "equal to 100, it's negative. That means depending on how\n",
    "much you regularize the direction of effect goes in opposite\n",
    "directions, what that tells me is don't interpret your\n",
    "models too much because clearly, either it has a positive or\n",
    "negative effect, it can't have both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ridge100 = Ridge(alpha=100).fit(X_train, y_train)\n",
    "ridge1 = Ridge(alpha=.1).fit(X_train, y_train)\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(ridge1.coef_, 'o', label=\"alpha=.1\")\n",
    "plt.plot(ridge.coef_, 'o', label=f\"alpha={ridge.alpha:.2f}\")\n",
    "plt.plot(ridge100.coef_, 'o', label=\"alpha=100\")\n",
    "plt.legend()\n",
    "```\n",
    "\n",
    ".center[\n",
    "![:scale 60%](images/ridge_coefficients_alpha.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "One other way to visualize the coefficient is to look at the\n",
    "coefficient path or regularization path. On the x-axis is\n",
    "the alpha, and on the y-axis, the coefficient magnitude.\n",
    "Basically, I looped over all of the different alphas and you\n",
    "can see how they shrink towards zero to increase alpha.\n",
    "There are some very big coefficients that go to zero very\n",
    "quickly and some coefficients here that stay the same for a\n",
    "long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "![:scale 90%](images/ridge_learning_curve.png)\n",
    "\n",
    "\n",
    "\n",
    "The thing I want to illustrate is if you have less data,\n",
    "regularization helps more. You can see the effects here\n",
    "between the orange and the red are very drastic. The green\n",
    "one probably has too much regularization. If you have very\n",
    "little data, even regularizing way too much is okay, but\n",
    "there's sort of a sweet spot here which is the orange one\n",
    "and the orange one does best. The more data you add, the\n",
    "less important regularization becomes. If you take the blue\n",
    "curve at the beginning it’s much higher than the red curve,\n",
    "but then, in the end, they overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "$$ \\min_{w \\in \\mathbb{R}^p, b\\in\\mathbb{R}} \\sum_{i=1}^n (w^T\\mathbf{x}_i + b - y_i)^2 + \\alpha ||w||_1 $$\n",
    "\n",
    "- Shrinks w towards zero like Ridge\n",
    "\n",
    "- Sets some w exactly to zero - automatic feature selection!\n",
    "\n",
    "\n",
    "\n",
    "Lasso Regression looks very similar to Ridge Regression. The\n",
    "only thing that is changed is we use the L1 norm instead of\n",
    "the L2 norm. L2 norm is the sum of squares, the L1 norm is\n",
    "the sum of the absolute values. So again, we are shrinking w\n",
    "towards 0, but we're shrinking it in a different way. The L2\n",
    "norm penalizes very large coefficients more, the L1 norm\n",
    "penalizes all coefficients equally. What this does in\n",
    "practice is its sets some entries of W to exactly 0. It does\n",
    "automatic feature selection if the coefficient of zero means\n",
    "it doesn't influence the prediction and so you can just drop\n",
    "it out of the model. This model does features selection\n",
    "together with prediction. Ideally what you would want is,\n",
    "let's say you want a model that does features selections.\n",
    "The goal is to make our model automatically select the\n",
    "features that are good. What you would want to penalize the\n",
    "number of features that it uses, that would be L0 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding L1 and L2 Penalties\n",
    ".wide-left-column[\n",
    "![:scale 100%](images/l2_l1_l0.png)\n",
    "]\n",
    "\n",
    ".narrow-right-column[\n",
    ".smaller[\n",
    "$$ \\ell_2(w) = \\sqrt{\\sum_i w_i ^ 2}$$\n",
    "$$ \\ell_1(w) = \\sum_i |w_i|$$\n",
    "$$ \\ell_0(w) = \\sum_i 1_{w_i != 0}$$\n",
    "]]\n",
    "\n",
    "\n",
    "The L0 norm penalizes the number of features that are not\n",
    "zero. The L1 norm penalizes the absolute values. L2 norm\n",
    "penalizes the squared norm. The problem with L0 norm is it's\n",
    "one everywhere except at zero. And that's not\n",
    "differentiable, it's not even continuous, so this is very\n",
    "hard to optimize. Basically, we just gave up on this and we\n",
    "use the next best thing we can do, which is the L1 norm. The\n",
    "L1 norm is not differentiable, there’s like this kink in\n",
    "zero, but we can still optimize it pretty well. The L2 norm\n",
    "penalizes large values much more than small values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding L1 and L2 Penalties\n",
    "\n",
    ".wide-left-column[\n",
    "![:scale 100%](images/l1_kink.png)\n",
    "]\n",
    ".narrow-right-column[\n",
    ".padding-top[\n",
    ".smaller[\n",
    "$ f(x) = (2 x - 1)^2 $\n",
    "\n",
    "$ f(x) + L2 = (2 x - 1)^2 + \\alpha x^2$\n",
    "\n",
    "$ f(x) + L1= (2 x - 1)^2 + \\alpha |x|$\n",
    "]]]\n",
    "\n",
    "\n",
    "Let's say I want to do a one-dimensional linear regression\n",
    "problem. The problem to solve is something of this form. I\n",
    "want my coefficients to be such that, let's say x, the one\n",
    "coefficient I'm looking for and this is sort of the squared\n",
    "loss function that I have. F is the daycare fitting term\n",
    "here, and this is would be like some quadratic. The loss\n",
    "function is in blue, its quadratic. The loss function plus\n",
    "our L2 regularization, the ridge is in orange. The data\n",
    "fitting plus L1 regularization is in green. The blue is a\n",
    "squared quadratic function, the orange is a quadratic\n",
    "function that sort of moved a little bit toward zero and the\n",
    "green one also moved towards zero but there's a kink here.\n",
    "Basically, this is just sort of the kink at the absolute\n",
    "value of zero and that makes it much more likely that the\n",
    "optimum will actually be at zero. So here the optimum for\n",
    "orange is somewhere here, so it was pushed towards zero but\n",
    "not exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding L1 and L2 Penalties\n",
    "\n",
    "![:scale 70%](images/l1l2ball.png)\n",
    "\n",
    "\n",
    "A different way to visualize this is in two dimensions.\n",
    "Let's say we have 2 coefficients. So one way to look at\n",
    "these norms, let’s say we want to minimize the norm but say\n",
    "we want to fix the norm, like more or less equivalent, these\n",
    "are the L1 and L2 balls. So this is all the 2D vectors that\n",
    "have Euclidean norm 1 or the ball and all the vectors that\n",
    "have L1 norm is the diamond. Let's say we restrict ourselves\n",
    "to a solution that lies on this ball."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding L1 and L2 Penalties\n",
    "\n",
    "![:scale 70%](images/l1l2ball_intersect.png)\n",
    "\n",
    "\n",
    "And then we have a quadratic function like this, this is\n",
    "sort of the data fitting term again and so now if we want a\n",
    "solution with L2 norm equal to zero, we get the solution\n",
    "here, the intersection of the height lines of the data\n",
    "fitting term and constraint to L2 norm. But for this\n",
    "diamond, we're going to hit very likely one of the corners\n",
    "just because of the geometry of the space, we're likely to\n",
    "either of these corners. The corner means one of the\n",
    "coefficients is exactly zero, and the other one is one. So\n",
    "this is another sort of geometric realization of trying to\n",
    "understand why does this lead to exact zeros hop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-Search for Lasso\n",
    "```python\n",
    "param_grid = {'alpha': np.logspace(-5, 0, 10)}\n",
    "grid = GridSearchCV(Lasso(normalize=True), param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "```\n",
    "```\n",
    "{'alpha': 0.0016}\n",
    "0.163\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now we can do Grid Search again, the default parameters\n",
    "usually don't work very well for Lasso. I use alpha on the\n",
    "logarithmic grid. I fitted and then I get the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 90%](images/lasso_alpha_triazine.png)\n",
    "\n",
    "\n",
    "\n",
    "Looking at the training test set performance, you can see\n",
    "that if you increase the regularization, this model gets\n",
    "really bad. The ridge regression didn't go this badly. If\n",
    "you set the realization to one, all coefficients become\n",
    "zero. Other than that there's reasonable performance, which\n",
    "is about as good as the ridge performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 60%](images/lasso_coefficients.png)\n",
    "]\n",
    "\n",
    "```python\n",
    "print(X_train.shape)\n",
    "np.sum(lasso.coef_ != 0)\n",
    "```\n",
    "```\n",
    "(139, 60)\n",
    "13\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "These are the coefficients of the model. Out of the 104\n",
    "features, it only selected 64 that are non-zero, the other\n",
    "ones are exactly zero. You can see this visualized here. The\n",
    "white ones are exactly zero and the other ones on non-zero.\n",
    "If I wanted I could prune the future space a lot and that\n",
    "makes the model possibly more interpretable. There's a\n",
    "slight caveat here if two of the features that are very\n",
    "correlated, Lasso will pick one of them at random and make\n",
    "the other one zero. Just because something's zero doesn't\n",
    "mean it's not important. It means you can drop it out of\n",
    "this model. If you have two features that are identical, one\n",
    "of them will be zero and one of them will be not zero and\n",
    "it's going to be randomly selected. That makes\n",
    "interpretation a little bit harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "- Combines benefits of Ridge and Lasso\n",
    "\n",
    "- two parameters to tune.\n",
    "\n",
    "\n",
    "$$\\min_{w \\in \\mathbb{R}^p, b\\in\\mathbb{R}} \\sum_{i=1}^n ||w^T\\mathbf{x}_i + b - y_i||^2 + \\alpha_1 ||w||_1 +  \\alpha_2 ||w||^2_2 $$\n",
    "\n",
    "\n",
    "\n",
    "You can also combine them. This actually what works best in\n",
    "practice. This is what's called the Elastic Net. Elastic Net\n",
    "tries to combine both of these penalizations together. You\n",
    "now have both terms, you have the L1 norm and the L2 norm\n",
    "and you have different values of alpha. Basically, this\n",
    "generalizes both. If you choose both these are alpha, it can\n",
    "become ridge and it can become Lasso, it can become any\n",
    "anything in between. Generally, ridge helps generalization.\n",
    "So it's a good idea to have the ridge penalty in there, but\n",
    "also maybe if there are some features that are really not\n",
    "useful, the L1 penalty helps makes the same exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing unit balls\n",
    "\n",
    "![:scale 70%](images/l1l2_elasticnet.png)\n",
    "\n",
    "\n",
    "\n",
    "Looking at the unit balls, if you zoom in here you would see\n",
    "the Elastic Net ball is kind of round, but also has a\n",
    "corner. Important things are the corners because that allows\n",
    "you to hit the exact zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrization in scikit-learn\n",
    "$$\\min_{w \\in \\mathbb{R}^p, b\\in\\mathbb{R}} \\sum_{i=1}^n (w^T\\mathbf{x}_i + b - y_i)^2 + \\alpha \\eta ||w||_1 +  \\alpha (1 - \\eta) ||w||^2_2 $$\n",
    "\n",
    "Where $\\eta$ is the relative amount of l1 penalty (`l1_ratio` in the code).\n",
    "\n",
    "\n",
    "The way this parameterize in scikit-learn is slightly\n",
    "different. In scikit-learn, you have a parameter alpha,\n",
    "which is the amount of regularization and then there's a\n",
    "parameter called l1_ratio, that says how much of the penalty\n",
    "should be L1 and L2. If you make this one, you have Lasso,\n",
    "if you make it zero, you have a Ridge. Don't use Lasso or\n",
    "Ridge and set alpha zero, because the solver will not handle\n",
    "it well. If you actually want alpha equal to zero, use\n",
    "linear regression. Now we have more parameters to tune, but\n",
    "we just have a more general model. This actually works\n",
    "pretty well often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-searching ElasticNet\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "param_grid = {'alpha': np.logspace(-4, -1, 10),\n",
    "              'l1_ratio': [0.01, .1, .5, .8, .9, .95, .98, 1]}\n",
    "\n",
    "grid = GridSearchCV(ElasticNet(), param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "```\n",
    "```\n",
    "{'alpha': 0.001, 'l1_ratio': 0.9}\n",
    "0.100\n",
    "```\n",
    "```python\n",
    "(grid.best_estimator_.coef_!= 0).sum()\n",
    "```\n",
    "```\n",
    "10\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Here is me doing a grid search. If you have two parameters\n",
    "for the grid search it will do all possible combinations.\n",
    "Here I do a logarithmic space for alpha and for the\n",
    "l1_ratio, I use something that’s very close to zero and\n",
    "something that's very close to one and some stuff in\n",
    "between. If you want to analyze the output of a 2D grid\n",
    "search a little bit harder we can’t do the nice curve\n",
    "anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing grid-search results\n",
    "```python\n",
    "import pandas as pd\n",
    "res = pd.pivot_table(pd.DataFrame(grid.cv_results_),\n",
    "    values='mean_test_score', index='param_alpha', columns='param_l1_ratio')\n",
    "```\n",
    ".center[\n",
    "![:scale 60%](images/elasticnet_search.png)\n",
    "]\n",
    "\n",
    "\n",
    "The way that I like to do it is, here's the grip.cv results.\n",
    "And I put it in a data frame and then I'll make a pivot\n",
    "table where the values are test score, the index is one\n",
    "parameter and the columns are the other parameter. It allows\n",
    "me to visualize the grid search nicely. This is alpha and\n",
    "this is l1_ratio and you can see that if the l1_ratio is\n",
    "pretty high, there are some pretty good results, if you set\n",
    "the alpha accordingly. So here's like the diagonal of pretty\n",
    "good things. This is the model that did best. There's a\n",
    "slight caveat here that right now I did this with\n",
    "cross-validation and so this is the cross-validation\n",
    "accuracy. Last time I said, this is not really a good\n",
    "measure of generalization performance. So here, I searched\n",
    "way more parameters, I tried like 5 or 10 times as many\n",
    "models. So it's likely that by chance, I'll get better\n",
    "results. I didn't do this here in particular, because the\n",
    "data set is small and very noisy but in practice, if you\n",
    "want to compare models, you should evaluate it on a test set\n",
    "and see which of the models actually are better on the test.\n",
    "One more thing, why this is helpful is if the best value is\n",
    "on the edge of this graph that means my ranges were too\n",
    "small. Question is why we're using r square instead of the\n",
    "squared loss, one of the answers is that's the default in\n",
    "scikit-learn and the other answer is it's nice to know the\n",
    "range so you know that perfect prediction is one and you\n",
    "have some idea of what 0.5 means, the RMSE (the other norm\n",
    "that you usually use is the RMSE) depends on the scale of\n",
    "the output. So for example for the housing prices, it might\n",
    "be interesting to see what is the standard error in terms of\n",
    "dollars. If you want, like something that is in the units of\n",
    "the output, RMSE is good or mean absolute error might even\n",
    "be better. If you want something that is independent of the\n",
    "units of the output r square is pretty good because you know\n",
    "it's going to be between zero and one and it's measure\n",
    "something like the correlation and so if it's like 0.9, you\n",
    "know it’s a pretty good model. If my RMSE is 10,000 I don't\n",
    "know if have a good model or a bad model depends on what the\n",
    "range of the outputs is. The last thing I want to talk about\n",
    "today is this was basically changing the regularization\n",
    "parts. The two most times regularization we looked at is\n",
    "Ridge which is L2 penalty, Lasso which is an L1 penalty and\n",
    "combining two of them which is Elastic Net. So now I want to\n",
    "talk about changing the first part, which was the squared\n",
    "loss of the predictions, basically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Questions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "sklearn.set_config(print_changed_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    if i > 12:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    ax.plot(X[:, i], y, 'o', alpha=.5)\n",
    "    ax.set_title(\"{}: {}\".format(i, boston.feature_names[i]))\n",
    "    ax.set_ylabel(\"MEDV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(LinearRegression(),\n",
    "                        X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(\n",
    "        Ridge(), X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'alpha': np.logspace(-3, 3, 14)}\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(Ridge(), param_grid, cv=10, return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "plt.figure(dpi=200)\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.plot('param_alpha', 'mean_train_score', ax=plt.gca())\n",
    "results.plot('param_alpha', 'mean_test_score', ax=plt.gca())\n",
    "\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, scale\n",
    "# being lazy and not really doing things properly whoops\n",
    "X_poly = PolynomialFeatures(include_bias=False).fit_transform(scale(X))\n",
    "print(X_poly.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(LinearRegression(),\n",
    "                        X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(Ridge(),\n",
    "                        X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(Ridge(), param_grid, cv=10, return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "results.plot('param_alpha', 'mean_train_score', ax=plt.gca())\n",
    "results.plot('param_alpha', 'mean_test_score', ax=plt.gca())\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "plt.scatter(range(X_poly.shape[1]), lr.coef_, c=np.sign(lr.coef_), cmap=\"bwr_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = grid.best_estimator_\n",
    "plt.scatter(range(X_poly.shape[1]), ridge.coef_, c=np.sign(ridge.coef_), cmap=\"bwr_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge100 = Ridge(alpha=100).fit(X_train, y_train)\n",
    "ridge1 = Ridge(alpha=1).fit(X_train, y_train)\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(ridge1.coef_, 'o', label=\"alpha=1\")\n",
    "plt.plot(ridge.coef_, 'o', label=\"alpha=14\")\n",
    "plt.plot(ridge100.coef_, 'o', label=\"alpha=100\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used:\", np.sum(lasso.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Load the diabetes dataset using ``sklearn.datasets.load_diabetes``. Apply ``LinearRegression``, ``Ridge`` and ``Lasso`` and visualize the coefficients. Try polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/linear_models_diabetes.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
