

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Working with highly imbalanced data &#8212; Applied Machine Learning in Python</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Automatic Feature Selection" href="12-feature-selection.html" />
    <link rel="prev" title="Advanced Topics" href="index.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Applied Machine Learning in Python</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../index.html">1. Welcome</a>
  </li>
  <li class="">
    <a href="../00-introduction/00-introduction.html">2. Introduction</a>
  </li>
  <li class="">
    <a href="../01-ml-workflow/00-ml-workflow.html">3. The Machine Learning Workflow</a>
  </li>
  <li class="">
    <a href="../02-supervised-learning/index.html">4. Supervised Learning Algorithms</a>
  </li>
  <li class="">
    <a href="../03-unsupervised-learning/index.html">5. Unsupervised Learning Algorithms</a>
  </li>
  <li class="">
    <a href="../04-model-evaluation/index.html">6. Model Evaluation</a>
  </li>
  <li class="active">
    <a href="index.html">7. Advanced Topics</a>
  <ul class="nav sidenav_l2">
    <li class="active">
      <a href="">7.1 Working with highly imbalanced data</a>
    </li>
    <li class="">
      <a href="12-feature-selection.html">7.2 Automatic Feature Selection</a>
    </li>
    <li class="">
      <a href="13-text-data.html">7.3 Working with Text data</a>
    </li>
    <li class="">
      <a href="14-custom-estimators.html">7.4 Custom Estimators</a>
    </li>
  </ul>
  </li>
</ul>
</nav>
<p class="navbar_footer">Powered by <a href="https://jupyterbook.org">Jupyter Book</a></p>
</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../_sources/05-advanced-topics/11-imbalanced-datasets.ipynb.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.ipynb</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Edit this page -->
        

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
            <div class="dropdown-buttons">
                
                <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/05-advanced-topics/11-imbalanced-datasets.ipynb"><button type="button" class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip" data-placement="left"><img class="binder-button-logo" src="../_static/images/logo_binder.svg" alt="Interact on binder">Binder</button></a>
                
                
                
            </div>
        </div>
        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#recap-on-imbalanced-data" class="nav-link">Recap on imbalanced data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#two-sources-of-imbalance" class="nav-link">Two sources of imbalance</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#why-do-we-care" class="nav-link">Why do we care?</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#changing-thresholds" class="nav-link">Changing Thresholds</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#roc-curve" class="nav-link">Roc Curve</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#remedies-for-the-model" class="nav-link">Remedies for the model</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#mammography-data" class="nav-link">Mammography Data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#id1" class="nav-link">Mammography Data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#basic-approaches" class="nav-link">Basic Approaches</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#sckit-learn-vs-resampling" class="nav-link">Sckit-learn vs resampling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#imbalance-learn" class="nav-link">Imbalance-Learn</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#sampler" class="nav-link">Sampler</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#random-undersampling" class="nav-link">Random Undersampling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#id2" class="nav-link">Random Undersampling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#id3" class="nav-link">]</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#random-oversampling" class="nav-link">Random Oversampling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#id4" class="nav-link">Random Oversampling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#id5" class="nav-link">0.917, 0.585
]</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#curves-for-logreg" class="nav-link">Curves for LogReg</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#curves-for-random-forest" class="nav-link">Curves for Random Forest</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#class-weights" class="nav-link">Class-weights</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#class-weights-in-linear-models" class="nav-link">Class-weights in linear models</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#class-weights-in-trees" class="nav-link">Class weights in trees</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#ensemble-resampling" class="nav-link">Ensemble Resampling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#easy-ensemble-with-imblearn" class="nav-link">Easy Ensemble with imblearn</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#synthetic-sample-generation" class="nav-link">Synthetic Sample Generation</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#synthetic-minority-oversampling-technique-smote" class="nav-link">Synthetic Minority Oversampling Technique (SMOTE)</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#summary" class="nav-link">Summary</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#id6" class="nav-link">Class Weights</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#resampled-ensembles" class="nav-link">Resampled Ensembles</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#exercise" class="nav-link">Exercise</a>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="working-with-highly-imbalanced-data">
<h1>Working with highly imbalanced data<a class="headerlink" href="#working-with-highly-imbalanced-data" title="Permalink to this headline">¶</a></h1>
<p>class: center, middle</p>
<div class="section" id="recap-on-imbalanced-data">
<h2>Recap on imbalanced data<a class="headerlink" href="#recap-on-imbalanced-data" title="Permalink to this headline">¶</a></h2>
<p>class: spacious</p>
</div>
<div class="section" id="two-sources-of-imbalance">
<h2>Two sources of imbalance<a class="headerlink" href="#two-sources-of-imbalance" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Asymmetric cost</p></li>
<li><p>Asymmetric data</p></li>
</ul>
<p>In general, there’s are two ways in which a classification
task can be imbalanced. First one is asymmetric costs. Even
if the probability of class 0 and class 1 are the same, they
might be different like in business costs, or health costs,
or any other kind of cost or benefit associated with making
different kinds of mistakes. The second one is having
asymmetrical data. Meaning that one class is much more
common than the other class.</p>
<p>class: spacious</p>
</div>
<div class="section" id="why-do-we-care">
<h2>Why do we care?<a class="headerlink" href="#why-do-we-care" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Why should cost be symmetric?</p></li>
<li><p>All data is imbalanced</p></li>
<li><p>Detect rare events</p></li>
</ul>
<p>One of these two is true in basically all real world
applications. Usually, both of them are true. There’s no
reason why a false positive and a false negative should have
the same business cost, they’re usually very, very different
things, no matter whether you do ad-click prediction or
whether you do health, the two kinds of mistakes are usually
quite different, and have quite different real world
consequences.</p>
<p>Also, data is always imbalanced, and often very drastically.
In particular if you do diagnosis, or if you do ad clicks-or
marketing….For ad-clicks, I think it’s like below 0.01% of
ads I clicked on, depending on how good doing with your
targeting. So very often, we have very few positives. And so
this is really a topic that is basically all of
classification. So balance classification with balance
costs, is not really something that happens a lot in the
real world.</p>
</div>
<div class="section" id="changing-thresholds">
<h2>Changing Thresholds<a class="headerlink" href="#changing-thresholds" title="Permalink to this headline">¶</a></h2>
<p>.tiny-code[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>          <span class="n">precision</span>   <span class="n">recall</span>  <span class="n">f1</span><span class="o">-</span><span class="n">score</span>  <span class="n">support</span>
<span class="mi">0</span>              <span class="mf">0.91</span>     <span class="mf">0.92</span>      <span class="mf">0.92</span>       <span class="mi">53</span>
<span class="mi">1</span>              <span class="mf">0.96</span>     <span class="mf">0.94</span>      <span class="mf">0.95</span>       <span class="mi">90</span>
<span class="n">avg</span><span class="o">/</span><span class="n">total</span>      <span class="mf">0.94</span>     <span class="mf">0.94</span>      <span class="mf">0.94</span>      <span class="mi">143</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="o">.</span><span class="mi">85</span>

<span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>          <span class="n">precision</span>   <span class="n">recall</span>  <span class="n">f1</span><span class="o">-</span><span class="n">score</span>  <span class="n">support</span>
<span class="mi">0</span>              <span class="mf">0.84</span>     <span class="mf">1.00</span>      <span class="mf">0.91</span>       <span class="mi">53</span>
<span class="mi">1</span>              <span class="mf">1.00</span>     <span class="mf">0.89</span>      <span class="mf">0.94</span>       <span class="mi">90</span>
<span class="n">avg</span><span class="o">/</span><span class="n">total</span>      <span class="mf">0.94</span>     <span class="mf">0.93</span>      <span class="mf">0.93</span>      <span class="mi">143</span>

</pre></div>
</div>
<p>]</p>
<p>So apart from evaluation we talked about one way we could’ve
changed the outcome to take into account which was changing
the threshold of greater probability. So not only taking
into account the predicted class. Assume I have a logistic
regression model and I can either use the predict method
which basically makes the cut off at 0.5 probability of the
positive class, then I can look at the classification
report, which will tell me precision and recall for both the
positive and the negative class. But if I want to increase
recall for class 0 or increase precision for class 1, I can
say only predict things as class 1 where the estimated
probability of class 1 is 0.85. And then I will have only
the ones that I’m very certain predicted as class 1.</p>
<p>If you have given actual a cost function of how much each
mistake costs, you can optimize this threshold.</p>
<p>FIXME new classification report!!</p>
</div>
<div class="section" id="roc-curve">
<h2>Roc Curve<a class="headerlink" href="#roc-curve" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 85%" src="../_images/roc_svc_rf_curve.png" />
]</p>
<p>We also looked at ROC curves, which basically look at all
possible thresholds as you can apply. Either for
probabilistic prediction, or for any sort of continuous
uncertainty estimate.</p>
<p>class: center, middle</p>
<div class="section" id="remedies-for-the-model">
<h3>Remedies for the model<a class="headerlink" href="#remedies-for-the-model" title="Permalink to this headline">¶</a></h3>
<p>Today, I really want to talk about more, how we can change
the model more than just changing the threshold. So how can
we change the building of the model so that it takes into
account the asymmetric costs, or asymmetric data.</p>
</div>
</div>
<div class="section" id="mammography-data">
<h2>Mammography Data<a class="headerlink" href="#mammography-data" title="Permalink to this headline">¶</a></h2>
<p>.smallest[
.left-column[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="c1">## mammography https://www.openml.org/d/310</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_openml</span><span class="p">(</span><span class="s1">&#39;mammography&#39;</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<p>(11183, 6)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="mi">1</span>    <span class="mi">10923</span>
<span class="mi">1</span>       <span class="mi">260</span>
</pre></div>
</div>
<p>]
.right-column[
.center[
<img alt=":scale 100%" src="../_images/mammography_data1.png" />
]
]
.reset-column[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## make y boolean</span>
<span class="c1">## this allows sklearn to determine the positive class more easily</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>]
]</p>
<p>I use this mammography data set, which is very imbalanced.
This is a data set that has many samples, only six features
and it’s very imbalanced.</p>
<p>The datasets are about mammography data, and whether there
are calcium deposits in the breast. They are often mistaken
for cancer, which is why it’s good to detect them. Since its
rigidly low dimensional, we can do a scatter plot. And we
can see that these are much skewed distributions and there’s
really a lot more of one class than the other.</p>
</div>
<div class="section" id="id1">
<h2>Mammography Data<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>0.920, 0.630</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">RandomForestClassifier</span><span class="p">(),</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>0.939, 0.722
]</p>
<p>So as a baseline here is just evaluating logistic regression
and the random forest on this. Actually, I ran it under ROC
curve and average precision. I’ve used a cross-validate
function model selection that allows you to specify multiple
metrics. So I only need to train the model once but I can
look at multiple metrics.</p>
<p>I do a 10 fold cross-validation on the data set and I split
into training and test and so I can look at the scores here.
The scores are dictionary, they give me training and test
scores for all the metrics I specified. And so you can look
at the mean test drug score and the mean test average
precision score. This gives a high AUC and a quite low
average precision.</p>
<p>Here is a second baseline with a random forest doing the
same evaluation with ROC AUC and average precision. We get
slightly higher AUC and quite a bit higher average
precision.</p>
</div>
<div class="section" id="basic-approaches">
<h2>Basic Approaches<a class="headerlink" href="#basic-approaches" title="Permalink to this headline">¶</a></h2>
<p>.left-column[
.center[
<img alt=":scale 100%" src="../_images/basic_approaches.png" />
]
]</p>
<p>.right-column[</p>
<p>Change the training procedure
]</p>
<p>Now we want to change these basic training methods to be
better adapted to this imbalanced dataset. There are
generally two approaches. One is changing the data. And the
other is change the training procedure and how you built the
model. The easier one is to change the data. We can either
add samples to the data, we can remove samples to the data,
or we can do both. Resampling is not possible in
scikit-learn because of some API issues.</p>
<p>class: center, spacious</p>
</div>
<div class="section" id="sckit-learn-vs-resampling">
<h2>Sckit-learn vs resampling<a class="headerlink" href="#sckit-learn-vs-resampling" title="Permalink to this headline">¶</a></h2>
<p><img alt=":scale 55%" src="../_images/pipeline1.png" /></p>
<p>The problem with pipelines, as they’re in scikit-learn right
now is, if you create a pipeline, and you call fit, it’ll
always use the original y and the output of a transformer is
always a transformed x. So we can’t change y. So we can
re-sample the data.</p>
<ul class="simple">
<li><p>The transform method only transforms X</p></li>
<li><p>Pipelines work by chaining transforms</p></li>
<li><p>To resample the data, we need to also change y</p></li>
</ul>
<p>class: spacious</p>
</div>
<div class="section" id="imbalance-learn">
<h2>Imbalance-Learn<a class="headerlink" href="#imbalance-learn" title="Permalink to this headline">¶</a></h2>
<p>http://imbalanced-learn.org</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">imbalanced</span><span class="o">-</span><span class="n">learn</span>
</pre></div>
</div>
<p>Extends <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> API</p>
<p>So we’re going to use imbalance learn, which is an extension
of the scikit-learn API that basically allows us to
resample.</p>
</div>
<div class="section" id="sampler">
<h2>Sampler<a class="headerlink" href="#sampler" title="Permalink to this headline">¶</a></h2>
<p>To resample a data sets, each sampler implements:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>  <span class="n">data_resampled</span><span class="p">,</span> <span class="n">targets_resampled</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
</pre></div>
</div>
<p>Fitting and sampling can also be done in one step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>  <span class="n">data_resampled</span><span class="p">,</span> <span class="n">targets_resampled</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">fit_sample</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
</pre></div>
</div>
<p>–</p>
<p>In Pipelines:
Sampling only done in <code class="docutils literal notranslate"><span class="pre">fit</span></code>!</p>
<p>This extends sampler objects in scikit-learn. The sampler
objects have a sample method which returns resample data and
resample targets.</p>
<p>There’s also pipelines in imbalance learn.  The important
part here is the sampling is only done in fit. So only your
training data will be resampled. But when you do
predictions, you want predictions to be made on the whole
test set, and on the original test set. So you don’t want to
mess with the test set in your evaluation. So resampling is
only done when you’re building a model.</p>
<ul class="simple">
<li><p>Imbalance-learn extends scikit-learn interface with a
“sample” method.</p></li>
<li><p>Imbalance-learn has a custom pipeline that allows
resampling.</p></li>
<li><p>Imbalance-learn: resampling is only performed during fitting</p></li>
<li><p>Warning: not everything in imbalance-learn is multiclass!</p></li>
</ul>
</div>
<div class="section" id="random-undersampling">
<h2>Random Undersampling<a class="headerlink" href="#random-undersampling" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imblearn.under_sampling</span> <span class="kn">import</span> <span class="n">RandomUnderSampler</span>
<span class="n">rus</span> <span class="o">=</span> <span class="n">RandomUnderSampler</span><span class="p">(</span><span class="n">replacement</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">X_train_subsample</span><span class="p">,</span> <span class="n">y_train_subsample</span> <span class="o">=</span> <span class="n">rus</span><span class="o">.</span><span class="n">fit_sample</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train_subsample</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_train_subsample</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">8387</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="p">(</span><span class="mi">390</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="p">[</span><span class="mi">195</span> <span class="mi">195</span><span class="p">]</span>
</pre></div>
</div>
<p>The easiest strategy is randomly undersampling. The default
strategy is to undersample the majority class so that it has
the same size as the minority class, that’s implemented in
the random under sampler. Here, I instantiate to random
under sampler, I set replacement equal to false, which means
sampling without replacement and then I can do fit sample on
the training set. And so the original training set was 8387
samples, the subsample data set is only 390 samples. And you
can see now in the bin count here, the data set is balanced.</p>
<p>Basically, I reduced the majority class randomly to a very
much smaller dataset, which made the majority class the same
size as the minority class. And you can see that the dataset
is 20 times smaller because the dataset was imbalanced. So
building anything on this dataset will be much much faster.
But also keep in mind, we threw away 98% of our data in
doing that.</p>
<ul class="simple">
<li><p>Drop data from the majority class randomly</p></li>
<li><p>Often untill balanced</p></li>
<li><p>Very fast training (data shrinks to 2x minority)</p></li>
<li><p>Loses data !</p></li>
</ul>
</div>
<div class="section" id="id2">
<h2>Random Undersampling<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imblearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span> <span class="k">as</span> <span class="n">make_imb_pipeline</span>

<span class="n">undersample_pipe</span> <span class="o">=</span> <span class="n">make_imb_pipeline</span><span class="p">(</span><span class="n">RandomUnderSampler</span><span class="p">(),</span> <span class="n">LogisticRegressionCV</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">undersample_pipe</span><span class="p">,</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1">## baseline was 0.920, 0.630</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.927</span><span class="p">,</span> <span class="mf">0.527</span>
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h2>]<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">undersample_pipe_rf</span> <span class="o">=</span> <span class="n">make_imb_pipeline</span><span class="p">(</span><span class="n">RandomUnderSampler</span><span class="p">(),</span>
                                        <span class="n">RandomForestClassifier</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">undersample_pipe_rf</span><span class="p">,</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1">## baseline was 0.939, 0.722</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.951</span><span class="p">,</span> <span class="mf">0.629</span>
</pre></div>
</div>
<p>]</p>
<p>Looking at the result, ROC AUC actually improved a little
bit while the average precision decreased a little bit.
Given that we threw away 98% of our data, I think the fact
that the ROC AUC improved is quite remarkable. So this is
clearly still a reasonable model, given one particular
measure, it’s even a better model even though we threw away
most of the data.</p>
<p>Even though it’s very simplistic, it’s actually a viable
strategy that people often use in practice. In particular,
if you have a very big data set, you might not have enough
compute to actually do something on the whole dataset or you
might only do something simple on the whole dataset. But
after you resampled it, you can maybe train a much more
complicated model.</p>
<p>Here by default, this random under simpler makes it balanced
but of course, you could be slightly less extreme, so that
you throw away a little bit fewer data.</p>
<p>We can do the same thing with random forest.  After
computing, the area under the ROC curve actually went up
substantially. Again, it’s quite surprising given that we
used much less data, but the average precision went down
quite a bit.</p>
<ul class="simple">
<li><p>As accurate with fraction of samples!</p></li>
<li><p>Really good for large datasets</p></li>
</ul>
</div>
<div class="section" id="random-oversampling">
<h2>Random Oversampling<a class="headerlink" href="#random-oversampling" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">RandomOverSampler</span>
<span class="n">ros</span> <span class="o">=</span> <span class="n">RandomOverSampler</span><span class="p">()</span>
<span class="n">X_train_oversample</span><span class="p">,</span> <span class="n">y_train_oversample</span> <span class="o">=</span> <span class="n">ros</span><span class="o">.</span><span class="n">fit_sample</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train_oversample</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_train_oversample</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">8387</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="p">(</span><span class="mi">16384</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="p">[</span><span class="mi">8192</span> <span class="mi">8192</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Repeat samples from the minority class randomly</p></li>
<li><p>Often untill balanced</p></li>
<li><p>Much slower (dataset grows to 2x majority)</p></li>
</ul>
</div>
<div class="section" id="id4">
<h2>Random Oversampling<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">oversample_pipe</span> <span class="o">=</span> <span class="n">make_imb_pipeline</span><span class="p">(</span><span class="n">RandomOverSampler</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">oversample_pipe</span><span class="p">,</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1">## baseline was 0.920, 0.630</span>
</pre></div>
</div>
</div>
<div class="section" id="id5">
<h2>0.917, 0.585
]<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">oversample_pipe_rf</span> <span class="o">=</span> <span class="n">make_imb_pipeline</span><span class="p">(</span><span class="n">RandomOverSampler</span><span class="p">(),</span>
                                       <span class="n">RandomForestClassifier</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">oversample_pipe_rf</span><span class="p">,</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1">## baseline was 0.939, 0.722</span>
</pre></div>
</div>
<p>0.926, 0.715
]</p>
<p>The complement of doing random sampling of the data is
random oversampling of data. So in random oversampling, we
do the opposite. We basically resample the training dataset
so that the minority class has the same number of samples as
the majority class.</p>
<p>Given that this dataset was very imbalanced, we nearly
doubled the size of the training dataset. So now everything
will be actually much slower because we have many more
samples. We nearly doubled the dataset size because we
reproduced many copies of the minority class and then we
have a balanced dataset again.</p>
<p>Here, what we did is we just sampled with replacement. We
sampled with replacement about 8000 times from this pool of
195 samples.</p>
<p>Q: Does that mean there are repeat records?</p>
<p>Yeah, most of them are repeated like 40 times on average.</p>
<p>Q: What distribution you’re sampling from?</p>
<p>We’re not sampling IID from the original distribution
because in the original distribution there was a strongly
imbalanced. So we now sample from the traditional
distributions basically where we sample first the label and
then we sample from this class. And we do this for the two
labels independently in the same amount of time. It’s
slightly weird because we have a lot of copies of the
sample.</p>
<p>Logreg the same, Random Forest much worse than undersampling (about same as doing nothing)</p>
</div>
<div class="section" id="curves-for-logreg">
<h2>Curves for LogReg<a class="headerlink" href="#curves-for-logreg" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 100%" src="../_images/curves_logreg.png" />
]</p>
<p>Here, I’m making a pipeline with logistic aggression random
forest and you can see that the area under the ROC curve is
actually lower, while the average precision is higher than
with undersampling but slightly lower than the original
dataset.</p>
</div>
<div class="section" id="curves-for-random-forest">
<h2>Curves for Random Forest<a class="headerlink" href="#curves-for-random-forest" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 100%" src="../_images/curves_rf.png" />
]</p>
<p>We can also look at the curves. We have on the left, the ROC
curve for logistic regression with the original data set,
the oversample, and the under-sample dataset. On the
right-hand side, we have the recall curve.</p>
<p>These are the same for random forest. If you look at these
curves from afar, they give you the exact opposite ideas. On
the left, under sample seems to be best and oversample is
the worst while under sample is clearly the worst and under
sample is not so bad on the curve in the right.</p>
<p>If I look at the precision-recall curve, the original data
set did best. Looking at these two curves you get quite
different ideas. The TPR axis is the same as the recall
axis.</p>
<p>The idea is since we have the cost function, we know which
could be the area under one of these curves, or could be a
particular recall or position value or particular cost
matrix we want to achieve and we want to optimize this. And
we hope that by taking into account the imbalance of the
classes, we can optimize this cost better than just
basically using the IID data set. But precision and false
positive rate measure quite different things.</p>
<p>class: spacious</p>
</div>
<div class="section" id="class-weights">
<h2>Class-weights<a class="headerlink" href="#class-weights" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Instead of repeating samples, re-weight the loss function.</p></li>
<li><p>Works for most models!</p></li>
<li><p>Same effect as over-sampling (though not random), but not as expensive (dataset size the same).</p></li>
</ul>
<p>One way we can make the resampling more efficient is by
using class weights instead of actually resampling. So we
can change our loss function to do the same thing as if you
would resample but under sampling case, we don’t actually
throw away any data and in the oversampling case, we don’t
actually make our computational problem harder by repeating
some of the samples. This works for most models and it’s
pretty simple to do in scikit-learn. Basically, it’s the
same as oversampling in a sense because you’re not throwing
away any data.</p>
<p>class: spacious</p>
</div>
<div class="section" id="class-weights-in-linear-models">
<h2>Class-weights in linear models<a class="headerlink" href="#class-weights-in-linear-models" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">$$\min_{w</span> <span class="pre">\in</span> <span class="pre">ℝ^{p},</span> <span class="pre">b</span> <span class="pre">\in</span> <span class="pre">\mathbb{R}}-C</span> <span class="pre">\sum_{i=1}^n\log(\exp(-y_i(w^T</span> <span class="pre">\textbf{x}_i</span> <span class="pre">+</span> <span class="pre">b</span> <span class="pre">))</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">+</span> <span class="pre">||w||_2^2$$</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">$$\min_{w</span> <span class="pre">\in</span> <span class="pre">ℝ^{p},</span> <span class="pre">b</span> <span class="pre">\in</span> <span class="pre">\mathbb{R}}-C</span> <span class="pre">\sum_{i=1}^n</span> <span class="pre">c_{y_i}</span>&#160; <span class="pre">\log(\exp(-y_i(w^T</span> <span class="pre">\textbf{x}_i</span> <span class="pre">+</span> <span class="pre">b</span> <span class="pre">))</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">+</span> <span class="pre">||w||_2^2$$</span></code></p>
<p>Similar for linear and non-linear SVM</p>
<p>So for the linear model, for example, let’s say we do
logistic regression. Instead of minimizing this problem, for
each class, we have a class weight c_(y_i )  and so the loss
for each sample gets multiplied by this class weight and
usually, they sum to one. This is similar to having a
different penalty C for all of the different classes.</p>
<p>You can see that this is the same as if I repeat each sample
x_i c_(y_i ) many times in each class. So if I set the cost
weight of one class to 2, that would be the same as
repeating each sample in this class twice, only now I don’t
actually have to duplicate any sampling. So this is cheaper
than over sampling but has the same effect.</p>
<p>You can do this for hinge loss and SVMs by just changing the
loss.</p>
</div>
<div class="section" id="class-weights-in-trees">
<h2>Class weights in trees<a class="headerlink" href="#class-weights-in-trees" title="Permalink to this headline">¶</a></h2>
<p>Gini Index:</p>
<p><code class="docutils literal notranslate"><span class="pre">$$H_\text{gini}(X_m)</span> <span class="pre">=</span> <span class="pre">\sum_{k\in\mathcal{Y}}</span> <span class="pre">p_{mk}</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">p_{mk})$$</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">$$H_\text{gini}(X_m)</span> <span class="pre">=</span> <span class="pre">\sum_{k\in\mathcal{Y}}</span> <span class="pre">c_k</span> <span class="pre">p_{mk}</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">p_{mk})$$</span></code></p>
<p>Prediction:</p>
<p>Weighted vote</p>
<p>For trees and all trees based models, you can just change
the splitting criteria. For example, if you have a Gini
index here, and you compute the Gini index for each class or
the cross-entropy for each class, and you have the class
weights in here.</p>
<p>And again, you can see this is the same as replicating the
data points c_k many times. If you want to make predictions,
you can do a weighted vote.</p>
<p>#Using Class-Weights</p>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">),</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1">## baseline was 0.920, 0.630</span>
</pre></div>
</div>
<p>0.918, 0.587</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                               <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">),</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1">## baseline was 0.939, 0.722</span>
</pre></div>
</div>
<p>0.917, 0.701
]</p>
<p>In scikit-learn, all the classifier has a class weight
parameter. You can set them to anything you want, basically,
giving arbitrary integers to the particular classes, there
is normalized sum to one.</p>
<p>Balanced setting means do the same as oversampling so that
the populations have the same size for all the classes. You
can see this has a somewhat similar effect to the
oversampling for only half the computational price.</p>
<p>This is a pretty simple way to try to change the model
towards a positive class.</p>
<p>class: spacious</p>
</div>
<div class="section" id="ensemble-resampling">
<h2>Ensemble Resampling<a class="headerlink" href="#ensemble-resampling" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Random resampling separate for each instance in
an ensemble!</p></li>
<li><p>Chen, Liaw, Breiman: “Using random forest to learn imbalanced data.”</p></li>
<li><p>Paper: “Exploratory Undersampling for Class Imbalance Learning”</p></li>
<li><p>Not in sklearn (yet)</p></li>
<li><p>Easy with imblearn</p></li>
</ul>
<p>There’s something a little bit better that’s called easy
ensembles or resampling within an ensemble. So the idea is
you build an ensemble like bagging classifier, but instead
of doing a bootstrap sample, you can do a random
undersampling into a balance dataset separately for each
classifier in ensemble. Right now, you can only do this with
imbalance learn.</p>
</div>
<div class="section" id="easy-ensemble-with-imblearn">
<h2>Easy Ensemble with imblearn<a class="headerlink" href="#easy-ensemble-with-imblearn" title="Permalink to this headline">¶</a></h2>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">imblearn.ensemble</span> <span class="kn">import</span> <span class="n">BalancedBaggingClassifier</span>

<span class="c1">## from imblearn.ensemble import BalancedRandomForestClassifier</span>
<span class="c1">## resampled_rf = BalancedRandomForestClassifier()</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">resampled_rf</span> <span class="o">=</span> <span class="n">BalancedBaggingClassifier</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">tree</span><span class="p">,</span>
                                         <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">resampled_rf</span><span class="p">,</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1">## baseline was 0.939, 0.722</span>
</pre></div>
</div>
<p>0.957, 0.654</p>
<p>]</p>
<p>For example, I can do a balanced random forest, I’m using a
decision tree as a base classifier with max features as
auto. For classifier, this is the square root of number of
features.</p>
<p>And so this will build 100 estimators. Each estimator will
be basically trained on the random undersample of the data
set. So I do a random under a sample of the data set 100
times and I build a tree on each of them. So this is exactly
as expensive as building a random forest on the undersample
data set, which is pretty cheap because under sampling
throws away most of the data. In fact, we’re not throwing
away as much data, we’re keeping a lot of the data but only
in different trees in the ensemble.</p>
<p>Of all the models that we looked at today, this is the best
one in terms of area under the curve. It’s cheap to do and
it allows you to use a lot of your data in a pretty nice
way. This has been shown to be like pretty competitive in a
bunch of benchmarks.</p>
<p>-As cheap as undersampling, but much better results than anything else!</p>
<p>-Didn’t do anything for Logistic Regression.</p>
<p>class: center, middle</p>
<p><img alt=":scale 100%" src="../_images/roc_vs_pr.png" /></p>
<p>Looking at the curves again, comparing the easy ensemble.
You can see that in the higher recall area, it does quite
well and in the high precision area, it does better than
just undersampling. So remember, this is much much cheaper
than the oversampling by a large amount. And so if we are
anywhere in this area here, it seems to be like a pretty
decent solution.</p>
<p>To explain the difference between easy ensemble and under
sample….In undersampling, I randomly under sample the
dataset once. So then I have a balance dataset 195-195 and I
built a model on this. And in this case, I build a random
forest model. And in the easy ensemble, what I do is for
each tree in the random forest I separately do an
undersampling in 295-95. So they will all have the same
minority class samples, but they all will have different
majority class samples. So in total, I’m looking at more
than 195 samples from the majority class since I under
sample for each tree in a different way. That makes it quite
a bit better.</p>
<p>These are all the randomly sample methods that I want to
talk about. You can see that here, they make quite a big
difference. Depending on where you want to be on this
precision-recall curve, you would choose quite different
methods.</p>
<p>class: center, middle</p>
<div class="section" id="synthetic-sample-generation">
<h3>Synthetic Sample Generation<a class="headerlink" href="#synthetic-sample-generation" title="Permalink to this headline">¶</a></h3>
<p>There are many methods, but the only method that your
interviewer will expect you to know is SMOTE.</p>
</div>
</div>
<div class="section" id="synthetic-minority-oversampling-technique-smote">
<h2>Synthetic Minority Oversampling Technique (SMOTE)<a class="headerlink" href="#synthetic-minority-oversampling-technique-smote" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>Adds synthetic interpolated data to smaller class</p></li>
<li><p>For each sample in minority class:</p>
<p>– Pick random neighbor from k neighbors.</p>
<p>– Pick point on line connecting the two uniformly (or within rectangle)</p>
<p>– Repeat.</p>
</li>
</ul>
<p>I don’t think it’s actually used commonly in practice like
the random oversampling and random undersampling. In
particular, the undersample is great because it makes
everything much faster.</p>
<p>The idea is to add samples to the smaller class. So you
synthetically want to add samples that kind of looks like
the smaller class and this way hopefully, bias the
classifier more towards the smaller class. It’s also
neighbor space so again if you have a very big dataset or if
you’re in a very high dimension this might be slow. So in
very high dimensions not even work that well.</p>
<p>So what we’re doing here is for each sample on the minority
class you pick a random neighbor among the K nearest
neighbors and then you pick a point on the line between the
two.</p>
<ul class="simple">
<li><p>Leads to very large datasets (oversampling)</p></li>
<li><p>Can be combined with undersampling strategies</p></li>
</ul>
<p>class: center, middle</p>
<p>.center[
<img alt=":scale 100%" src="../_images/smote_mammography.png" />
]</p>
<p>This is feature three and feature four from the mammography
dataset. You can see basically that for this dataset it’s
pretty far away, K is three and so these neighbors were
picked and then a bunch of times points was randomly picked
on the line between the two. So basically, you get something
that is sort of connecting all two data points.</p>
<p>This might set of make more sense than just repeating the
sample over and over again. But also, if you’re in high
dimensions, it’s not entirely clear how well this will work.</p>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">smote_pipe</span> <span class="o">=</span> <span class="n">make_imb_pipeline</span><span class="p">(</span><span class="n">SMOTE</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">smote_pipe</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)[[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;test_average_precision&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1">## baseline was 0.920, 0.630</span>
</pre></div>
</div>
<p>0.919, 0.585</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">smote_pipe_rf</span> <span class="o">=</span> <span class="n">make_imb_pipeline</span><span class="p">(</span><span class="n">SMOTE</span><span class="p">(),</span>
                                  <span class="n">RandomForestClassifier</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">smote_pipe_rf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)[[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;test_average_precision&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1">## baseline was 0.939, 0.722</span>
</pre></div>
</div>
<p>0.946, 0.688</p>
<p>]</p>
<p>The results are pretty similar to either the original
dataset or the random sampling.
Performing nearest neighbors, I found 11 to be best.</p>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;smote__k_neighbors&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">31</span><span class="p">]}</span>
<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">smote_pipe_rf</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                      <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;average_precision&quot;</span><span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">search</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>
<span class="n">results</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s2">&quot;param_smote__k_neighbors&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;mean_test_score&quot;</span><span class="p">,</span> <span class="s2">&quot;mean_train_score&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>]
.center[
<img alt=":scale 60%" src="../_images/param_smote_k_neighbors.png" />
]</p>
<p>Also here in these plots, it looks now, there are more
yellow points than purple points that’s because I followed
the purple points first and then the yellow points. Because
if I didn’t, then in the original data, you wouldn’t see any
yellow points.</p>
<p>Again, if you look at the metrics, this doesn’t really make
a big difference.</p>
<p>.center[
<img alt=":scale 100%" src="../_images/smote_k_neighbors.png" />
]</p>
<p>class: center, middle</p>
<p>.center[
<img alt=":scale 100%" src="../_images/roc_vs_pr_smote.png" />
]</p>
<p>class: spacious</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Always check roc_auc and average_precision look at curves</p></li>
<li><p>Undersampling is very fast and can help!</p></li>
<li><p>Undersampling + Ensembles is very powerful!</p></li>
<li><p>Can add synthetic samples with SMOTE</p></li>
</ul>
<p>Always check the ROC AUC and the average precision and look
at the curves. You’ve seen that the different strategies and
different parts of the curves, they behave quite
differently. Undersampling is really fast, and it can
sometimes make things better. So I would always try under
sampling just because maybe your model is slightly worse but
if you get 100 times to speed up, it’s still worth it.</p>
<p>Using undersampling with easy ensembles is as fast as
undersampling, but often gives better results.</p>
<p>People in machine learning research like balance datasets,
but in the real world data sets are never balanced.
Unfortunately, most of the datasets we have come from
machine learning researchers. And so there’s really not a
lot of interesting data sets that are very balanced.</p>
<p>The other thing I could have covered is you can directly
optimize particular metrics. So you can train a model to
optimize precision at K or area under the curve or something
like that, at least for linear model, that’s relatively
straightforward. I’m not sure what the outcome was if
someone tried it with trees. But I think the most common is
just reweighting to samples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="n">sklearn</span><span class="o">.</span><span class="n">set_config</span><span class="p">(</span><span class="n">print_changed_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">DeprecationWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># mammography dataset https://www.openml.org/d/310</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/mammography.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">target</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">!=</span> <span class="s2">&quot;&#39;-1&#39;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">plotting</span><span class="o">.</span><span class="n">scatter_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imblearn.under_sampling</span> <span class="kn">import</span> <span class="n">RandomUnderSampler</span>

<span class="n">rus</span> <span class="o">=</span> <span class="n">RandomUnderSampler</span><span class="p">(</span><span class="n">replacement</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">X_train_subsample</span><span class="p">,</span> <span class="n">y_train_subsample</span> <span class="o">=</span> <span class="n">rus</span><span class="o">.</span><span class="n">fit_sample</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train_subsample</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_train_subsample</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imblearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span> <span class="k">as</span> <span class="n">make_imb_pipeline</span>

<span class="n">undersample_pipe</span> <span class="o">=</span> <span class="n">make_imb_pipeline</span><span class="p">(</span><span class="n">RandomUnderSampler</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">undersample_pipe</span><span class="p">,</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">RandomOverSampler</span>
<span class="n">ros</span> <span class="o">=</span> <span class="n">RandomOverSampler</span><span class="p">()</span>
<span class="n">X_train_oversample</span><span class="p">,</span> <span class="n">y_train_oversample</span> <span class="o">=</span> <span class="n">ros</span><span class="o">.</span><span class="n">fit_sample</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train_oversample</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_train_oversample</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">oversample_pipe</span> <span class="o">=</span> <span class="n">make_imb_pipeline</span><span class="p">(</span><span class="n">RandomOverSampler</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">oversample_pipe</span><span class="p">,</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">undersample_pipe_rf</span> <span class="o">=</span> <span class="n">make_imb_pipeline</span><span class="p">(</span><span class="n">RandomUnderSampler</span><span class="p">(),</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">undersample_pipe_rf</span><span class="p">,</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">oversample_pipe_rf</span> <span class="o">=</span> <span class="n">make_imb_pipeline</span><span class="p">(</span><span class="n">RandomOverSampler</span><span class="p">(),</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">oversample_pipe_rf</span><span class="p">,</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id6">
<h2>Class Weights<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">),</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">),</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="resampled-ensembles">
<h2>Resampled Ensembles<a class="headerlink" href="#resampled-ensembles" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">imblearn.ensemble</span> <span class="kn">import</span> <span class="n">BalancedRandomForestClassifier</span>
<span class="n">resampled_rf</span> <span class="o">=</span> <span class="n">BalancedRandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">resampled_rf</span><span class="p">,</span>
                        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">))</span>
<span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_average_precision&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">),</span> <span class="n">refit</span><span class="o">=</span><span class="s1">&#39;average_precision&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">scoring</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="s1">&#39;average_precision&#39;</span><span class="p">),</span> <span class="n">refit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h2>
<p>Pick two or three of the models and strategies above, run grid-search (optimizing roc_auc or average precision), and
plot the roc curves and PR-curves for these models.</p>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">Advanced Topics</a>
    <a class='right-next' id="next-link" href="12-feature-selection.html" title="next page">Automatic Feature Selection</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Andreas C. Müller<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>