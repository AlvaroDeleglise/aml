{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with highly imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Recap on imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Two sources of imbalance\n",
    "\n",
    "- Asymmetric cost\n",
    "- Asymmetric data\n",
    "\n",
    "\n",
    "\n",
    "In general, there's are two ways in which a classification\n",
    "task can be imbalanced. First one is asymmetric costs. Even\n",
    "if the probability of class 0 and class 1 are the same, they\n",
    "might be different like in business costs, or health costs,\n",
    "or any other kind of cost or benefit associated with making\n",
    "different kinds of mistakes. The second one is having\n",
    "asymmetrical data. Meaning that one class is much more\n",
    "common than the other class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Why do we care?\n",
    "\n",
    "- Why should cost be symmetric?\n",
    "- All data is imbalanced\n",
    "- Detect rare events\n",
    "\n",
    "\n",
    "\n",
    "One of these two is true in basically all real world\n",
    "applications. Usually, both of them are true. There's no\n",
    "reason why a false positive and a false negative should have\n",
    "the same business cost, they’re usually very, very different\n",
    "things, no matter whether you do ad-click prediction or\n",
    "whether you do health, the two kinds of mistakes are usually\n",
    "quite different, and have quite different real world\n",
    "consequences.\n",
    "\n",
    "Also, data is always imbalanced, and often very drastically.\n",
    "In particular if you do diagnosis, or if you do ad clicks-or\n",
    "marketing….For ad-clicks, I think it's like below 0.01% of\n",
    "ads I clicked on, depending on how good doing with your\n",
    "targeting. So very often, we have very few positives. And so\n",
    "this is really a topic that is basically all of\n",
    "classification. So balance classification with balance\n",
    "costs, is not really something that happens a lot in the\n",
    "real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Thresholds\n",
    ".tiny-code[\n",
    "```python\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, stratify=data.target, random_state=0)\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "classification_report(y_test, y_pred)\n",
    "```\n",
    "```\n",
    "          precision   recall  f1-score  support\n",
    "0              0.91     0.92      0.92       53\n",
    "1              0.96     0.94      0.95       90\n",
    "avg/total      0.94     0.94      0.94      143\n",
    "```\n",
    "```python\n",
    "y_pred = lr.predict_proba(X_test)[:, 1] > .85\n",
    "\n",
    "classification_report(y_test, y_pred)\n",
    "```\n",
    "```\n",
    "          precision   recall  f1-score  support\n",
    "0              0.84     1.00      0.91       53\n",
    "1              1.00     0.89      0.94       90\n",
    "avg/total      0.94     0.93      0.93      143\n",
    "\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "So apart from evaluation we talked about one way we could’ve\n",
    "changed the outcome to take into account which was changing\n",
    "the threshold of greater probability. So not only taking\n",
    "into account the predicted class. Assume I have a logistic\n",
    "regression model and I can either use the predict method\n",
    "which basically makes the cut off at 0.5 probability of the\n",
    "positive class, then I can look at the classification\n",
    "report, which will tell me precision and recall for both the\n",
    "positive and the negative class. But if I want to increase\n",
    "recall for class 0 or increase precision for class 1, I can\n",
    "say only predict things as class 1 where the estimated\n",
    "probability of class 1 is 0.85. And then I will have only\n",
    "the ones that I'm very certain predicted as class 1.\n",
    "\n",
    "If you have given actual a cost function of how much each\n",
    "mistake costs, you can optimize this threshold.\n",
    "\n",
    "FIXME new classification report!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roc Curve\n",
    "\n",
    ".center[\n",
    "![:scale 85%](images/roc_svc_rf_curve.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "We also looked at ROC curves, which basically look at all\n",
    "possible thresholds as you can apply. Either for\n",
    "probabilistic prediction, or for any sort of continuous\n",
    "uncertainty estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "### Remedies for the model\n",
    "\n",
    "\n",
    "\n",
    "Today, I really want to talk about more, how we can change\n",
    "the model more than just changing the threshold. So how can\n",
    "we change the building of the model so that it takes into\n",
    "account the asymmetric costs, or asymmetric data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mammography Data\n",
    "\n",
    ".smallest[\n",
    ".left-column[\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "## mammography https://www.openml.org/d/310\n",
    "data = fetch_openml('mammography', as_frame=True)\n",
    "X, y = data.data, data.target\n",
    "X.shape\n",
    "```\n",
    "(11183, 6)\n",
    "```python\n",
    "y.value_counts()\n",
    "```\n",
    "```\n",
    "-1    10923\n",
    "1       260\n",
    "```\n",
    "]\n",
    ".right-column[\n",
    ".center[\n",
    "![:scale 100%](images/mammography_data.png)\n",
    "]\n",
    "]\n",
    ".reset-column[\n",
    "```python\n",
    "## make y boolean\n",
    "## this allows sklearn to determine the positive class more easily\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y == '1', random_state=0)\n",
    "```\n",
    "]\n",
    "]\n",
    "\n",
    "\n",
    "I use this mammography data set, which is very imbalanced.\n",
    "This is a data set that has many samples, only six features\n",
    "and it's very imbalanced.\n",
    "\n",
    "The datasets are about mammography data, and whether there\n",
    "are calcium deposits in the breast. They are often mistaken\n",
    "for cancer, which is why it's good to detect them. Since its\n",
    "rigidly low dimensional, we can do a scatter plot. And we\n",
    "can see that these are much skewed distributions and there's\n",
    "really a lot more of one class than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mammography Data\n",
    "\n",
    ".smaller[\n",
    "```python\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_validate(LogisticRegression(),\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "```\n",
    "0.920, 0.630\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "scores = cross_validate(RandomForestClassifier(),\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "```\n",
    "0.939, 0.722\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So as a baseline here is just evaluating logistic regression\n",
    "and the random forest on this. Actually, I ran it under ROC\n",
    "curve and average precision. I’ve used a cross-validate\n",
    "function model selection that allows you to specify multiple\n",
    "metrics. So I only need to train the model once but I can\n",
    "look at multiple metrics.\n",
    "\n",
    "I do a 10 fold cross-validation on the data set and I split\n",
    "into training and test and so I can look at the scores here.\n",
    "The scores are dictionary, they give me training and test\n",
    "scores for all the metrics I specified. And so you can look\n",
    "at the mean test drug score and the mean test average\n",
    "precision score. This gives a high AUC and a quite low\n",
    "average precision.\n",
    "\n",
    "Here is a second baseline with a random forest doing the\n",
    "same evaluation with ROC AUC and average precision. We get\n",
    "slightly higher AUC and quite a bit higher average\n",
    "precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Approaches\n",
    "\n",
    ".left-column[\n",
    ".center[\n",
    "![:scale 100%](images/basic_approaches.png)\n",
    "]\n",
    "]\n",
    "\n",
    ".right-column[\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Change the training procedure\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Now we want to change these basic training methods to be\n",
    "better adapted to this imbalanced dataset. There are\n",
    "generally two approaches. One is changing the data. And the\n",
    "other is change the training procedure and how you built the\n",
    "model. The easier one is to change the data. We can either\n",
    "add samples to the data, we can remove samples to the data,\n",
    "or we can do both. Resampling is not possible in\n",
    "scikit-learn because of some API issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Sckit-learn vs resampling\n",
    "\n",
    "![:scale 55%](images/pipeline.png)\n",
    "\n",
    "\n",
    "\n",
    "The problem with pipelines, as they're in scikit-learn right\n",
    "now is, if you create a pipeline, and you call fit, it'll\n",
    "always use the original y and the output of a transformer is\n",
    "always a transformed x. So we can't change y. So we can\n",
    "re-sample the data.\n",
    "\n",
    "- The transform method only transforms X\n",
    "- Pipelines work by chaining transforms\n",
    "- To resample the data, we need to also change y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "## Imbalance-Learn\n",
    "\n",
    "http://imbalanced-learn.org\n",
    "\n",
    "```\n",
    "pip install -U imbalanced-learn\n",
    "```\n",
    "\n",
    "Extends `sklearn` API\n",
    "\n",
    "\n",
    "So we're going to use imbalance learn, which is an extension\n",
    "of the scikit-learn API that basically allows us to\n",
    "resample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler\n",
    "\n",
    "To resample a data sets, each sampler implements:\n",
    "```python\n",
    "  data_resampled, targets_resampled = obj.sample(data, targets)\n",
    "```\n",
    "Fitting and sampling can also be done in one step:\n",
    "```python\n",
    "  data_resampled, targets_resampled = obj.fit_sample(data, targets)\n",
    "```\n",
    "--\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In Pipelines:\n",
    "Sampling only done in `fit`!\n",
    "\n",
    "\n",
    "\n",
    "This extends sampler objects in scikit-learn. The sampler\n",
    "objects have a sample method which returns resample data and\n",
    "resample targets.\n",
    "\n",
    "There’s also pipelines in imbalance learn.  The important\n",
    "part here is the sampling is only done in fit. So only your\n",
    "training data will be resampled. But when you do\n",
    "predictions, you want predictions to be made on the whole\n",
    "test set, and on the original test set. So you don't want to\n",
    "mess with the test set in your evaluation. So resampling is\n",
    "only done when you're building a model.\n",
    "\n",
    "\n",
    "\n",
    "- Imbalance-learn extends scikit-learn interface with a\n",
    "“sample” method.\n",
    "- Imbalance-learn has a custom pipeline that allows\n",
    "resampling.\n",
    "- Imbalance-learn: resampling is only performed during fitting\n",
    "- Warning: not everything in imbalance-learn is multiclass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Undersampling\n",
    "\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(replacement=False)\n",
    "X_train_subsample, y_train_subsample = rus.fit_sample(\n",
    "    X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(X_train_subsample.shape)\n",
    "print(np.bincount(y_train_subsample))\n",
    "```\n",
    "```\n",
    "(8387, 6)\n",
    "(390, 6)\n",
    "[195 195]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The easiest strategy is randomly undersampling. The default\n",
    "strategy is to undersample the majority class so that it has\n",
    "the same size as the minority class, that's implemented in\n",
    "the random under sampler. Here, I instantiate to random\n",
    "under sampler, I set replacement equal to false, which means\n",
    "sampling without replacement and then I can do fit sample on\n",
    "the training set. And so the original training set was 8387\n",
    "samples, the subsample data set is only 390 samples. And you\n",
    "can see now in the bin count here, the data set is balanced.\n",
    "\n",
    "\n",
    "Basically, I reduced the majority class randomly to a very\n",
    "much smaller dataset, which made the majority class the same\n",
    "size as the minority class. And you can see that the dataset\n",
    "is 20 times smaller because the dataset was imbalanced. So\n",
    "building anything on this dataset will be much much faster.\n",
    "But also keep in mind, we threw away 98% of our data in\n",
    "doing that.\n",
    "\n",
    "\n",
    "- Drop data from the majority class randomly\n",
    "- Often untill balanced\n",
    "- Very fast training (data shrinks to 2x minority)\n",
    "- Loses data !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Undersampling\n",
    "\n",
    ".smaller[\n",
    "```python\n",
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "\n",
    "undersample_pipe = make_imb_pipeline(RandomUnderSampler(), LogisticRegressionCV())\n",
    "scores = cross_validate(undersample_pipe,\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "## baseline was 0.920, 0.630\n",
    "```\n",
    "```\n",
    "0.927, 0.527\n",
    "```\n",
    "]\n",
    "--\n",
    ".smaller[\n",
    "```python\n",
    "undersample_pipe_rf = make_imb_pipeline(RandomUnderSampler(),\n",
    "                                        RandomForestClassifier())\n",
    "scores = cross_validate(undersample_pipe_rf,\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "## baseline was 0.939, 0.722\n",
    "```\n",
    "```\n",
    "0.951, 0.629\n",
    "```\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Looking at the result, ROC AUC actually improved a little\n",
    "bit while the average precision decreased a little bit.\n",
    "Given that we threw away 98% of our data, I think the fact\n",
    "that the ROC AUC improved is quite remarkable. So this is\n",
    "clearly still a reasonable model, given one particular\n",
    "measure, it’s even a better model even though we threw away\n",
    "most of the data.\n",
    "\n",
    "Even though it's very simplistic, it's actually a viable\n",
    "strategy that people often use in practice. In particular,\n",
    "if you have a very big data set, you might not have enough\n",
    "compute to actually do something on the whole dataset or you\n",
    "might only do something simple on the whole dataset. But\n",
    "after you resampled it, you can maybe train a much more\n",
    "complicated model.\n",
    "\n",
    "Here by default, this random under simpler makes it balanced\n",
    "but of course, you could be slightly less extreme, so that\n",
    "you throw away a little bit fewer data.\n",
    "\n",
    "We can do the same thing with random forest.  After\n",
    "computing, the area under the ROC curve actually went up\n",
    "substantially. Again, it's quite surprising given that we\n",
    "used much less data, but the average precision went down\n",
    "quite a bit.\n",
    "\n",
    "\n",
    "- As accurate with fraction of samples!\n",
    "- Really good for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Oversampling\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "X_train_oversample, y_train_oversample = ros.fit_sample(\n",
    "    X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(X_train_oversample.shape)\n",
    "print(np.bincount(y_train_oversample))\n",
    "```\n",
    "```\n",
    "(8387, 6)\n",
    "(16384, 6)\n",
    "[8192 8192]\n",
    "```\n",
    "\n",
    "\n",
    "- Repeat samples from the minority class randomly\n",
    "- Often untill balanced\n",
    "- Much slower (dataset grows to 2x majority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Oversampling\n",
    "\n",
    ".smaller[\n",
    "```python\n",
    "oversample_pipe = make_imb_pipeline(RandomOverSampler(), LogisticRegression())\n",
    "scores = cross_validate(oversample_pipe,\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "## baseline was 0.920, 0.630\n",
    "```\n",
    "0.917, 0.585\n",
    "]\n",
    "--\n",
    ".smaller[\n",
    "```python\n",
    "oversample_pipe_rf = make_imb_pipeline(RandomOverSampler(),\n",
    "                                       RandomForestClassifier())\n",
    "scores = cross_validate(oversample_pipe_rf,\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "## baseline was 0.939, 0.722\n",
    "```\n",
    "0.926, 0.715\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "The complement of doing random sampling of the data is\n",
    "random oversampling of data. So in random oversampling, we\n",
    "do the opposite. We basically resample the training dataset\n",
    "so that the minority class has the same number of samples as\n",
    "the majority class.\n",
    "\n",
    "Given that this dataset was very imbalanced, we nearly\n",
    "doubled the size of the training dataset. So now everything\n",
    "will be actually much slower because we have many more\n",
    "samples. We nearly doubled the dataset size because we\n",
    "reproduced many copies of the minority class and then we\n",
    "have a balanced dataset again.\n",
    "\n",
    "Here, what we did is we just sampled with replacement. We\n",
    "sampled with replacement about 8000 times from this pool of\n",
    "195 samples.\n",
    "\n",
    "Q: Does that mean there are repeat records?\n",
    "\n",
    "Yeah, most of them are repeated like 40 times on average.\n",
    "\n",
    "Q: What distribution you’re sampling from?\n",
    "\n",
    "We're not sampling IID from the original distribution\n",
    "because in the original distribution there was a strongly\n",
    "imbalanced. So we now sample from the traditional\n",
    "distributions basically where we sample first the label and\n",
    "then we sample from this class. And we do this for the two\n",
    "labels independently in the same amount of time. It's\n",
    "slightly weird because we have a lot of copies of the\n",
    "sample.\n",
    "\n",
    "\n",
    "\n",
    "Logreg the same, Random Forest much worse than undersampling (about same as doing nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curves for LogReg\n",
    "\n",
    ".center[\n",
    "![:scale 100%](images/curves_logreg.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Here, I’m making a pipeline with logistic aggression random\n",
    "forest and you can see that the area under the ROC curve is\n",
    "actually lower, while the average precision is higher than\n",
    "with undersampling but slightly lower than the original\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curves for Random Forest\n",
    "\n",
    ".center[\n",
    "![:scale 100%](images/curves_rf.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "We can also look at the curves. We have on the left, the ROC\n",
    "curve for logistic regression with the original data set,\n",
    "the oversample, and the under-sample dataset. On the\n",
    "right-hand side, we have the recall curve.\n",
    "\n",
    "These are the same for random forest. If you look at these\n",
    "curves from afar, they give you the exact opposite ideas. On\n",
    "the left, under sample seems to be best and oversample is\n",
    "the worst while under sample is clearly the worst and under\n",
    "sample is not so bad on the curve in the right.\n",
    "\n",
    "If I look at the precision-recall curve, the original data\n",
    "set did best. Looking at these two curves you get quite\n",
    "different ideas. The TPR axis is the same as the recall\n",
    "axis.\n",
    "\n",
    "The idea is since we have the cost function, we know which\n",
    "could be the area under one of these curves, or could be a\n",
    "particular recall or position value or particular cost\n",
    "matrix we want to achieve and we want to optimize this. And\n",
    "we hope that by taking into account the imbalance of the\n",
    "classes, we can optimize this cost better than just\n",
    "basically using the IID data set. But precision and false\n",
    "positive rate measure quite different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Class-weights\n",
    "\n",
    "- Instead of repeating samples, re-weight the loss function.\n",
    "- Works for most models!\n",
    "- Same effect as over-sampling (though not random), but not as expensive (dataset size the same).\n",
    "\n",
    "\n",
    "\n",
    "One way we can make the resampling more efficient is by\n",
    "using class weights instead of actually resampling. So we\n",
    "can change our loss function to do the same thing as if you\n",
    "would resample but under sampling case, we don't actually\n",
    "throw away any data and in the oversampling case, we don't\n",
    "actually make our computational problem harder by repeating\n",
    "some of the samples. This works for most models and it's\n",
    "pretty simple to do in scikit-learn. Basically, it's the\n",
    "same as oversampling in a sense because you're not throwing\n",
    "away any data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Class-weights in linear models\n",
    "\n",
    "$$\\min_{w \\in ℝ^{p}, b \\in \\mathbb{R}}-C \\sum_{i=1}^n\\log(\\exp(-y_i(w^T \\textbf{x}_i + b )) + 1) + ||w||_2^2$$\n",
    "\n",
    "$$\\min_{w \\in ℝ^{p}, b \\in \\mathbb{R}}-C \\sum_{i=1}^n c_{y_i}  \\log(\\exp(-y_i(w^T \\textbf{x}_i + b )) + 1) + ||w||_2^2$$\n",
    "\n",
    "Similar for linear and non-linear SVM\n",
    "\n",
    "\n",
    "\n",
    "So for the linear model, for example, let's say we do\n",
    "logistic regression. Instead of minimizing this problem, for\n",
    "each class, we have a class weight c_(y_i )  and so the loss\n",
    "for each sample gets multiplied by this class weight and\n",
    "usually, they sum to one. This is similar to having a\n",
    "different penalty C for all of the different classes.\n",
    "\n",
    "You can see that this is the same as if I repeat each sample\n",
    "x_i c_(y_i ) many times in each class. So if I set the cost\n",
    "weight of one class to 2, that would be the same as\n",
    "repeating each sample in this class twice, only now I don't\n",
    "actually have to duplicate any sampling. So this is cheaper\n",
    "than over sampling but has the same effect.\n",
    "\n",
    "You can do this for hinge loss and SVMs by just changing the\n",
    "loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class weights in trees\n",
    "\n",
    "Gini Index:\n",
    "\n",
    "$$H_\\text{gini}(X_m) = \\sum_{k\\in\\mathcal{Y}} p_{mk} (1 - p_{mk})$$\n",
    "\n",
    "$$H_\\text{gini}(X_m) = \\sum_{k\\in\\mathcal{Y}} c_k p_{mk} (1 - p_{mk})$$\n",
    "\n",
    "Prediction:\n",
    "\n",
    "Weighted vote\n",
    "\n",
    "\n",
    "\n",
    "For trees and all trees based models, you can just change\n",
    "the splitting criteria. For example, if you have a Gini\n",
    "index here, and you compute the Gini index for each class or\n",
    "the cross-entropy for each class, and you have the class\n",
    "weights in here.\n",
    "\n",
    "And again, you can see this is the same as replicating the\n",
    "data points c_k many times. If you want to make predictions,\n",
    "you can do a weighted vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Using Class-Weights\n",
    "\n",
    ".smaller[\n",
    "```python\n",
    "scores = cross_validate(LogisticRegression(class_weight='balanced'),\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "## baseline was 0.920, 0.630\n",
    "```\n",
    "0.918, 0.587\n",
    "```python\n",
    "scores = cross_validate(RandomForestClassifier(n_estimators=100,\n",
    "                                               class_weight='balanced'),\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "## baseline was 0.939, 0.722\n",
    "```\n",
    "0.917, 0.701\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "In scikit-learn, all the classifier has a class weight\n",
    "parameter. You can set them to anything you want, basically,\n",
    "giving arbitrary integers to the particular classes, there\n",
    "is normalized sum to one.\n",
    "\n",
    "Balanced setting means do the same as oversampling so that\n",
    "the populations have the same size for all the classes. You\n",
    "can see this has a somewhat similar effect to the\n",
    "oversampling for only half the computational price.\n",
    "\n",
    "This is a pretty simple way to try to change the model\n",
    "towards a positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Ensemble Resampling\n",
    "\n",
    "- Random resampling separate for each instance in\n",
    "an ensemble!\n",
    "- Chen, Liaw, Breiman: “Using random forest to learn imbalanced data.”\n",
    "- Paper: “Exploratory Undersampling for Class Imbalance Learning”\n",
    "- Not in sklearn (yet)\n",
    "- Easy with imblearn\n",
    "\n",
    "\n",
    "There’s something a little bit better that’s called easy\n",
    "ensembles or resampling within an ensemble. So the idea is\n",
    "you build an ensemble like bagging classifier, but instead\n",
    "of doing a bootstrap sample, you can do a random\n",
    "undersampling into a balance dataset separately for each\n",
    "classifier in ensemble. Right now, you can only do this with\n",
    "imbalance learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Ensemble with imblearn\n",
    "\n",
    ".smaller[\n",
    "```python\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "## from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "## resampled_rf = BalancedRandomForestClassifier()\n",
    "\n",
    "tree = DecisionTreeClassifier(max_features='auto')\n",
    "resampled_rf = BalancedBaggingClassifier(base_estimator=tree,\n",
    "                                         random_state=0)\n",
    "scores = cross_validate(resampled_rf,\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "## baseline was 0.939, 0.722\n",
    "```\n",
    "0.957, 0.654\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "For example, I can do a balanced random forest, I’m using a\n",
    "decision tree as a base classifier with max features as\n",
    "auto. For classifier, this is the square root of number of\n",
    "features.\n",
    "\n",
    "And so this will build 100 estimators. Each estimator will\n",
    "be basically trained on the random undersample of the data\n",
    "set. So I do a random under a sample of the data set 100\n",
    "times and I build a tree on each of them. So this is exactly\n",
    "as expensive as building a random forest on the undersample\n",
    "data set, which is pretty cheap because under sampling\n",
    "throws away most of the data. In fact, we're not throwing\n",
    "away as much data, we're keeping a lot of the data but only\n",
    "in different trees in the ensemble.\n",
    "\n",
    "Of all the models that we looked at today, this is the best\n",
    "one in terms of area under the curve. It’s cheap to do and\n",
    "it allows you to use a lot of your data in a pretty nice\n",
    "way. This has been shown to be like pretty competitive in a\n",
    "bunch of benchmarks.\n",
    "\n",
    "\n",
    "\n",
    "-As cheap as undersampling, but much better results than anything else! \n",
    "\n",
    "-Didn't do anything for Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "![:scale 100%](images/roc_vs_pr.png)\n",
    "\n",
    "\n",
    "Looking at the curves again, comparing the easy ensemble.\n",
    "You can see that in the higher recall area, it does quite\n",
    "well and in the high precision area, it does better than\n",
    "just undersampling. So remember, this is much much cheaper\n",
    "than the oversampling by a large amount. And so if we are\n",
    "anywhere in this area here, it seems to be like a pretty\n",
    "decent solution.\n",
    "\n",
    "To explain the difference between easy ensemble and under\n",
    "sample….In undersampling, I randomly under sample the\n",
    "dataset once. So then I have a balance dataset 195-195 and I\n",
    "built a model on this. And in this case, I build a random\n",
    "forest model. And in the easy ensemble, what I do is for\n",
    "each tree in the random forest I separately do an\n",
    "undersampling in 295-95. So they will all have the same\n",
    "minority class samples, but they all will have different\n",
    "majority class samples. So in total, I'm looking at more\n",
    "than 195 samples from the majority class since I under\n",
    "sample for each tree in a different way. That makes it quite\n",
    "a bit better.\n",
    "\n",
    "These are all the randomly sample methods that I want to\n",
    "talk about. You can see that here, they make quite a big\n",
    "difference. Depending on where you want to be on this\n",
    "precision-recall curve, you would choose quite different\n",
    "methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "### Synthetic Sample Generation\n",
    "\n",
    "\n",
    "\n",
    "There are many methods, but the only method that your\n",
    "interviewer will expect you to know is SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "- Adds synthetic interpolated data to smaller class\n",
    "- For each sample in minority class:\n",
    "\n",
    "  – Pick random neighbor from k neighbors.\n",
    "\n",
    "  – Pick point on line connecting the two uniformly (or within rectangle)\n",
    "\n",
    "  – Repeat.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I don't think it's actually used commonly in practice like\n",
    "the random oversampling and random undersampling. In\n",
    "particular, the undersample is great because it makes\n",
    "everything much faster.\n",
    "\n",
    "The idea is to add samples to the smaller class. So you\n",
    "synthetically want to add samples that kind of looks like\n",
    "the smaller class and this way hopefully, bias the\n",
    "classifier more towards the smaller class. It's also\n",
    "neighbor space so again if you have a very big dataset or if\n",
    "you’re in a very high dimension this might be slow. So in\n",
    "very high dimensions not even work that well.\n",
    "\n",
    "So what we're doing here is for each sample on the minority\n",
    "class you pick a random neighbor among the K nearest\n",
    "neighbors and then you pick a point on the line between the\n",
    "two.\n",
    "\n",
    "\n",
    "- Leads to very large datasets (oversampling)\n",
    "- Can be combined with undersampling strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    ".center[\n",
    "![:scale 100%](images/smote_mammography.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This is feature three and feature four from the mammography\n",
    "dataset. You can see basically that for this dataset it’s\n",
    "pretty far away, K is three and so these neighbors were\n",
    "picked and then a bunch of times points was randomly picked\n",
    "on the line between the two. So basically, you get something\n",
    "that is sort of connecting all two data points.\n",
    "\n",
    "This might set of make more sense than just repeating the\n",
    "sample over and over again. But also, if you're in high\n",
    "dimensions, it's not entirely clear how well this will work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".smaller[\n",
    "```python\n",
    "smote_pipe = make_imb_pipeline(SMOTE(), LogisticRegression())\n",
    "scores = cross_validate(smote_pipe, X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "pd.DataFrame(scores)[['test_roc_auc', 'test_average_precision']].mean()\n",
    "## baseline was 0.920, 0.630\n",
    "```\n",
    "0.919, 0.585\n",
    "\n",
    "```python\n",
    "smote_pipe_rf = make_imb_pipeline(SMOTE(),\n",
    "                                  RandomForestClassifier())\n",
    "scores = cross_validate(smote_pipe_rf, X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "pd.DataFrame(scores)[['test_roc_auc', 'test_average_precision']].mean()\n",
    "## baseline was 0.939, 0.722\n",
    "```\n",
    "0.946, 0.688\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "The results are pretty similar to either the original\n",
    "dataset or the random sampling.\n",
    "Performing nearest neighbors, I found 11 to be best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".smaller[\n",
    "```python\n",
    "param_grid = {'smote__k_neighbors': [3, 5, 7, 9, 11, 15, 31]}\n",
    "search = GridSearchCV(smote_pipe_rf, param_grid, cv=10,\n",
    "                      scoring=\"average_precision\")\n",
    "search.fit(X_train, y_train)\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "results.plot(\"param_smote__k_neighbors\", [\"mean_test_score\", \"mean_train_score\"])\n",
    "```\n",
    "]\n",
    ".center[\n",
    "![:scale 60%](images/param_smote_k_neighbors.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Also here in these plots, it looks now, there are more\n",
    "yellow points than purple points that's because I followed\n",
    "the purple points first and then the yellow points. Because\n",
    "if I didn't, then in the original data, you wouldn't see any\n",
    "yellow points.\n",
    "\n",
    "Again, if you look at the metrics, this doesn't really make\n",
    "a big difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 100%](images/smote_k_neighbors.png)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    ".center[\n",
    "![:scale 100%](images/roc_vs_pr_smote.png)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "\n",
    "## Summary\n",
    "\n",
    "- Always check roc_auc and average_precision look at curves\n",
    "- Undersampling is very fast and can help!\n",
    "- Undersampling + Ensembles is very powerful!\n",
    "- Can add synthetic samples with SMOTE\n",
    "\n",
    "\n",
    "\n",
    "Always check the ROC AUC and the average precision and look\n",
    "at the curves. You’ve seen that the different strategies and\n",
    "different parts of the curves, they behave quite\n",
    "differently. Undersampling is really fast, and it can\n",
    "sometimes make things better. So I would always try under\n",
    "sampling just because maybe your model is slightly worse but\n",
    "if you get 100 times to speed up, it's still worth it.\n",
    "\n",
    "Using undersampling with easy ensembles is as fast as\n",
    "undersampling, but often gives better results.\n",
    "\n",
    "\n",
    "People in machine learning research like balance datasets,\n",
    "but in the real world data sets are never balanced.\n",
    "Unfortunately, most of the datasets we have come from\n",
    "machine learning researchers. And so there's really not a\n",
    "lot of interesting data sets that are very balanced.\n",
    "\n",
    "The other thing I could have covered is you can directly\n",
    "optimize particular metrics. So you can train a model to\n",
    "optimize precision at K or area under the curve or something\n",
    "like that, at least for linear model, that's relatively\n",
    "straightforward. I'm not sure what the outcome was if\n",
    "someone tried it with trees. But I think the most common is\n",
    "just reweighting to samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "sklearn.set_config(print_changed_only=True)\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mammography dataset https://www.openml.org/d/310\n",
    "data = pd.read_csv(\"data/mammography.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (target != \"'-1'\").astype(np.int)\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.hist(bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(X, c=y, alpha=.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.values, y.values, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_validate(LogisticRegression(),\n",
    "                        X_train, y_train, cv=10, scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "scores = cross_validate(RandomForestClassifier(n_estimators=100),\n",
    "                        X_train, y_train, cv=10, scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(replacement=False)\n",
    "X_train_subsample, y_train_subsample = rus.fit_sample(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(X_train_subsample.shape)\n",
    "print(np.bincount(y_train_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "\n",
    "undersample_pipe = make_imb_pipeline(RandomUnderSampler(), LogisticRegression())\n",
    "scores = cross_validate(undersample_pipe,\n",
    "                        X_train, y_train, cv=10, scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "X_train_oversample, y_train_oversample = ros.fit_sample(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(X_train_oversample.shape)\n",
    "print(np.bincount(y_train_oversample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample_pipe = make_imb_pipeline(RandomOverSampler(), LogisticRegression())\n",
    "scores = cross_validate(oversample_pipe,\n",
    "                        X_train, y_train, cv=10, scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample_pipe_rf = make_imb_pipeline(RandomUnderSampler(), RandomForestClassifier(n_estimators=100))\n",
    "scores = cross_validate(undersample_pipe_rf,\n",
    "                        X_train, y_train, cv=10, scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample_pipe_rf = make_imb_pipeline(RandomOverSampler(), RandomForestClassifier(n_estimators=100))\n",
    "scores = cross_validate(oversample_pipe_rf,\n",
    "                        X_train, y_train, cv=10, scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(LogisticRegression(class_weight='balanced'),\n",
    "                        X_train, y_train, cv=10, scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(RandomForestClassifier(n_estimators=100, class_weight='balanced'),\n",
    "                        X_train, y_train, cv=10, scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampled Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "resampled_rf = BalancedRandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(resampled_rf,\n",
    "                        X_train, y_train, cv=10, scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV(scoring=('roc_auc', 'average_precision'), refit='average_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV(scoring=('roc_auc', 'average_precision'), refit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Pick two or three of the models and strategies above, run grid-search (optimizing roc_auc or average precision), and\n",
    "plot the roc curves and PR-curves for these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
