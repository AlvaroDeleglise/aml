

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Working with Text data &#8212; Applied Machine Learning in Python</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Custom Estimators" href="14-custom-estimators.html" />
    <link rel="prev" title="Automatic Feature Selection" href="12-feature-selection.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Applied Machine Learning in Python</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../index.html">1. Welcome</a>
  </li>
  <li class="">
    <a href="../00-introduction/00-introduction.html">2. Introduction</a>
  </li>
  <li class="">
    <a href="../01-ml-workflow/00-ml-workflow.html">3. The Machine Learning Workflow</a>
  </li>
  <li class="">
    <a href="../02-supervised-learning/index.html">4. Supervised Learning Algorithms</a>
  </li>
  <li class="">
    <a href="../03-unsupervised-learning/index.html">5. Unsupervised Learning Algorithms</a>
  </li>
  <li class="">
    <a href="../04-model-evaluation/index.html">6. Model Evaluation</a>
  </li>
  <li class="active">
    <a href="index.html">7. Advanced Topics</a>
  <ul class="nav sidenav_l2">
    <li class="">
      <a href="11-imbalanced-datasets.html">7.1 Working with highly imbalanced data</a>
    </li>
    <li class="">
      <a href="12-feature-selection.html">7.2 Automatic Feature Selection</a>
    </li>
    <li class="active">
      <a href="">7.3 Working with Text data</a>
    </li>
    <li class="">
      <a href="14-custom-estimators.html">7.4 Custom Estimators</a>
    </li>
  </ul>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../_sources/05-advanced-topics/13-text-data.ipynb.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.ipynb</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Edit this page -->
        

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
            <div class="dropdown-buttons">
                
                <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/05-advanced-topics/13-text-data.ipynb"><button type="button" class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip" data-placement="left"><img class="binder-button-logo" src="../_static/images/logo_binder.svg" alt="Interact on binder">Binder</button></a>
                
                
                
            </div>
        </div>
        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#more-kinds-of-data" class="nav-link">More kinds of data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#typical-text-data" class="nav-link">Typical Text Data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#other-types-of-text-data" class="nav-link">Other Types of text data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#bag-of-words" class="nav-link">Bag of Words</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#toy-example" class="nav-link">Toy Example</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#bag" class="nav-link">“bag”</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#text-classification-example" class="nav-link">Text classification example:</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#imdb-movie-reviews" class="nav-link">IMDB Movie Reviews</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#data-loading" class="nav-link">Data loading</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#id1" class="nav-link">Data loading</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#vectorization" class="nav-link">Vectorization</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#vocabulary" class="nav-link">Vocabulary</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#classification" class="nav-link">Classification</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#soo-many-options" class="nav-link">Soo many options!</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#tokenization" class="nav-link">Tokenization</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#changing-the-token-pattern-regex" class="nav-link">Changing the token pattern regex</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#our-meeting-today-be-bad-than-yesterday-i-be-scar-of-meet-the-client-tomorrow" class="nav-link">['our', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', 'i',  'be', 'scar', 'of', 'meet', 'the', 'client', 'tomorrow', '.']
]</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#restricting-the-vocabulary" class="nav-link">Restricting the Vocabulary</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#stop-words" class="nav-link">Stop Words</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#infrequent-words" class="nav-link">Infrequent Words</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#tf-idf-rescaling" class="nav-link">Tf-idf rescaling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#tfidfvectorizer-tfidftransformer" class="nav-link">TfidfVectorizer, TfidfTransformer</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#n-grams-beyond-single-words" class="nav-link">N-grams: Beyond single words</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#bigrams-toy-example" class="nav-link">Bigrams toy example</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#n-grams-on-imdb-data" class="nav-link">N-grams on IMDB data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#stop-word-impact-on-4-grams" class="nav-link">Stop-word impact on 4-grams</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#character-n-grams" class="nav-link">Character n-grams</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#id2" class="nav-link">Toy example</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#imdb-data" class="nav-link">IMDB Data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#predicting-nationality-from-name" class="nav-link">Predicting Nationality from Name</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#comparing-words-vs-chars" class="nav-link">Comparing words vs chars</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#grid-search-parameters" class="nav-link">Grid-search parameters</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#other-features" class="nav-link">Other features</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#large-scale-text-vectorization" class="nav-link">Large Scale Text Vectorization</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#trade-offs" class="nav-link">Trade-offs</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#near-drop-in-replacement" class="nav-link">Near drop-in replacement</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#other-libraries" class="nav-link">Other libraries</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#nltk" class="nav-link">nltk</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#spacy" class="nav-link">spaCy</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#gensim" class="nav-link">gensim</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h4">
            <a href="#w4995-applied-machine-learning" class="nav-link">W4995 Applied Machine Learning</a>
        </li>
    
            </ul>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#lsa-topic-models" class="nav-link">LSA & Topic Models</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#beyond-bags-of-words" class="nav-link">Beyond Bags of Words</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#topic-models" class="nav-link">Topic Models</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#motivation" class="nav-link">Motivation</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#latent-semantic-analysis-lsa" class="nav-link">Latent Semantic Analysis (LSA)</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#lsa-with-truncated-svd" class="nav-link">LSA with Truncated SVD</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#first-six-eigenvectors" class="nav-link">First Six eigenvectors</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#scale-before-lsa" class="nav-link">Scale before LSA</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#eigenvectors-after-scaling" class="nav-link">Eigenvectors after scaling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#some-components-capture-sentiment" class="nav-link">Some Components Capture Sentiment</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#nmf-for-topic-models" class="nav-link">NMF for topic models</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#nmf-loss-and-algorithm" class="nav-link">NMF Loss and algorithm</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#s-t-h-w-all-entries-positive" class="nav-link">s.t. H, W all entries positive</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#id3" class="nav-link">NMF for topic models</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#nmf-on-scaled-data" class="nav-link">NMF on Scaled Data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#nmf-components-without-scaling" class="nav-link">NMF components without scaling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#nmf-with-tfidf" class="nav-link">NMF with tfidf</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#nmf-with-tfidf-and-10-components" class="nav-link">NMF with tfidf and 10 components</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#the-other-lda" class="nav-link">(the other LDA)</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#the-lda-model" class="nav-link">The LDA Model</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#multinomial" class="nav-link">Multinomial</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#dirichlet" class="nav-link">Dirichlet</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#conjugate-prior" class="nav-link">Conjugate Prior</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#dirichlet-distributions" class="nav-link">Dirichlet Distributions</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#two-schools-of-solvers" class="nav-link">Two Schools (of solvers)</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#pick-a-solver" class="nav-link">Pick a solver</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#hyper-parameters" class="nav-link">Hyper-Parameters</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#rough-overview-of-mcmc-and-gibbs-sampling" class="nav-link">Rough overview of MCMC and Gibbs sampling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#markov-chain-monte-carlo-mcmc" class="nav-link">Markov-Chain Monte-Carlo (MCMC)</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#gibbs-sampling" class="nav-link">Gibbs Sampling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#assumption-can-sample-from-conditional-p-x-i-x-j-neq-i" class="nav-link">Assumption: Can sample from conditional p(X_i|X_{j\neq i})</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#block-gibbs-sampling-for-lda" class="nav-link">(Block) Gibbs Sampling for LDA</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#collapsed-gibbs-sampling-for-lda" class="nav-link">Collapsed Gibbs Sampling for LDA</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#further-reading" class="nav-link">Further Reading</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#http-ai-stanford-edu-amaas-data-sentiment" class="nav-link">http://ai.stanford.edu/~amaas/data/sentiment/</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#example-application-sentiment-analysis-of-movie-reviews" class="nav-link">Example application: Sentiment analysis of movie reviews</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#applying-bag-of-words-to-a-toy-dataset" class="nav-link">Applying bag-of-words to a toy dataset</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#bag-of-word-for-movie-reviews" class="nav-link">Bag-of-word for movie reviews</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#id4" class="nav-link">Stop-words</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#rescaling-the-data-with-tfidf" class="nav-link">Rescaling the data with TFIDF</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#investigating-model-coefficients" class="nav-link">Investigating model coefficients</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#bag-of-words-with-more-than-one-word-n-grams" class="nav-link">Bag of words with more than one word (n-grams)</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#exercise" class="nav-link">Exercise</a>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="working-with-text-data">
<h1>Working with Text data<a class="headerlink" href="#working-with-text-data" title="Permalink to this headline">¶</a></h1>
<p>Today, we’ll talk about working with text data.</p>
<p>FIXME CountVectorizer in Column transformer is weird, no list, just string. Example of combining text and non-text features.
FIXME explain L2 normalization and intuition?
FIXME BOW figure needs repainting
FIXME hashing vectorizer slide missing score output
FIXME a bit too long, remove european parliament example?</p>
<div class="section" id="more-kinds-of-data">
<h2>More kinds of data<a class="headerlink" href="#more-kinds-of-data" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>So far:</p>
<ul>
<li><p>Fixed number of features</p></li>
<li><p>Contiguous</p></li>
<li><p>Categorical</p></li>
</ul>
</li>
</ul>
<p>–</p>
<ul class="simple">
<li><p>Next up:</p>
<ul>
<li><p>No pre-defined features</p></li>
<li><p>Free text</p></li>
<li><p>Images</p></li>
<li><p>(Audio, video, graphs, …: not this class)</p></li>
</ul>
</li>
</ul>
<p>So far we talked about data where we have a fixed number of
features, and where the features are either continuous or
categorical. And that is most of the data out there and the
simplest kind of data to work with.</p>
<p>The rest of the semester, we’re going to talk about
different kinds of data that is opposite to what we already
worked with.</p>
<p>We’ll look at data that has no predefined features. In
particular, we’re going to look at free text and images.
We’re also going to look at time series, which is a similar
category of not having a clear way to express it as a
standard supervised learning task. Another common example in
this category is audio and video, we’re not going to do any
audio or video in this lecture. But I think text and images
are the common types of data.</p>
<ul class="simple">
<li><p>Need to create fixed-length description</p></li>
</ul>
</div>
<div class="section" id="typical-text-data">
<h2>Typical Text Data<a class="headerlink" href="#typical-text-data" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 100%" src="../_images/typical_text_data_1.png" />
]</p>
<p>.center[
<img alt=":scale 100%" src="../_images/typical_text_data_2.png" />
]</p>
<p>Here’s an example of some text dataset that I’m going to use
later. This is like relatively typical text, its user input
data from IMDb movie reviews.</p>
<p>Clearly, they don’t have the same length, there’s no clear
way to express this is as like some floating point features
which we would use for a standard machine learning approach.
You can also see there are weird things about
capitalization, some weird words in there like Godzilla that
you might not see very often, there is some HTML markup,
there’s punctuation and so on. This is sort of how text
looks like in the wild very often. You could also do things
on much longer texts, like whole books, or articles or so
on.</p>
<p>People also work on text that’s much shorter. Tweets are
still sort of their own domain. Working on tweets is quite
hard because they are very short.</p>
<p>I’m going to talk about a particular kind of data that you
see very often because it’s often used. But there are other
kinds of data that is text data.</p>
</div>
<div class="section" id="other-types-of-text-data">
<h2>Other Types of text data<a class="headerlink" href="#other-types-of-text-data" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 100%" src="../_images/other_types_of_text_data.png" />
]</p>
<p>Here is a table of the text data consisting of people
sitting in the European Parliament. Each person has a
country, a name, an ID, a party, and a political
affiliation. The variable country here, you might hope that
it’s categorical but for the European Union, actually, the
categories are well known. Though, if you have a user input
a country, they will never input the same country in the
same way. The UK could be the United Kingdom, or England, or
Britain, or Great Britain, and all of these might correspond
in this context to the same entity.</p>
<p>If you have users input this, they will also make typos.
This variable is representing a category concept but you
have to do some cleaning before you can actually convert it
into a category. This is often a manual thing to do. There
are things that can help you with this, like duplication or
an entity recognition but we’re not going to talk about
that.</p>
<p>The names are strings. They are definitely not categorical,
and they’re also not words. So it’s not really clear if
there’s like any information in the name and we’re going to
look into this later today.</p>
<p>But you clearly don’t want to treat them the same way as
words that you find in a dictionary.</p>
<p>We have the political groups and affiliations. Again, these
are sort of free texts. They’re sort of similar to the
variable country only now there’s many more of them, in a
sense, they try to encode a categorical variable, which
political group you’re from, but they also have more
information, for example, you could also treat them as texts
like if there’s freedom or democracy or Christianity or
something in the party title that will probably say
something. So you could either treat it as categorical or as
text and both will give you information. And you have the
same problem, that the same group can be referred with
different names or different spellings.</p>
<p>So there’s this whole area of named entity recognition that
actually tries to extract from any text the named entities
and then tries to disambiguate. We’re actually going to talk
about this, but it’s also some particular kind of text data
that I want you to be aware of.</p>
</div>
<div class="section" id="bag-of-words">
<h2>Bag of Words<a class="headerlink" href="#bag-of-words" title="Permalink to this headline">¶</a></h2>
<p>.center[</p>
<p><img alt=":scale 85%" src="../_images/bag_of_words.png" />
]</p>
<p>The most common approach that people use for this is bag of
words, which is a very simple approach.</p>
<p>We start with a string which would be your whole document.
The first step is to tokenize it, which is basically
breaking it up the document into words. And then, you built
a vocabulary over all documents over all words.</p>
<p>So you look over all the movie reviews, or all the emails or
the text you have, and you collect all the tokens which are
representing the words.</p>
<p>Finally, using the vocabulary, you create a group
representation. The length of your feature vector will be
the size of the vocabulary. For each word in the vocabulary,
you count how often this word appears in the string that you
want to encode.</p>
<p>Most of the words in the English language don’t appear in a
sentence. So most of these entries will be 0. But the word
‘ants’ appears once, so increment the count for ‘ants’ by 1
and so on.</p>
<p>So I’ll have 6 non-zero entries and all the other entries
will be 0. So this is what we call like a very sparse vector
so most entries of this vector are 0.</p>
<p>When coding this, we’ll use a sparse matrix encoding. The
idea of sparse matrix encoding is that you only store the
non-zero entries.</p>
<p>So basically, we will only store the 1s, so storing the
string as a bag of word will cost us a nearly 6 floating
point numbers.</p>
</div>
<div class="section" id="toy-example">
<h2>Toy Example<a class="headerlink" href="#toy-example" title="Permalink to this headline">¶</a></h2>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">malory</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Do you want ants?&quot;</span><span class="p">,</span>
          <span class="s2">&quot;Because that’s how you get ants.&quot;</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">vect</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;ants&#39;</span><span class="p">,</span> <span class="s1">&#39;because&#39;</span><span class="p">,</span> <span class="s1">&#39;do&#39;</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="s1">&#39;how&#39;</span><span class="p">,</span> <span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="s1">&#39;want&#39;</span><span class="p">,</span> <span class="s1">&#39;you&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
</pre></div>
</div>
<p>]</p>
<p>Here, I have a corpus. Corpus is what people in NLP called
datasets. My dataset is 2 strings, “Do you want ants?”,
“Because that’s how you get ants.”</p>
<p>The bag of word representation is implemented in the count
vectorizer in scikit-learn, which is a
sklearn.feature_extraction.text</p>
<p>This is a transformer, it’s kind of similar to most
transformers. The only difference is it takes in lists of
strings as input, not numeric data. Usually, transformers
take in a Numpy area as anything in scikit-learn.</p>
<p>But here, count vectorizer takes in a list of strings. I
gave the list of string and I fit it on my documents and
this builds the vocabulary. I can get back to vocabulary by
looking at the feature names using get_feature_names()</p>
<p>The vocabulary is sorted alphabetically. Vocabulary has 8
entries from ‘ants’ to ‘you’. So if I use this count
vectorizer to create like a word representation, then I’ll
get an 8-dimensional feature space.</p>
<p>I can use this to transform the 2 documents that I have. X
here, being the outcome of the transform will be a sparse
matrix. So to print it, I call toarray() on it. So sparse
matrices are in scipy, and you can easily convert to and
from numpy arrays.</p>
<p>Usually, it’s a very bad idea to convert to sparse matrix
into Numpy array, since usually, it will not fit into your
memory. If you have like a vocabulary of size 100,000 and
100,000 documents, and you make it into Numpy array, then
your memory will just fill up and your computer will crash
since you won’t have enough memory to store all the zeros.</p>
<p>In this toy example, we can easily convert this to a Numpy
array and you can see that this is the feature
representation. In this case, we only have 1s and 0s.</p>
<p>Each word appears either once or zero times. So here, in the
first string, the first feature is ‘ant’, so ‘ant’, ‘do’,
‘want’ and ‘you ’ appears once, ‘because’, ‘get’, ‘how’, and
‘that’ doesn’t appear. The second string is processed in the
same way.</p>
<p>Q: What would you do if your document has more unique tokens
or tokens that are not in the vocabulary?</p>
<p>Usually, you either ignore them, that’s the default thing to
do. Your test set will have tokens that your training set
will not have, and so you can’t really learn anything about
that. One thing you could do is count how many words out of
the vocabulary there are.</p>
<p>We going to talk a little bit later about how to restrict
your vocabulary and you can basically add a new feature,
that says, “There’s X amount of features that are not in the
vocabulary.” Often, this can be useful for people cascading
something or miss typing something or using names or
something like that.</p>
<p>But usually, you assume that most of the words appear in
your training data set.</p>
<p>Q: What if there are spelling errors?</p>
<p>You can treat them but by default, if you do that, they’re
just different features. So they’re completely unrelated. So
if there’s a new spelling error in your test dataset that
never appeared in your training dataset, then the word will
be completely ignored.</p>
<p>You can use spell correction, I’ll talk about that in a bit.</p>
<p>If you don’t do any spell correction, it will be a
completely distinct feature. Spell correction is also not
entirely trivial because there are many possible words.</p>
<p>Q: Does the count vectorizer trim the apostrophe in ‘that’s’
in the second string?</p>
<p>Yes, and as you can see, the question marks and the dots
also don’t appear. And we’re going to talk about how exactly
it works in a little bit.</p>
<p>Consider two documents in a dataset “malory”</p>
</div>
<div class="section" id="bag">
<h2>“bag”<a class="headerlink" href="#bag" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;Do you want ants?&#39;</span><span class="p">,</span> <span class="s1">&#39;Because that’s how you get ants.&#39;</span><span class="p">]</span>
<span class="p">[</span><span class="s1">&#39;ants&#39;</span> <span class="s1">&#39;do&#39;</span> <span class="s1">&#39;want&#39;</span> <span class="s1">&#39;you&#39;</span><span class="p">]</span>
<span class="p">[</span><span class="s1">&#39;ants&#39;</span> <span class="s1">&#39;because&#39;</span> <span class="s1">&#39;get&#39;</span> <span class="s1">&#39;how&#39;</span> <span class="s1">&#39;that&#39;</span> <span class="s1">&#39;you&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>The reason why this is called a bag of words is since you’re
completely ignoring the order of words. So you can do
inverse transform to get back the string representation from
the numerical representation but if you do that the
numerical representation doesn’t contain the order of the
words so you will only get back ‘ants’ ‘do’ ‘want’ ‘you’ and
‘ants’ ‘because’ ‘get’ ‘how’ ‘that’ ‘you’</p>
<p>Basically, you store all the words in a big bag and
completely lose any context and any order of the words.
That’s why it’s called a bag of words.</p>
</div>
<div class="section" id="text-classification-example">
<h2>Text classification example:<a class="headerlink" href="#text-classification-example" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="imdb-movie-reviews">
<h2>IMDB Movie Reviews<a class="headerlink" href="#imdb-movie-reviews" title="Permalink to this headline">¶</a></h2>
<p>We’re going to do a binary classification task on a dataset
of IMDb movie reviews. You can look on GitHub, I have the
notebook that runs through all of this if you want to run it
yourself.</p>
<p>The idea here is that we want to do sentiment analysis
basically classifying reviews into either being positive or
negative. In IMDb, they give stars from 0 to 10. This is
from a research paper where the author stated 1, 2 and 3 as
negative while 7, 8, 9 and 10 are positive.</p>
</div>
<div class="section" id="data-loading">
<h2>Data loading<a class="headerlink" href="#data-loading" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_files</span>
<span class="n">reviews_train</span> <span class="o">=</span> <span class="n">load_files</span><span class="p">(</span><span class="s2">&quot;../data/aclImdb/train/&quot;</span><span class="p">)</span>

<span class="n">text_trainval</span><span class="p">,</span> <span class="n">y_trainval</span> <span class="o">=</span> <span class="n">reviews_train</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">reviews_train</span><span class="o">.</span><span class="n">target</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;type of text_train: &quot;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">text_trainval</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;length of text_train: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_trainval</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;class balance: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_trainval</span><span class="p">))</span>

</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">type</span> <span class="n">of</span> <span class="n">text_trainval</span><span class="p">:</span> 

<span class="n">length</span> <span class="n">of</span> <span class="n">text_trainval</span><span class="p">:</span> <span class="mi">25000</span>
<span class="k">class</span> <span class="nc">balance</span><span class="p">:</span> <span class="p">[</span><span class="mi">12500</span> <span class="mi">12500</span><span class="p">]</span>
</pre></div>
</div>
<p>Text data is in either it’s in a CSV format or if they’re
longer documents, people in NLP like to have a single text
file for each data point.</p>
<p>There’s a tool in scikit-learn that allows you to load data,
method load_files function, which basically iterates over
all the folders in a given folder. Each folder is supposed
to correspond to a class and then inside each folder, each
text document corresponds to a data point, that’s a very
common format that people use for text data.</p>
<p>We can load this, then we get the actual data and the
targets.</p>
<p>Text_trainval is used as a training and validation set is
just a list. The length is 25,000. So there are 25,000
documents, and this is a balanced dataset meaning there are
12,500 positive and negative samples.</p>
</div>
<div class="section" id="id1">
<h2>Data loading<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;text_train[1]:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text_trainval</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">decode</span><span class="p">())</span>
</pre></div>
</div>
<p>.smaller[</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">text_train</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
<span class="s1">&#39;Words can&#39;</span><span class="n">t</span> <span class="n">describe</span> <span class="n">how</span> <span class="n">bad</span> <span class="n">this</span> <span class="n">movie</span> <span class="ow">is</span><span class="o">.</span> <span class="n">I</span> <span class="n">can</span><span class="s1">&#39;t explain it by</span>
<span class="n">writing</span> <span class="n">only</span><span class="o">.</span> <span class="n">You</span> <span class="n">have</span> <span class="n">too</span> <span class="n">see</span> <span class="n">it</span> <span class="k">for</span> <span class="n">yourself</span> <span class="n">to</span> <span class="n">get</span> <span class="n">at</span> <span class="n">grip</span> <span class="n">of</span> <span class="n">how</span>
<span class="n">horrible</span> <span class="n">a</span> <span class="n">movie</span> <span class="n">really</span> <span class="n">can</span> <span class="n">be</span><span class="o">.</span> <span class="n">Not</span> <span class="n">that</span> <span class="n">I</span> <span class="n">recommend</span> <span class="n">you</span> <span class="n">to</span> <span class="n">do</span> <span class="n">that</span><span class="o">.</span>
<span class="n">There</span> <span class="n">are</span> <span class="n">so</span> <span class="n">many</span> <span class="n">clichés</span><span class="p">,</span> <span class="n">mistakes</span> <span class="p">(</span><span class="ow">and</span> <span class="nb">all</span> <span class="n">other</span> <span class="n">negative</span> <span class="n">things</span>
<span class="n">you</span> <span class="n">can</span> <span class="n">imagine</span><span class="p">)</span> <span class="n">here</span> <span class="n">that</span> <span class="n">will</span> <span class="n">just</span> <span class="n">make</span> <span class="n">you</span> <span class="n">cry</span><span class="o">.</span> <span class="n">To</span> <span class="n">start</span> <span class="k">with</span> <span class="n">the</span>
<span class="n">technical</span> <span class="n">first</span><span class="p">,</span> <span class="n">there</span> <span class="n">are</span> <span class="n">a</span> <span class="n">LOT</span> <span class="n">of</span> <span class="n">mistakes</span> <span class="n">regarding</span> <span class="n">the</span> <span class="n">airplane</span><span class="o">.</span> <span class="n">I</span>
<span class="n">won</span><span class="s1">&#39;t list them here, but just mention the coloring of the plane. They</span>
<span class="n">didn</span><span class="s1">&#39;t even manage to show an airliner in the colors of a fictional</span>
<span class="n">airline</span><span class="p">,</span> <span class="n">but</span> <span class="n">instead</span> <span class="n">used</span> <span class="n">a</span> <span class="mi">747</span> <span class="n">painted</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">original</span> <span class="n">Boeing</span> <span class="n">livery</span><span class="o">.</span>
<span class="n">Very</span> <span class="n">bad</span><span class="o">.</span> <span class="n">The</span> <span class="n">plot</span> <span class="ow">is</span> <span class="n">stupid</span> <span class="ow">and</span> <span class="n">has</span> <span class="n">been</span> <span class="n">done</span> <span class="n">many</span> <span class="n">times</span> <span class="n">before</span><span class="p">,</span> <span class="n">only</span>
<span class="n">much</span><span class="p">,</span> <span class="n">much</span> <span class="n">better</span><span class="o">.</span> <span class="n">There</span> <span class="n">are</span> <span class="n">so</span> <span class="n">many</span> <span class="n">ridiculous</span> <span class="n">moments</span> <span class="n">here</span> <span class="n">that</span> <span class="n">i</span>
<span class="n">lost</span> <span class="n">count</span> <span class="n">of</span> <span class="n">it</span> <span class="n">really</span> <span class="n">early</span><span class="o">.</span> <span class="n">Also</span><span class="p">,</span> <span class="n">I</span> <span class="n">was</span> <span class="n">on</span> <span class="n">the</span> <span class="n">bad</span> <span class="n">guys</span><span class="s1">&#39; side all</span>
<span class="n">the</span> <span class="n">time</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">movie</span><span class="p">,</span> <span class="n">because</span> <span class="n">the</span> <span class="n">good</span> <span class="n">guys</span> <span class="n">were</span> <span class="n">so</span> <span class="n">stupid</span><span class="o">.</span> <span class="s2">&quot;Executive</span>
<span class="n">Decision</span><span class="s2">&quot; should without a doubt be you&#39;re choice over this one, even the</span>
<span class="s2">&quot;Turbulence&quot;</span><span class="o">-</span><span class="n">movies</span> <span class="n">are</span> <span class="n">better</span><span class="o">.</span> <span class="n">In</span> <span class="n">fact</span><span class="p">,</span> <span class="n">every</span> <span class="n">other</span> <span class="n">movie</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">world</span> <span class="ow">is</span>
<span class="n">better</span> <span class="n">than</span> <span class="n">this</span> <span class="n">one</span><span class="o">.</span><span class="s1">&#39;</span>
</pre></div>
</div>
<p>]</p>
<p>We can also look at data points. I’m calling decode here to
print the Unicode characters.</p>
<p>The word ‘cliché’ can be typed with or without the axon and
they would be two different words. But also, you need to
determine if it’s a Unicode character or not. In Python 3,
by default, everything is a unique code.</p>
</div>
<div class="section" id="vectorization">
<h2>Vectorization<a class="headerlink" href="#vectorization" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text_train_val</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;</span>
<span class="s2">&quot;, b&quot;</span> <span class="s2">&quot;)</span>
                  <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">text_train_val</span><span class="p">]</span>
<span class="n">text_train</span><span class="p">,</span> <span class="n">text_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">text_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y_trainval</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_val</span><span class="p">)</span>
<span class="n">X_train</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
<span class="o">&lt;</span>
<span class="mi">18750</span><span class="n">x66651</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s1">&#39;</span>
<span class="s1">&#39;</span>
  <span class="k">with</span> <span class="mi">2580448</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Row</span> <span class="nb">format</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>I removed all the irrelevant HTML formatting. And then I
split it into training and test set. Then I called the count
vectorizer. Then I fitted the training set and transform the
validation set.</p>
<p>Then it returns a 18750x66651 sparse matrix meaning there
are 18,750 samples and 66,651 features.</p>
<p>So the vocabulary that we built is 66,651. There’s 2.5
million stored elements (non-zero entries).</p>
<p>This is a much, much smaller number than the product of
these two numbers. So most of the entries are zero.</p>
<p>Remember, you need to know whether you are in Python 2 or 3
and then you need to know the type of text.</p>
<p>There are two different things that you need to keep in
mind. So the text can be a byte string or Unicode string.
But if it’s a byte string, it also has an encoding attached
with it. You need to know the encoding to go from the byte
string to the string.</p>
</div>
<div class="section" id="vocabulary">
<h2>Vocabulary<a class="headerlink" href="#vocabulary" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
<p>.smaller[</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;00&#39;</span><span class="p">,</span> <span class="s1">&#39;000&#39;</span><span class="p">,</span> <span class="s1">&#39;0000000000001&#39;</span><span class="p">,</span> <span class="s1">&#39;00001&#39;</span><span class="p">,</span> <span class="s1">&#39;00015&#39;</span><span class="p">,</span> <span class="s1">&#39;000s&#39;</span><span class="p">,</span> <span class="s1">&#39;001&#39;</span><span class="p">,</span> <span class="s1">&#39;003830&#39;</span><span class="p">,</span> <span class="s1">&#39;006&#39;</span><span class="p">,</span> <span class="s1">&#39;007&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>]</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">20000</span><span class="p">:</span><span class="mi">20020</span><span class="p">])</span>
</pre></div>
</div>
<p>.smaller[</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;eschews&#39;</span><span class="p">,</span> <span class="s1">&#39;escort&#39;</span><span class="p">,</span> <span class="s1">&#39;escorted&#39;</span><span class="p">,</span> <span class="s1">&#39;escorting&#39;</span><span class="p">,</span> <span class="s1">&#39;escorts&#39;</span><span class="p">,</span> <span class="s1">&#39;escpecially&#39;</span><span class="p">,</span> <span class="s1">&#39;escreve&#39;</span><span class="p">,</span>
 <span class="s1">&#39;escrow&#39;</span><span class="p">,</span> <span class="s1">&#39;esculator&#39;</span><span class="p">,</span> <span class="s1">&#39;ese&#39;</span><span class="p">,</span> <span class="s1">&#39;eser&#39;</span><span class="p">,</span> <span class="s1">&#39;esha&#39;</span><span class="p">,</span> <span class="s1">&#39;eshaan&#39;</span><span class="p">,</span> <span class="s1">&#39;eshley&#39;</span><span class="p">,</span> <span class="s1">&#39;esk&#39;</span><span class="p">,</span> <span class="s1">&#39;eskimo&#39;</span><span class="p">,</span>
 <span class="s1">&#39;eskimos&#39;</span><span class="p">,</span> <span class="s1">&#39;esmerelda&#39;</span><span class="p">,</span> <span class="s1">&#39;esmond&#39;</span><span class="p">,</span> <span class="s1">&#39;esophagus&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>]</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[::</span><span class="mi">2000</span><span class="p">])</span>
</pre></div>
</div>
<p>.smaller[</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;00&#39;</span><span class="p">,</span> <span class="s1">&#39;ahoy&#39;</span><span class="p">,</span> <span class="s1">&#39;aspects&#39;</span><span class="p">,</span> <span class="s1">&#39;belting&#39;</span><span class="p">,</span> <span class="s1">&#39;bridegroom&#39;</span><span class="p">,</span> <span class="s1">&#39;cements&#39;</span><span class="p">,</span> <span class="s1">&#39;commas&#39;</span><span class="p">,</span> <span class="s1">&#39;crowds&#39;</span><span class="p">,</span>
 <span class="s1">&#39;detlef&#39;</span><span class="p">,</span> <span class="s1">&#39;druids&#39;</span><span class="p">,</span> <span class="s1">&#39;eschews&#39;</span><span class="p">,</span> <span class="s1">&#39;finishing&#39;</span><span class="p">,</span> <span class="s1">&#39;gathering&#39;</span><span class="p">,</span>  <span class="s1">&#39;gunrunner&#39;</span><span class="p">,</span> <span class="s1">&#39;homesickness&#39;</span><span class="p">,</span>
 <span class="s1">&#39;inhumanities&#39;</span><span class="p">,</span> <span class="s1">&#39;kabbalism&#39;</span><span class="p">,</span> <span class="s1">&#39;leech&#39;</span><span class="p">,</span> <span class="s1">&#39;makes&#39;</span><span class="p">,</span> <span class="s1">&#39;miki&#39;</span><span class="p">,</span> <span class="s1">&#39;nas&#39;</span><span class="p">,</span> <span class="s1">&#39;organ&#39;</span><span class="p">,</span> <span class="s1">&#39;pesci&#39;</span><span class="p">,</span>
 <span class="s1">&#39;principally&#39;</span><span class="p">,</span> <span class="s1">&#39;rebours&#39;</span><span class="p">,</span> <span class="s1">&#39;robotnik&#39;</span><span class="p">,</span> <span class="s1">&#39;sculptural&#39;</span><span class="p">,</span> <span class="s1">&#39;skinkons&#39;</span><span class="p">,</span> <span class="s1">&#39;stardom&#39;</span><span class="p">,</span> <span class="s1">&#39;syncer&#39;</span><span class="p">,</span>
 <span class="s1">&#39;tools&#39;</span><span class="p">,</span> <span class="s1">&#39;unflagging&#39;</span><span class="p">,</span> <span class="s1">&#39;waaaay&#39;</span><span class="p">,</span> <span class="s1">&#39;yanks&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>]</p>
<p>This is the first thing I do, it’s helpful to see if
something sensible happened. And you get an idea of the
data. I’m going to plot the first 10 data points, and I’m
going to plot some 20 in the middle and I’m going to plot
every 2000th data point.</p>
<p>The first couple of ones seem to be pretty boring. But then
it seems to do something relatively reasonable.</p>
<p>Now that we have the sparse matrix representation, we can
just do classification as usual.</p>
</div>
<div class="section" id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegressionCV</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">C_</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span> <span class="mf">0.046</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.882</span>
</pre></div>
</div>
<p>All the models in scikit-learn work directly on the sparse
matrix representation. Here, I do logistic regression CV so
it adjusts CR automatically for me. The validation set score
I get is 88% accuracy, which is pretty good.</p>
<p>Here, accuracy is meaningful because we know it’s a balanced
dataset. The next thing I always like to do is look at the
coefficients.</p>
<p>.center[
<img alt=":scale 100%" src="../_images/coefficients.png" />
]</p>
<p>So here I’m looking at the 20 most negative and 20 most
positive coefficients of this logistic regression. It’s
pretty easy to do because it’s a binary task. If you have
multiclass tasks, you’ll have way more coefficients and it’s
harder to visualize.</p>
<p>This seems to be like a pretty reasonable model to pick up
on the bad words and the good words. This is the baseline
approach.</p>
</div>
<div class="section" id="soo-many-options">
<h2>Soo many options!<a class="headerlink" href="#soo-many-options" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>How to tokenize?</p></li>
<li><p>How to normalize words?</p></li>
<li><p>What to include in vocabulary?</p></li>
</ul>
<p>In tokenization, there are many different options to change
what’s happening here.</p>
<p>In particular, there’s the tokenization, which is how do you
break it up into tokens. Then normalization, which is how do
you make tokens look sort of reasonable. And then what to
include in vocabulary.</p>
</div>
<div class="section" id="tokenization">
<h2>Tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Scikit-learn (very simplistic):</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">re.findall(r&quot;\b\w\w+\b&quot;)</span></code></p></li>
<li><p>Includes numbers</p></li>
<li><p>discards single-letter words</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-</span></code> or <code class="docutils literal notranslate"><span class="pre">'</span></code> break up words</p></li>
</ul>
</li>
</ul>
<p>Scikit-learn is not an LLP library. Some very good Python
LLP libraries are NLTK and spacy. They have a lot more
things for doing text analysis. Scikit-learn only has quite
simple things.</p>
<p>In tokenization, what the count vectorizer does the regular
expression, “\b\w\w+\b”. That means it finds anything of
length 2 or longer that has word boundaries, which are any
punctuation or white spaces, and matches w.</p>
<p>As we saw, this includes any numbers. Numbers can be at the
beginning of the word, the middle of the word or it can be
just all numbers.</p>
<p>You don’t include single letter work or single letter
numbers. So ‘I’ is always discarded. There’s an interesting
thing where you can get the gender of a person, depending on
how much often they use ‘I’.</p>
<p>It doesn’t include any punctuation because we only use the
w, which is digits.</p>
<p>This is pretty simple and pretty restrictive.</p>
</div>
<div class="section" id="changing-the-token-pattern-regex">
<h2>Changing the token pattern regex<a class="headerlink" href="#changing-the-token-pattern-regex" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">token_pattern</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;\b\w+\b&quot;</span><span class="p">)</span>
<span class="n">vect</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;ants&#39;</span><span class="p">,</span> <span class="s1">&#39;because&#39;</span><span class="p">,</span> <span class="s1">&#39;do&#39;</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="s1">&#39;how&#39;</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="s1">&#39;want&#39;</span><span class="p">,</span> <span class="s1">&#39;you&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>–</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">token_pattern</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;\b\w[\w’]+\b&quot;</span><span class="p">)</span>

<span class="n">vect</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;ants&#39;</span><span class="p">,</span> <span class="s1">&#39;because&#39;</span><span class="p">,</span> <span class="s1">&#39;do&#39;</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="s1">&#39;how&#39;</span><span class="p">,</span> <span class="s1">&#39;that’s&#39;</span><span class="p">,</span> <span class="s1">&#39;want&#39;</span><span class="p">,</span> <span class="s1">&#39;you&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>If we just want to use a different regular expression, we
can do something simple, like changing the regular
expression. So here, for example, I removed one of the Ws
and so now I match everything that’s a single letter or
more. So now I only the ‘s’ in ‘that’s’, without the
apostrophe, making it a single token.</p>
<p>I can also allow things to have an apostrophe in the middle
of the word, and get ‘that’s’ as a single token. This might
look nicer but actually, in reality, it usually doesn’t
matter that much.</p>
<p>But you can play around with this and see if the tokens you
get with standard tokenization are good for your
application.</p>
<p>Again, spacy and NLTK have much more sophisticated things.
The X in vectorization slide represents the sparse matrix
representation of the data and Y represents if it’s a
positive or a negative review.</p>
<p>Was not actually an apostroph but some unicode pattern
because I copy &amp; pasted the quote.</p>
<p>#Normalization</p>
<p>.smaller[</p>
<ul class="simple">
<li><p>Correct spelling?</p></li>
<li><p>Stemming: reduce to word stem</p></li>
<li><p>Lemmatization: smartly reduce to word stem
]
–
.smaller[
“Our meeting today was worse than yesterday,</p></li>
</ul>
<p>I’m scared of meeting the clients tomorrow.”</p>
<p>Stemming:</p>
<p><code class="docutils literal notranslate"><span class="pre">['our',</span> <span class="pre">'meet',</span> <span class="pre">'today',</span> <span class="pre">'wa',</span> <span class="pre">'wors',</span> <span class="pre">'than',</span> <span class="pre">'yesterday',</span> <span class="pre">',',</span> <span class="pre">'i',</span>&#160; <span class="pre">&quot;'m&quot;,</span> <span class="pre">'scare',</span> <span class="pre">'of',</span> <span class="pre">'meet',</span> <span class="pre">'the',</span> <span class="pre">'client',</span> <span class="pre">'tomorrow',</span> <span class="pre">'.']</span></code></p>
<p>Lemmatization:</p>
</div>
<div class="section" id="our-meeting-today-be-bad-than-yesterday-i-be-scar-of-meet-the-client-tomorrow">
<h2><code class="docutils literal notranslate"><span class="pre">['our',</span> <span class="pre">'meeting',</span> <span class="pre">'today',</span> <span class="pre">'be',</span> <span class="pre">'bad',</span> <span class="pre">'than',</span> <span class="pre">'yesterday',</span> <span class="pre">',',</span> <span class="pre">'i',</span>&#160; <span class="pre">'be',</span> <span class="pre">'scar',</span> <span class="pre">'of',</span> <span class="pre">'meet',</span> <span class="pre">'the',</span> <span class="pre">'client',</span> <span class="pre">'tomorrow',</span> <span class="pre">'.']</span></code>
]<a class="headerlink" href="#our-meeting-today-be-bad-than-yesterday-i-be-scar-of-meet-the-client-tomorrow" title="Permalink to this headline">¶</a></h2>
<p>.smaller[</p>
<ul class="simple">
<li><p>scikit-learn:</p>
<ul>
<li><p>Lower-case it</p></li>
<li><p>Configurable, use nltk or spacy
]</p></li>
</ul>
</li>
</ul>
<p>Normalization is basically how do you want to bring your
token in a standardized form. For example, you could get rid
of plurals or you could correct spelling. There are three
main methods.</p>
<p>Correct spelling, which only very few people use in
practice.</p>
<p>Stemming reduces words to their word stem. The idea is to
get rid of plural ‘s’ and ‘ing’ at the end of the verbs and
so on so that you don’t have the conjugations. Stemming
removes some letters at the end of the word depending on a
fixed rule set.</p>
<p>Lemmatization also uses reduces the word stem but in a smart
way. It tries to parse the sentence and then uses a
dictionary of all English words with all its verb forms and
conjugations and tries to map it to a standardized form.</p>
<p>The reason why I picked this example is ‘meeting’ appears
twice in here, once as a noun and once as a verb. Using
stemming:
‘was’ becomes ‘wa’
‘worse’ becomes ‘wors’ and so on
‘I’m’ is just split up in two without the apostrophe.
‘scared’ becomes ‘scare’
‘meeting’ becomes ‘meet’</p>
<p>Lemmatization parsed the sentence and figured out that the
first ‘meeting’ is a noun and keeps it as it is but the
second ‘meeting’ which is a verb is simplified to ‘meet’.
You can also see that the ‘was’ was normalized to ‘be’ and
the ‘worse’ was normalized to ‘bad’ and the ’m was
normalized to ‘be’ as well.</p>
<p>The question with any kind of normalization you do is will
this be helpful for your application.</p>
<p>Q: is stemming only applicable to English literature?</p>
<p>These are both language dependent. This model exists for
many languages. Today, I’m only talking about languages that
work similar to English.</p>
<p>Scikit-learn only lowercases it. You can plug in other
normalization from NLTK or spacy into the count vectorizer
if you want, but the default is it lower cases the tokens.</p>
</div>
<div class="section" id="restricting-the-vocabulary">
<h2>Restricting the Vocabulary<a class="headerlink" href="#restricting-the-vocabulary" title="Permalink to this headline">¶</a></h2>
<p>The other main thing that you can play with is restricting
the vocabulary. So far, I said, we’re just going to use all
the tokens that we see.</p>
</div>
<div class="section" id="stop-words">
<h2>Stop Words<a class="headerlink" href="#stop-words" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">vect</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;ants&#39;</span><span class="p">,</span> <span class="s1">&#39;want&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">ENGLISH_STOP_WORDS</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">ENGLISH_STOP_WORDS</span><span class="p">))</span>
</pre></div>
</div>
<p>.tiny-code[</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;former&#39;</span><span class="p">,</span> <span class="s1">&#39;above&#39;</span><span class="p">,</span> <span class="s1">&#39;inc&#39;</span><span class="p">,</span> <span class="s1">&#39;off&#39;</span><span class="p">,</span> <span class="s1">&#39;on&#39;</span><span class="p">,</span> <span class="s1">&#39;those&#39;</span><span class="p">,</span> <span class="s1">&#39;not&#39;</span><span class="p">,</span> <span class="s1">&#39;fifteen&#39;</span><span class="p">,</span> <span class="s1">&#39;sometimes&#39;</span><span class="p">,</span> <span class="s1">&#39;too&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;move&#39;</span><span class="p">,</span> <span class="s1">&#39;much&#39;</span><span class="p">,</span> <span class="s1">&#39;own&#39;</span><span class="p">,</span> <span class="s1">&#39;until&#39;</span><span class="p">,</span> <span class="s1">&#39;wherein&#39;</span><span class="p">,</span>
<span class="s1">&#39;which&#39;</span><span class="p">,</span> <span class="s1">&#39;over&#39;</span><span class="p">,</span> <span class="s1">&#39;thru&#39;</span><span class="p">,</span> <span class="s1">&#39;whoever&#39;</span><span class="p">,</span> <span class="s1">&#39;this&#39;</span><span class="p">,</span> <span class="s1">&#39;indeed&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">,</span> <span class="s1">&#39;whatever&#39;</span><span class="p">,</span> <span class="s1">&#39;us&#39;</span><span class="p">,</span> <span class="s1">&#39;somewhere&#39;</span><span class="p">,</span> <span class="s1">&#39;after&#39;</span><span class="p">,</span> <span class="s1">&#39;eleven&#39;</span><span class="p">,</span> <span class="s1">&#39;most&#39;</span><span class="p">,</span> <span class="s1">&#39;de&#39;</span><span class="p">,</span> <span class="s1">&#39;full&#39;</span><span class="p">,</span>
<span class="s1">&#39;into&#39;</span><span class="p">,</span> <span class="s1">&#39;being&#39;</span><span class="p">,</span> <span class="s1">&#39;yourselves&#39;</span><span class="p">,</span> <span class="s1">&#39;neither&#39;</span><span class="p">,</span> <span class="s1">&#39;he&#39;</span><span class="p">,</span> <span class="s1">&#39;onto&#39;</span><span class="p">,</span> <span class="s1">&#39;seems&#39;</span><span class="p">,</span> <span class="s1">&#39;who&#39;</span><span class="p">,</span> <span class="s1">&#39;between&#39;</span><span class="p">,</span> <span class="s1">&#39;few&#39;</span><span class="p">,</span> <span class="s1">&#39;couldnt&#39;</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="s1">&#39;found&#39;</span><span class="p">,</span> <span class="s1">&#39;nobody&#39;</span><span class="p">,</span> <span class="s1">&#39;hereafter&#39;</span><span class="p">,</span>
<span class="s1">&#39;therein&#39;</span><span class="p">,</span> <span class="s1">&#39;together&#39;</span><span class="p">,</span> <span class="s1">&#39;con&#39;</span><span class="p">,</span> <span class="s1">&#39;ours&#39;</span><span class="p">,</span> <span class="s1">&#39;an&#39;</span><span class="p">,</span> <span class="s1">&#39;anyone&#39;</span><span class="p">,</span> <span class="s1">&#39;became&#39;</span><span class="p">,</span> <span class="s1">&#39;mine&#39;</span><span class="p">,</span> <span class="s1">&#39;myself&#39;</span><span class="p">,</span> <span class="s1">&#39;before&#39;</span><span class="p">,</span> <span class="s1">&#39;call&#39;</span><span class="p">,</span> <span class="s1">&#39;already&#39;</span><span class="p">,</span> <span class="s1">&#39;nothing&#39;</span><span class="p">,</span> <span class="s1">&#39;top&#39;</span><span class="p">,</span> <span class="s1">&#39;further&#39;</span><span class="p">,</span>
<span class="s1">&#39;thereby&#39;</span><span class="p">,</span> <span class="s1">&#39;why&#39;</span><span class="p">,</span> <span class="s1">&#39;here&#39;</span><span class="p">,</span> <span class="s1">&#39;next&#39;</span><span class="p">,</span> <span class="s1">&#39;these&#39;</span><span class="p">,</span> <span class="s1">&#39;ever&#39;</span><span class="p">,</span> <span class="s1">&#39;whereby&#39;</span><span class="p">,</span> <span class="s1">&#39;cannot&#39;</span><span class="p">,</span> <span class="s1">&#39;anyhow&#39;</span><span class="p">,</span> <span class="s1">&#39;thereupon&#39;</span><span class="p">,</span> <span class="s1">&#39;somehow&#39;</span><span class="p">,</span> <span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;out&#39;</span><span class="p">,</span> <span class="s1">&#39;ltd&#39;</span><span class="p">,</span> <span class="s1">&#39;latterly&#39;</span><span class="p">,</span>
<span class="s1">&#39;although&#39;</span><span class="p">,</span> <span class="s1">&#39;beforehand&#39;</span><span class="p">,</span> <span class="s1">&#39;hundred&#39;</span><span class="p">,</span> <span class="s1">&#39;else&#39;</span><span class="p">,</span> <span class="s1">&#39;per&#39;</span><span class="p">,</span> <span class="s1">&#39;if&#39;</span><span class="p">,</span> <span class="s1">&#39;afterwards&#39;</span><span class="p">,</span> <span class="s1">&#39;any&#39;</span><span class="p">,</span> <span class="s1">&#39;since&#39;</span><span class="p">,</span> <span class="s1">&#39;nor&#39;</span><span class="p">,</span> <span class="s1">&#39;thereafter&#39;</span><span class="p">,</span> <span class="s1">&#39;it&#39;</span><span class="p">,</span> <span class="s1">&#39;around&#39;</span><span class="p">,</span> <span class="s1">&#39;them&#39;</span><span class="p">,</span>
<span class="s1">&#39;alone&#39;</span><span class="p">,</span> <span class="s1">&#39;up&#39;</span><span class="p">,</span> <span class="s1">&#39;sometime&#39;</span><span class="p">,</span> <span class="s1">&#39;very&#39;</span><span class="p">,</span> <span class="s1">&#39;give&#39;</span><span class="p">,</span> <span class="s1">&#39;elsewhere&#39;</span><span class="p">,</span> <span class="s1">&#39;always&#39;</span><span class="p">,</span> <span class="s1">&#39;cant&#39;</span><span class="p">,</span> <span class="s1">&#39;due&#39;</span><span class="p">,</span> <span class="s1">&#39;forty&#39;</span><span class="p">,</span> <span class="s1">&#39;still&#39;</span><span class="p">,</span> <span class="s1">&#39;either&#39;</span><span class="p">,</span> <span class="s1">&#39;was&#39;</span><span class="p">,</span> <span class="s1">&#39;beyond&#39;</span><span class="p">,</span> <span class="s1">&#39;fill&#39;</span><span class="p">,</span>
<span class="s1">&#39;hereupon&#39;</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span> <span class="s1">&#39;might&#39;</span><span class="p">,</span> <span class="s1">&#39;by&#39;</span><span class="p">,</span> <span class="s1">&#39;everyone&#39;</span><span class="p">,</span> <span class="s1">&#39;five&#39;</span><span class="p">,</span> <span class="s1">&#39;often&#39;</span><span class="p">,</span> <span class="s1">&#39;several&#39;</span><span class="p">,</span> <span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="s1">&#39;something&#39;</span><span class="p">,</span> <span class="s1">&#39;formerly&#39;</span><span class="p">,</span> <span class="s1">&#39;she&#39;</span><span class="p">,</span> <span class="s1">&#39;him&#39;</span><span class="p">,</span> <span class="s1">&#39;become&#39;</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span>
<span class="s1">&#39;could&#39;</span><span class="p">,</span> <span class="s1">&#39;ten&#39;</span><span class="p">,</span> <span class="s1">&#39;below&#39;</span><span class="p">,</span> <span class="s1">&#39;had&#39;</span><span class="p">,</span> <span class="s1">&#39;how&#39;</span><span class="p">,</span> <span class="s1">&#39;back&#39;</span><span class="p">,</span> <span class="s1">&#39;nevertheless&#39;</span><span class="p">,</span> <span class="s1">&#39;namely&#39;</span><span class="p">,</span> <span class="s1">&#39;herself&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;be&#39;</span><span class="p">,</span> <span class="s1">&#39;himself&#39;</span><span class="p">,</span> <span class="s1">&#39;becomes&#39;</span><span class="p">,</span> <span class="s1">&#39;hereby&#39;</span><span class="p">,</span>
<span class="s1">&#39;never&#39;</span><span class="p">,</span> <span class="s1">&#39;along&#39;</span><span class="p">,</span> <span class="s1">&#39;while&#39;</span><span class="p">,</span> <span class="s1">&#39;side&#39;</span><span class="p">,</span> <span class="s1">&#39;amoungst&#39;</span><span class="p">,</span> <span class="s1">&#39;toward&#39;</span><span class="p">,</span> <span class="s1">&#39;made&#39;</span><span class="p">,</span> <span class="s1">&#39;their&#39;</span><span class="p">,</span> <span class="s1">&#39;part&#39;</span><span class="p">,</span> <span class="s1">&#39;everything&#39;</span><span class="p">,</span> <span class="s1">&#39;his&#39;</span><span class="p">,</span> <span class="s1">&#39;becoming&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;now&#39;</span><span class="p">,</span> <span class="s1">&#39;am&#39;</span><span class="p">,</span>
<span class="s1">&#39;perhaps&#39;</span><span class="p">,</span> <span class="s1">&#39;moreover&#39;</span><span class="p">,</span> <span class="s1">&#39;seeming&#39;</span><span class="p">,</span> <span class="s1">&#39;themselves&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;etc&#39;</span><span class="p">,</span> <span class="s1">&#39;more&#39;</span><span class="p">,</span> <span class="s1">&#39;another&#39;</span><span class="p">,</span> <span class="s1">&#39;whither&#39;</span><span class="p">,</span> <span class="s1">&#39;see&#39;</span><span class="p">,</span> <span class="s1">&#39;herein&#39;</span><span class="p">,</span> <span class="s1">&#39;whom&#39;</span><span class="p">,</span> <span class="s1">&#39;among&#39;</span><span class="p">,</span> <span class="s1">&#39;un&#39;</span><span class="p">,</span> <span class="s1">&#39;via&#39;</span><span class="p">,</span>
<span class="s1">&#39;every&#39;</span><span class="p">,</span> <span class="s1">&#39;cry&#39;</span><span class="p">,</span> <span class="s1">&#39;me&#39;</span><span class="p">,</span> <span class="s1">&#39;should&#39;</span><span class="p">,</span> <span class="s1">&#39;its&#39;</span><span class="p">,</span> <span class="s1">&#39;again&#39;</span><span class="p">,</span> <span class="s1">&#39;co&#39;</span><span class="p">,</span> <span class="s1">&#39;itself&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;yourself&#39;</span><span class="p">,</span> <span class="s1">&#39;seemed&#39;</span><span class="p">,</span> <span class="s1">&#39;under&#39;</span><span class="p">,</span> <span class="s1">&#39;then&#39;</span><span class="p">,</span> <span class="s1">&#39;meanwhile&#39;</span><span class="p">,</span> <span class="s1">&#39;anywhere&#39;</span><span class="p">,</span>
<span class="s1">&#39;beside&#39;</span><span class="p">,</span> <span class="s1">&#39;seem&#39;</span><span class="p">,</span> <span class="s1">&#39;please&#39;</span><span class="p">,</span> <span class="s1">&#39;behind&#39;</span><span class="p">,</span> <span class="s1">&#39;sixty&#39;</span><span class="p">,</span> <span class="s1">&#39;were&#39;</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">,</span> <span class="s1">&#39;upon&#39;</span><span class="p">,</span> <span class="s1">&#39;than&#39;</span><span class="p">,</span> <span class="s1">&#39;twelve&#39;</span><span class="p">,</span> <span class="s1">&#39;when&#39;</span><span class="p">,</span> <span class="s1">&#39;third&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;though&#39;</span><span class="p">,</span> <span class="s1">&#39;hence&#39;</span><span class="p">,</span>
<span class="s1">&#39;done&#39;</span><span class="p">,</span> <span class="s1">&#39;other&#39;</span><span class="p">,</span> <span class="s1">&#39;where&#39;</span><span class="p">,</span> <span class="s1">&#39;someone&#39;</span><span class="p">,</span> <span class="s1">&#39;of&#39;</span><span class="p">,</span> <span class="s1">&#39;whose&#39;</span><span class="p">,</span> <span class="s1">&#39;during&#39;</span><span class="p">,</span> <span class="s1">&#39;many&#39;</span><span class="p">,</span> <span class="s1">&#39;as&#39;</span><span class="p">,</span> <span class="s1">&#39;except&#39;</span><span class="p">,</span> <span class="s1">&#39;besides&#39;</span><span class="p">,</span> <span class="s1">&#39;for&#39;</span><span class="p">,</span> <span class="s1">&#39;within&#39;</span><span class="p">,</span> <span class="s1">&#39;mostly&#39;</span><span class="p">,</span> <span class="s1">&#39;but&#39;</span><span class="p">,</span>
<span class="s1">&#39;nowhere&#39;</span><span class="p">,</span> <span class="s1">&#39;we&#39;</span><span class="p">,</span> <span class="s1">&#39;our&#39;</span><span class="p">,</span> <span class="s1">&#39;through&#39;</span><span class="p">,</span> <span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="s1">&#39;bill&#39;</span><span class="p">,</span> <span class="s1">&#39;yours&#39;</span><span class="p">,</span> <span class="s1">&#39;less&#39;</span><span class="p">,</span> <span class="s1">&#39;well&#39;</span><span class="p">,</span> <span class="s1">&#39;have&#39;</span><span class="p">,</span> <span class="s1">&#39;therefore&#39;</span><span class="p">,</span> <span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;last&#39;</span><span class="p">,</span> <span class="s1">&#39;throughout&#39;</span><span class="p">,</span> <span class="s1">&#39;can&#39;</span><span class="p">,</span>
<span class="s1">&#39;mill&#39;</span><span class="p">,</span> <span class="s1">&#39;against&#39;</span><span class="p">,</span> <span class="s1">&#39;anyway&#39;</span><span class="p">,</span> <span class="s1">&#39;at&#39;</span><span class="p">,</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;noone&#39;</span><span class="p">,</span> <span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="s1">&#39;would&#39;</span><span class="p">,</span> <span class="s1">&#39;only&#39;</span><span class="p">,</span> <span class="s1">&#39;rather&#39;</span><span class="p">,</span> <span class="s1">&#39;wherever&#39;</span><span class="p">,</span> <span class="s1">&#39;least&#39;</span><span class="p">,</span> <span class="s1">&#39;are&#39;</span><span class="p">,</span> <span class="s1">&#39;empty&#39;</span><span class="p">,</span> <span class="s1">&#39;almost&#39;</span><span class="p">,</span>
<span class="s1">&#39;latter&#39;</span><span class="p">,</span> <span class="s1">&#39;front&#39;</span><span class="p">,</span> <span class="s1">&#39;my&#39;</span><span class="p">,</span> <span class="s1">&#39;amount&#39;</span><span class="p">,</span> <span class="s1">&#39;put&#39;</span><span class="p">,</span> <span class="s1">&#39;what&#39;</span><span class="p">,</span> <span class="s1">&#39;whereas&#39;</span><span class="p">,</span> <span class="s1">&#39;across&#39;</span><span class="p">,</span> <span class="s1">&#39;whereupon&#39;</span><span class="p">,</span> <span class="s1">&#39;otherwise&#39;</span><span class="p">,</span> <span class="s1">&#39;thin&#39;</span><span class="p">,</span> <span class="s1">&#39;others&#39;</span><span class="p">,</span> <span class="s1">&#39;go&#39;</span><span class="p">,</span> <span class="s1">&#39;thus&#39;</span><span class="p">,</span>
<span class="s1">&#39;enough&#39;</span><span class="p">,</span> <span class="s1">&#39;her&#39;</span><span class="p">,</span> <span class="s1">&#39;fire&#39;</span><span class="p">,</span> <span class="s1">&#39;may&#39;</span><span class="p">,</span> <span class="s1">&#39;once&#39;</span><span class="p">,</span> <span class="s1">&#39;show&#39;</span><span class="p">,</span> <span class="s1">&#39;because&#39;</span><span class="p">,</span> <span class="s1">&#39;ourselves&#39;</span><span class="p">,</span> <span class="s1">&#39;some&#39;</span><span class="p">,</span> <span class="s1">&#39;such&#39;</span><span class="p">,</span> <span class="s1">&#39;yet&#39;</span><span class="p">,</span> <span class="s1">&#39;eight&#39;</span><span class="p">,</span> <span class="s1">&#39;sincere&#39;</span><span class="p">,</span> <span class="s1">&#39;from&#39;</span><span class="p">,</span> <span class="s1">&#39;been&#39;</span><span class="p">,</span> <span class="s1">&#39;twenty&#39;</span><span class="p">,</span>
<span class="s1">&#39;whether&#39;</span><span class="p">,</span> <span class="s1">&#39;without&#39;</span><span class="p">,</span> <span class="s1">&#39;you&#39;</span><span class="p">,</span> <span class="s1">&#39;do&#39;</span><span class="p">,</span> <span class="s1">&#39;everywhere&#39;</span><span class="p">,</span> <span class="s1">&#39;six&#39;</span><span class="p">,</span> <span class="s1">&#39;however&#39;</span><span class="p">,</span> <span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="s1">&#39;find&#39;</span><span class="p">,</span> <span class="s1">&#39;hers&#39;</span><span class="p">,</span> <span class="s1">&#39;towards&#39;</span><span class="p">,</span> <span class="s1">&#39;will&#39;</span><span class="p">,</span> <span class="s1">&#39;also&#39;</span><span class="p">,</span> <span class="s1">&#39;even&#39;</span><span class="p">,</span> <span class="s1">&#39;or&#39;</span><span class="p">,</span>
<span class="s1">&#39;re&#39;</span><span class="p">,</span> <span class="s1">&#39;describe&#39;</span><span class="p">,</span> <span class="s1">&#39;serious&#39;</span><span class="p">,</span> <span class="s1">&#39;so&#39;</span><span class="p">,</span> <span class="s1">&#39;anything&#39;</span><span class="p">,</span> <span class="s1">&#39;must&#39;</span><span class="p">,</span> <span class="s1">&#39;ie&#39;</span><span class="p">,</span> <span class="s1">&#39;the&#39;</span><span class="p">,</span> <span class="s1">&#39;whenever&#39;</span><span class="p">,</span> <span class="s1">&#39;thick&#39;</span><span class="p">,</span> <span class="s1">&#39;bottom&#39;</span><span class="p">,</span> <span class="s1">&#39;they&#39;</span><span class="p">,</span> <span class="s1">&#39;keep&#39;</span><span class="p">,</span> <span class="s1">&#39;your&#39;</span><span class="p">,</span> <span class="s1">&#39;has&#39;</span><span class="p">,</span> <span class="s1">&#39;about&#39;</span><span class="p">,</span>
<span class="s1">&#39;each&#39;</span><span class="p">,</span> <span class="s1">&#39;four&#39;</span><span class="p">,</span> <span class="s1">&#39;eg&#39;</span><span class="p">,</span> <span class="s1">&#39;interest&#39;</span><span class="p">,</span> <span class="s1">&#39;hasnt&#39;</span><span class="p">,</span> <span class="s1">&#39;detail&#39;</span><span class="p">,</span> <span class="s1">&#39;amongst&#39;</span><span class="p">,</span> <span class="s1">&#39;take&#39;</span><span class="p">,</span> <span class="s1">&#39;thence&#39;</span><span class="p">,</span> <span class="s1">&#39;down&#39;</span><span class="p">,</span> <span class="s1">&#39;fifty&#39;</span><span class="p">,</span> <span class="s1">&#39;whence&#39;</span><span class="p">,</span> <span class="s1">&#39;whereafter&#39;</span><span class="p">,</span> <span class="s1">&#39;nine&#39;</span><span class="p">,</span> <span class="s1">&#39;with&#39;</span><span class="p">,</span>
<span class="s1">&#39;whole&#39;</span><span class="p">,</span> <span class="s1">&#39;there&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>]</p>
<p>One way to restrict the vocabulary often is to remove stop
words. Stop words, are words that are not so important to
the content and so you discard them.</p>
<p>If I do this for my example, the only thing that remains are
‘ants’ and ‘want’ because ‘this’, ‘I’ and ‘how’ are all stop
words.</p>
<p>Scikit-learn has some stop words built in. So if you do stop
word equal to English, it will use the built-in the stop
word list. It’s a little bit strange and we’re working on
improving it. So for example, there’s ‘system’ in the stop
word list and ‘bill’ is in the stop word list.</p>
<p>For unsupervised problems, using stop words might be
helpful. For supervised problems, I rarely found it helpful
with the tools that we talked about.</p>
<p>So problem is, these are nearly 200 words but we have 66,000
features, removing 200 features doesn’t really make a
difference. And because these words appear very often, the
model won’t be able to figure out are they important or not.</p>
<p>If you use an unsupervised model, in the unsupervised model
since they appear so often, these might just dominate
whatever clustering you do and so removing them might be a
good idea.</p>
<ul class="simple">
<li><p>not a very good stop-word list? Why is system in it? bill?</p></li>
<li><p>For supervised learning often little effect on large corpuses (on small corpuses and for unsupervised learning it can help)</p></li>
</ul>
</div>
<div class="section" id="infrequent-words">
<h2>Infrequent Words<a class="headerlink" href="#infrequent-words" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Remove words that appear in less than 2 documents:</p></li>
</ul>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">vect</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;ants&#39;</span><span class="p">,</span> <span class="s1">&#39;you&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>]</p>
<ul class="simple">
<li><p>Restrict vocabulary size to max_features most frequent words:</p></li>
</ul>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">vect</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;ants&#39;</span><span class="p">,</span> <span class="s1">&#39;because&#39;</span><span class="p">,</span> <span class="s1">&#39;do&#39;</span><span class="p">,</span> <span class="s1">&#39;you&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>]</p>
<p>Another thing that is often helpful is removing infrequent
words. There’s a parameter called min_df that basically
removes words that appear less than twice.</p>
<p>It’ll only keep words that appeared twice or more, and so
only ‘ants’ and ‘you’ remain in the toy dataset example.</p>
<p>This is often useful because it can remove a lot of
features. And if a word appears only once in your data, it’s
unlikely that your algorithm will be able to learn. If you
have hundreds of thousands of thousands of features, and one
feature appears once in your dataset, then it’s probably not
going to be helpful.</p>
<p>You can also go the other way around and set max_features.
Max_features sets the number of features that you want to
keep, in this case, four most common features.</p>
<p>It’s kind of interesting, this goes a little bit in the
opposite direction of what the stop words do. The stop words
remove the most common ones that are meaningless and here,
we removed the most infrequent ones because they’re probably
not helpful. These are often misspellings, or names or
something like that.</p>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_train_df2</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>

<span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">X_train_df4</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="n">X_val_df4</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_val</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train_df2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train_df4</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">18750</span><span class="p">,</span> <span class="mi">66651</span><span class="p">)</span>
<span class="p">(</span><span class="mi">18750</span><span class="p">,</span> <span class="mi">39825</span><span class="p">)</span>
<span class="p">(</span><span class="mi">18750</span><span class="p">,</span> <span class="mi">26928</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_df4</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">C_</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span> <span class="mf">0.046</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val_df4</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.881</span>
</pre></div>
</div>
<p>]</p>
<p>Here I used the count vectorizer with min_df=2 and min_df=4.
That means either use tokens that appear in 2 documents in a
training set, or at least 4 documents in a training set.</p>
<p>Min_df=2 gives me 39825 while min_df=4 gives me 26928. I’ve
drastically reduced the feature space when using min_df=2
and if I set min_df=4, I cut down feature space even more.</p>
<p>Min_df means minimum document frequency, meaning the number
of documents the words appear in. It’s not the number of
times the word appears. So if a word appears in 100 times in
a single document, it’s still going to be thrown out by
this.</p>
<p>Here, I used X_train_df=4, which is much, much smaller, it’s
less than half the size. And the result I get is identical,
basically, in terms of accuracy.</p>
<p>These are the main configuration you just used a single bag
of words.</p>
<p>That said, they’re throwing away a lot of the context and
information. One way to improve upon this is what’s called
N-grams.</p>
<ul class="simple">
<li><p>Removed nearly 1/3 of features!</p></li>
<li><p>As good as before</p></li>
</ul>
</div>
<div class="section" id="tf-idf-rescaling">
<h2>Tf-idf rescaling<a class="headerlink" href="#tf-idf-rescaling" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[ \text{tf-idf}(t,d) = \text{tf}(t,d)\cdot \text{idf}(t)\]</div>
<div class="math notranslate nohighlight">
\[ \text{idf}(t) = \log\frac{1+n_d}{1+\text{df}(d,t)} + 1\]</div>
<p><span class="math notranslate nohighlight">\(n_d\)</span> = total number of documents</p>
<p><span class="math notranslate nohighlight">\(df(d,t)\)</span> = number of documents containing term <span class="math notranslate nohighlight">\(t\)</span></p>
<p>–</p>
<ul class="simple">
<li><p>In sklearn: by default also L2 normalisation!</p></li>
</ul>
<p>Next thing we can do is actually try to rescale our words
with something called Tf-idf which stands for Term Frequency
Inverse Document Frequency.</p>
<p>This is sort of an alternative to using soft stop words in a
sentence. The idea here is to down-weight things that are
very common.</p>
<p>It’s the logarithm of 1 over the number of documents
containing the term.</p>
<p>So if something happens to appear in most of the documents,
the inverse document frequency will give it a very low
weight. This is something that’s very commonly used in
information retrieval.</p>
<p>Information retrieval basically means you’re trying to find
relevant documents. If you can think of how a search engine
works, if you want to match a bag of words representation of
two different query strings, you might not care so much
about matching the ‘a’ and ‘the’, but you care a lot about
matching the parts that are rare and that are very specific
to a particular search term. That is the motivation for
this.</p>
<p>But people find it also sometimes helps in a machine
learning context.</p>
<p>You can’t really know in advance whether this helps for the
particular dataset or not, but basically you should keep in
mind, this emphasizes rare words and does a soft removal of
very common words. This also means the sort of stop words
here are corpus specific so here it will learn to
down-weight ‘movie’ and ‘film’ because they’re very common.</p>
<p>The implementation of this in scikit-learn is a little bit
non-standard but I don’t think it makes a big difference.</p>
<p>You should also keep in mind is that if you use the
implementation of TF-IDF in scikit-learn by default, it will
also do L2 normalization. L2 normalization means you divide
each row by its length. That means you normalize the weight
the length of the document. You only want to count how often
the word appears relative to how long the document is.
Basically, if you make a document that basically repeats
each word twice, it should still have the same
representation. By default, that’s turned on for TF-IDF. But
it’s not turned on for the bag of words.</p>
<ul class="simple">
<li><p>Emphasizes “rare” words - “soft stop word removal”</p></li>
<li><p>Slightly non-standard smoothing (many +1s)</p></li>
</ul>
</div>
<div class="section" id="tfidfvectorizer-tfidftransformer">
<h2>TfidfVectorizer, TfidfTransformer<a class="headerlink" href="#tfidfvectorizer-tfidftransformer" title="Permalink to this headline">¶</a></h2>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span><span class="p">,</span> <span class="n">TfidfTransformer</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">malory_tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="n">malory_tfidf</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.41</span> <span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.576</span><span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.576</span><span class="p">,</span>  <span class="mf">0.41</span> <span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.318</span><span class="p">,</span>  <span class="mf">0.447</span><span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.447</span><span class="p">,</span>  <span class="mf">0.447</span><span class="p">,</span>  <span class="mf">0.447</span><span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.318</span><span class="p">]])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">malory_tfidf</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(),</span>
                             <span class="n">TfidfTransformer</span><span class="p">())</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="n">malory_tfidf</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.41</span> <span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.576</span><span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.576</span><span class="p">,</span>  <span class="mf">0.41</span> <span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.318</span><span class="p">,</span>  <span class="mf">0.447</span><span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.447</span><span class="p">,</span>  <span class="mf">0.447</span><span class="p">,</span>  <span class="mf">0.447</span><span class="p">,</span>  <span class="mf">0.</span>   <span class="p">,</span>  <span class="mf">0.318</span><span class="p">]])</span>
</pre></div>
</div>
<p>]</p>
<p>In scikit-learn, there are two ways to do this. You can
either use the TF-IDF vectorizer which you can use directly
on the text and it will build the vocabulary and then do the
rescaling. Or you can use to count vectorizer,
TFIDFTransformer, which will work on the sparse matrix
created by the count vectorizer and then just transform it
using rescaling.</p>
<p>This is an alternative to using stop words or putting more
emphasis on rare words.</p>
<p>Q: Explain the part about the L2 norm.</p>
<p>For the L1 norm, it might be easier to visualize. Dividing
by L1 norm would mean just dividing by word count and so
that would mean that basically, if you have a very long
review, and it says bad 3 times, but it says good 20 times,
then you only care about the relative frequency. And if it’s
a very short review that says good, 0 times, and bad also 3
times, then it’s probably a bad review. So you want to
really scale these so that the relative frequency of the
word only matters and not the total length document. That’s
sort of what the normalization does.</p>
<p>And so the common thing to do is L2 normalization, which
divides by Euclidean norm.</p>
</div>
<div class="section" id="n-grams-beyond-single-words">
<h2>N-grams: Beyond single words<a class="headerlink" href="#n-grams-beyond-single-words" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Bag of words completely removes word order.</p></li>
<li><p>“didn’t love” and “love” are very different!</p></li>
</ul>
<p>–</p>
<p>.center[
<img alt=":scale 80%" src="../_images/single_words.png" />
]</p>
<p>Because there’s a big difference between ‘didn’t love’ and
‘love’ and the default settings in the count vectorizer
cannot distinguish between the two because somewhere along
the document, it could appear ‘don’t’ and you don’t know if
it’s in front of ‘love’, or if it’s ‘love, don’t hate’ or
‘don’t love hate’, since they basically have the same
representation.</p>
<p>The idea behind N-grams is you look at pairs of words that
appear next to each other. Unigrams looks at single words,
bigrams look at pairs of two words next to each other, and
trigrams looks at three words next to each other, and so on.</p>
<p>Here instead of splitting it up into single tokens, we can
split it up into pairs of tokens allowing me to have a
little bit of context around each of the words.</p>
<ul class="simple">
<li><p>N-grams: tuples of consecutive words</p></li>
</ul>
</div>
<div class="section" id="bigrams-toy-example">
<h2>Bigrams toy example<a class="headerlink" href="#bigrams-toy-example" title="Permalink to this headline">¶</a></h2>
<p>.tiny-code[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Vocabulary</span> <span class="n">size</span><span class="p">:</span> <span class="mi">8</span>
<span class="n">Vocabulary</span><span class="p">:</span>
<span class="p">[</span><span class="s1">&#39;ants&#39;</span><span class="p">,</span> <span class="s1">&#39;because&#39;</span><span class="p">,</span> <span class="s1">&#39;do&#39;</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="s1">&#39;how&#39;</span><span class="p">,</span> <span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="s1">&#39;want&#39;</span><span class="p">,</span> <span class="s1">&#39;you&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Vocabulary</span> <span class="n">size</span><span class="p">:</span> <span class="mi">8</span>
<span class="n">Vocabulary</span><span class="p">:</span>
<span class="p">[</span><span class="s1">&#39;because that&#39;</span><span class="p">,</span> <span class="s1">&#39;do you&#39;</span><span class="p">,</span> <span class="s1">&#39;get ants&#39;</span><span class="p">,</span> <span class="s1">&#39;how you&#39;</span><span class="p">,</span> <span class="s1">&#39;that how&#39;</span><span class="p">,</span> <span class="s1">&#39;want ants&#39;</span><span class="p">,</span> <span class="s1">&#39;you get&#39;</span><span class="p">,</span> <span class="s1">&#39;you want&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Vocabulary</span> <span class="n">size</span><span class="p">:</span> <span class="mi">16</span>
<span class="n">Vocabulary</span><span class="p">:</span>
<span class="p">[</span><span class="s1">&#39;ants&#39;</span><span class="p">,</span> <span class="s1">&#39;because&#39;</span><span class="p">,</span> <span class="s1">&#39;because that&#39;</span><span class="p">,</span> <span class="s1">&#39;do&#39;</span><span class="p">,</span> <span class="s1">&#39;do you&#39;</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="s1">&#39;get ants&#39;</span><span class="p">,</span> <span class="s1">&#39;how&#39;</span><span class="p">,</span> <span class="s1">&#39;how you&#39;</span><span class="p">,</span> <span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="s1">&#39;that how&#39;</span><span class="p">,</span> <span class="s1">&#39;want&#39;</span><span class="p">,</span> <span class="s1">&#39;want ants&#39;</span><span class="p">,</span>
<span class="s1">&#39;you&#39;</span><span class="p">,</span> <span class="s1">&#39;you get&#39;</span><span class="p">,</span> <span class="s1">&#39;you want&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>]</p>
<p>When implementing in scikit-learn, you have to specify the
N-gram range, which has the minimum number of tokens and the
maximum number of tokens you want to look at.</p>
<p>Usually, you want to look at Unigram + Bigrams + Trigrams,
don’t look at only Bigrams. If I only look at Unigrams, I
get a vocabulary of size 8. If I look only at Bigrams, I get
a vocabulary of size 8 as well, that’s because this was a
toy example and in the real world there will be much more
Bigrams than Unigrams.</p>
<p>If you look at the result from combining Unigrams and
Bigrams together, we get 16, which is a combination of both
the result.</p>
<p>Now I have a feature representation of size 16, and so I
extended my feature space so that I can take the context
into account.</p>
<p>Unigrams and Bigrams together are the most common
combinations. Using higher order N-grams is rare.</p>
<ul class="simple">
<li><p>Typically: higher n-grams lead to blow up of feature space!</p></li>
</ul>
</div>
<div class="section" id="n-grams-on-imdb-data">
<h2>N-grams on IMDB data<a class="headerlink" href="#n-grams-on-imdb-data" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Vocabulary</span> <span class="n">Sizes</span>
<span class="mi">1</span><span class="o">-</span><span class="n">gram</span> <span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span> <span class="mi">26928</span>
<span class="mi">2</span><span class="o">-</span><span class="n">gram</span> <span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span> <span class="mi">128426</span>
<span class="mi">1</span><span class="o">-</span><span class="n">gram</span> <span class="o">&amp;</span> <span class="mi">2</span><span class="o">-</span><span class="n">gram</span> <span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span> <span class="mi">155354</span>
<span class="mi">1</span><span class="o">-</span><span class="mi">3</span><span class="n">gram</span> <span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span> <span class="mi">254274</span>
<span class="mi">1</span><span class="o">-</span><span class="mi">4</span><span class="n">gram</span> <span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span> <span class="mi">289443</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size 1-4gram: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Vocabulary</span> <span class="n">size</span> <span class="mi">1</span><span class="o">-</span><span class="mi">4</span><span class="n">gram</span> <span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span> <span class="mi">7815528</span>
</pre></div>
</div>
<ul class="simple">
<li><p>More than 20x more 4-grams!</p></li>
</ul>
<p>This is using no stop words.</p>
<p>Now because many bigrams and many trigrams are very
infrequent. So if I use the example of min_df=4 in Unigrams
I get 26928 features while in Bigrams I get 128426 features.
So it grew by nearly the order of magnitude. So I have a
much, much bigger feature space now. If I put it together,
it’s even bigger.</p>
<p>The higher I go, the bigger it gets. Though, you can see
that it actually grows relatively slowly here. So if I look
into 1-3grams programs, and 1-4grams, I only get 300,000
features. But this is mostly because I did the min_df=4.
There are very few sequences of 4 words that appear often
together. If I don’t use the min_df=4 and I look at all
things I get nearly 8 million features. This is like 20
times more four grams than what we had before.</p>
<p>So particularly if you use higher N-grams using this pruning
of document frequency might be quite helpful. You can see
that probably they have about the same amount of
information, but you’ll have 20 times fewer features, which
will definitely impact runtime and possibly impact
generalization.</p>
<p>#Stop-word impact on bi-grams</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;(1, 2), min_df=4: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                     <span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="n">cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;(1, 2), stopwords, min_df=4: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">:</span> <span class="mi">155354</span>
<span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stopwords</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">:</span> <span class="mi">81085</span>
</pre></div>
</div>
<p>This is with stop words. What we’re doing is we do stop
words first and then N-grams. What I find interesting is
that here using unigrams and bigrams we got 155354, but if
we remove the stop words it gets halved to 81085. This is
pretty extreme.</p>
<p>The stop word list is only 200 words. So if we remove these
200 words and if you look at the bigrams we half the number
of bigrams. This is because the most common combinations are
actually we with stop words. So ‘the movie’ and ‘the film’
are probably the most common Bigrams in this.</p>
<p>So if we remove this then whatever word that came before
‘the’ is now makes after bigram. And so now, the
combinations appear much less frequently.</p>
</div>
<div class="section" id="stop-word-impact-on-4-grams">
<h2>Stop-word impact on 4-grams<a class="headerlink" href="#stop-word-impact-on-4-grams" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>cv4 = CountVectorizer(ngram_range=(4, 4), min_df=4)
cv4.fit(text_train)
cv4sw = CountVectorizer(ngram_range=(4, 4), min_df=4,
                       stop_words=&quot;english&quot;)
cv4sw.fit(text_train)
print(len(cv4.get_feature_names()))
print(len(cv4sw.get_feature_names()))```
</pre></div>
</div>
<p>31585
369</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>

<span class="mi">4</span><span class="n">grams</span> <span class="n">are</span> <span class="n">even</span> <span class="n">rarer</span> <span class="ow">and</span> <span class="n">so</span> <span class="n">there</span> <span class="n">are</span> <span class="mi">31585</span> <span class="mi">4</span><span class="n">grams</span> <span class="n">that</span>
<span class="n">appear</span> <span class="n">at</span> <span class="n">least</span> <span class="mi">4</span> <span class="n">times</span> <span class="n">but</span> <span class="k">if</span> <span class="n">I</span> <span class="n">remove</span> <span class="n">the</span> <span class="n">stop</span> <span class="n">words</span> <span class="n">I</span> <span class="n">get</span>
<span class="n">only</span> <span class="mi">369</span> <span class="mi">4</span><span class="n">grams</span> <span class="n">that</span> <span class="n">appeared</span> <span class="n">at</span> <span class="n">least</span> <span class="mi">4</span> <span class="n">times</span><span class="o">.</span>
<span class="o">+++</span>
</pre></div>
</div>
<p>[‘worst movie ve seen’ ‘40 year old virgin’ ‘ve seen long time’
‘worst movies ve seen’ ‘don waste time money’
‘mystery science theater 3000’ ‘worst film ve seen’
‘lose friends alienate people’ ‘best movies ve seen’
‘don waste time watching’ ‘jean claude van damme’
‘really wanted like movie’ ‘best movie ve seen’ ‘rock roll high school’
‘don think ve seen’ ‘let face music dance’ ‘don say didn warn’
‘worst films ve seen’ ‘fred astaire ginger rogers’ ‘ha ha ha ha’
‘la maman et la’ ‘maman et la putain’ ‘left cutting room floor’
‘ve seen ve seen’ ‘just doesn make sense’ ‘robert blake scott wilson’
‘late 70 early 80’ ‘crouching tiger hidden dragon’ ‘low budget sci fi’
‘movie ve seen long’ ‘toronto international film festival’
‘night evelyn came grave’ ‘good guys bad guys’ ‘low budget horror movies’
‘waste time watching movie’ ‘vote seven title brazil’ ‘bad bad bad bad’
‘morning sunday night monday’ ‘14 year old girl’ ‘film based true story’
‘don make em like’ ‘silent night deadly night’
‘rating saturday night friday’ ‘right place right time’
‘friday night friday morning’ ‘night friday night friday’
‘friday morning sunday night’ ‘don waste time movie’
‘saturday night friday night’ ‘really wanted like film’]</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>

This is the most common 4grams after removing stop words.
Whenever you have the ability to look into something
interesting in our data, do it.

+++
.center[
![:scale 75%](images/stopwords_1.png)
]

.center[
![:scale 75%](images/stopwords_2.png)
]


Here are some models using Unigrams, Bigrams, and Trigrams,
with and without stop words. You can see that most of the
most common features are actually Unigrams.

It&#39;s interesting because previously ‘worth’ didn&#39;t show up
but since we have bigrams, we can distinguish between ‘not
worth’ and ‘well worth’.

You can also see that most of these actually were Bigrams
that included stop word.

It gets slightly worse because all of these Bigrams actually
have stop words in them.




Stopwords removed fares slightly worse
+++

.tiny-code[
```python
my_stopwords = set(ENGLISH_STOP_WORDS)
my_stopwords.remove(&quot;well&quot;)
my_stopwords.remove(&quot;not&quot;)
my_stopwords.add(&quot;ve&quot;)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vect3msw</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="n">my_stopwords</span><span class="p">)</span>
<span class="n">X_train3msw</span> <span class="o">=</span> <span class="n">vect3msw</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="n">lr3msw</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train3msw</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">X_val3msw</span> <span class="o">=</span> <span class="n">vect3msw</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_val</span><span class="p">)</span>
<span class="n">lr3msw</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val3msw</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.883</span>
</pre></div>
</div>
<p>]</p>
<p>.center[
<img alt=":scale 90%" src="../_images/stopwords_3.png" />
]</p>
<p>Basically, removing stop words is fine but you should keep
in the ‘not’ and the ‘well’ so that the important things can
still be expressed.</p>
</div>
<div class="section" id="character-n-grams">
<h2>Character n-grams<a class="headerlink" href="#character-n-grams" title="Permalink to this headline">¶</a></h2>
<p>The next thing I want to talk about is character n-grams.
Character n-grams are a different way to extract features
from text data. It’s not that useful for general text
classification but it’s useful for some more specific
applications.</p>
<p>The idea is that, instead of looking at tokens, which are
words, we look at windows of characters.</p>
<p>#Principle</p>
<p>.center[
<img alt=":scale 100%" src="../_images/char_ngram_1.png" />
]</p>
<p>#Principle</p>
<p>.center[
<img alt=":scale 100%" src="../_images/char_ngram_2.png" />
]</p>
<p>#Principle</p>
<p>.center[
<img alt=":scale 100%" src="../_images/char_ngram_3.png" />
]</p>
<p>#Principle</p>
<p>.center[
<img alt=":scale 100%" src="../_images/char_ngram_4.png" />
]</p>
<p>#Principle</p>
<p>.center[
<img alt=":scale 100%" src="../_images/char_ngram_5.png" />
]</p>
<p>Here, I want to look at character trigrams, look at windows
of length 3.</p>
<p>My first one would be “Do_” and then “o_y” and then “_you”
and so…</p>
<p>Then you build the vocabulary over these trigrams.</p>
<p>#Applications</p>
<ul class="simple">
<li><p>Be robust to misspelling / obfuscation</p></li>
<li><p>Language detection</p></li>
<li><p>Learn from Names / made-up words</p></li>
</ul>
<p>This can be helpful to be more robust towards misspelling or
obfuscation. So people might replace a single particular
letter in a word with like internet speak. And so if you use
character n-grams, you can still match if someone used the
same characters for the rest of the word.</p>
<p>You can also use it for language detection. Language
detection, I think by now relatively soft task. But
basically, it’s very easy to detect the language, if you
have a text, you can use character n-grams. You can’t really
use a bag of words because the words in different languages
are sort of distinct, and you don’t actually need to build
vocabularies for all the languages. If you just look at the
n-grams, that’s enough to know whether something is English
or French or something.</p>
<p>Also, it helps to learn from names or any made-up words.</p>
<p>One thing that is interesting in a social context is to look
at ethnicity from names. And you can actually get someone’s
ethnicity from their last name, and in some cases, also from
their first name.</p>
</div>
<div class="section" id="id2">
<h2>Toy example<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>“Naive”</p></li>
</ul>
<p>.tiny-code[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">analyzer</span><span class="o">=</span><span class="s2">&quot;char&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Vocabulary</span> <span class="n">size</span><span class="p">:</span> <span class="mi">73</span>
<span class="n">Vocabulary</span><span class="p">:</span>
<span class="p">[</span><span class="s1">&#39; a&#39;</span><span class="p">,</span> <span class="s1">&#39; an&#39;</span><span class="p">,</span> <span class="s1">&#39; g&#39;</span><span class="p">,</span> <span class="s1">&#39; ge&#39;</span><span class="p">,</span> <span class="s1">&#39; h&#39;</span><span class="p">,</span> <span class="s1">&#39; ho&#39;</span><span class="p">,</span> <span class="s1">&#39; t&#39;</span><span class="p">,</span> <span class="s1">&#39; th&#39;</span><span class="p">,</span> <span class="s1">&#39; w&#39;</span><span class="p">,</span> <span class="s1">&#39; wa&#39;</span><span class="p">,</span> <span class="s1">&#39; y&#39;</span><span class="p">,</span> <span class="s1">&#39; yo&#39;</span><span class="p">,</span> <span class="s1">&#39;an&#39;</span><span class="p">,</span> <span class="s1">&#39;ant&#39;</span><span class="p">,</span> <span class="s1">&#39;at&#39;</span><span class="p">,</span> <span class="s1">&#39;at’&#39;</span><span class="p">,</span> <span class="s1">&#39;au&#39;</span><span class="p">,</span> <span class="s1">&#39;aus&#39;</span><span class="p">,</span> <span class="s1">&#39;be&#39;</span><span class="p">,</span> <span class="s1">&#39;bec&#39;</span><span class="p">,</span> <span class="s1">&#39;ca&#39;</span><span class="p">,</span>
<span class="s1">&#39;cau&#39;</span><span class="p">,</span> <span class="s1">&#39;do&#39;</span><span class="p">,</span> <span class="s1">&#39;do &#39;</span><span class="p">,</span> <span class="s1">&#39;e &#39;</span><span class="p">,</span> <span class="s1">&#39;e t&#39;</span><span class="p">,</span> <span class="s1">&#39;ec&#39;</span><span class="p">,</span> <span class="s1">&#39;eca&#39;</span><span class="p">,</span> <span class="s1">&#39;et&#39;</span><span class="p">,</span> <span class="s1">&#39;et &#39;</span><span class="p">,</span> <span class="s1">&#39;ge&#39;</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="s1">&#39;ha&#39;</span><span class="p">,</span> <span class="s1">&#39;hat&#39;</span><span class="p">,</span> <span class="s1">&#39;ho&#39;</span><span class="p">,</span> <span class="s1">&#39;how&#39;</span><span class="p">,</span> <span class="s1">&#39;nt&#39;</span><span class="p">,</span> <span class="s1">&#39;nt &#39;</span><span class="p">,</span> <span class="s1">&#39;nts&#39;</span><span class="p">,</span> <span class="s1">&#39;o &#39;</span><span class="p">,</span> <span class="s1">&#39;o y&#39;</span><span class="p">,</span> <span class="s1">&#39;ou&#39;</span><span class="p">,</span>
<span class="s1">&#39;ou &#39;</span><span class="p">,</span> <span class="s1">&#39;ow&#39;</span><span class="p">,</span> <span class="s1">&#39;ow &#39;</span><span class="p">,</span> <span class="s1">&#39;s &#39;</span><span class="p">,</span> <span class="s1">&#39;s h&#39;</span><span class="p">,</span> <span class="s1">&#39;s.&#39;</span><span class="p">,</span> <span class="s1">&#39;s?&#39;</span><span class="p">,</span> <span class="s1">&#39;se&#39;</span><span class="p">,</span> <span class="s1">&#39;se &#39;</span><span class="p">,</span> <span class="s1">&#39;t &#39;</span><span class="p">,</span> <span class="s1">&#39;t a&#39;</span><span class="p">,</span> <span class="s1">&#39;th&#39;</span><span class="p">,</span> <span class="s1">&#39;tha&#39;</span><span class="p">,</span> <span class="s1">&#39;ts&#39;</span><span class="p">,</span> <span class="s1">&#39;ts.&#39;</span><span class="p">,</span> <span class="s1">&#39;ts?&#39;</span><span class="p">,</span> <span class="s1">&#39;t’&#39;</span><span class="p">,</span> <span class="s1">&#39;t’s&#39;</span><span class="p">,</span> <span class="s1">&#39;u &#39;</span><span class="p">,</span> <span class="s1">&#39;u g&#39;</span><span class="p">,</span> <span class="s1">&#39;u w&#39;</span><span class="p">,</span>
<span class="s1">&#39;us&#39;</span><span class="p">,</span> <span class="s1">&#39;use&#39;</span><span class="p">,</span> <span class="s1">&#39;w &#39;</span><span class="p">,</span> <span class="s1">&#39;w y&#39;</span><span class="p">,</span> <span class="s1">&#39;wa&#39;</span><span class="p">,</span> <span class="s1">&#39;wan&#39;</span><span class="p">,</span> <span class="s1">&#39;yo&#39;</span><span class="p">,</span> <span class="s1">&#39;you&#39;</span><span class="p">,</span> <span class="s1">&#39;’s&#39;</span><span class="p">,</span> <span class="s1">&#39;’s &#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>]</p>
<ul class="simple">
<li><p>Respect word boundaries</p></li>
</ul>
<p>.tiny-code[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">analyzer</span><span class="o">=</span><span class="s2">&quot;char_wb&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">malory</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Vocabulary size: 74
Vocabulary:
[&#39; a&#39;, &#39; an&#39;, &#39; b&#39;, &#39; be&#39;, &#39; d&#39;, &#39; do&#39;, &#39; g&#39;, &#39; ge&#39;, &#39; h&#39;, &#39; ho&#39;, &#39; t&#39;, &#39; th&#39;, &#39; w&#39;, &#39; wa&#39;, &#39; y&#39;, &#39; yo&#39;, &#39;. &#39;, &#39;? &#39;, &#39;an&#39;, &#39;ant&#39;, &#39;at&#39;, &#39;
at’&#39;, &#39;au&#39;, &#39;aus&#39;, &#39;be&#39;, &#39;bec&#39;, &#39;ca&#39;, &#39;cau&#39;, &#39;do&#39;, &#39;do &#39;, &#39;e &#39;, &#39;ec&#39;, &#39;eca&#39;, &#39;et&#39;, &#39;et &#39;, &#39;ge&#39;, &#39;get&#39;, &#39;ha&#39;, &#39;hat&#39;, &#39;ho&#39;, &#39;how&#39;, &#39;nt&#39;, &#39;nt &#39;,
&#39;nts&#39;, &#39;o &#39;, &#39;ou&#39;, &#39;ou &#39;, &#39;ow&#39;, &#39;ow &#39;, &#39;s &#39;, &#39;s.&#39;, &#39;s. &#39;, &#39;s?&#39;, &#39;s? &#39;, &#39;se&#39;, &#39;se &#39;, &#39;t &#39;, &#39;th&#39;, &#39;tha&#39;, &#39;ts&#39;, &#39;ts.&#39;, &#39;ts?&#39;, &#39;t’&#39;, &#39;t’s&#39;,
&#39;u &#39;, &#39;us&#39;, &#39;use&#39;, &#39;w &#39;, &#39;wa&#39;, &#39;wan&#39;, &#39;yo&#39;, &#39;you&#39;, &#39;’s&#39;, &#39;’s &#39;]
</pre></div>
</div>
<p>]</p>
<p>To do this with scikit-learn, you can use the count
vectorizer with the analyzer equal char. So instead of word
tokenization, it does character tokenization.</p>
<p>Usually, we look at characters that are longer, so single
characters don’t really tell us much. Here, I look at
n-grams of size 2 and 3. You can also go up to, like 5 or 7,
obviously, the feature space tends to explode if you go
higher.</p>
<p>After applying, I get a vocabulary of size 73, which is
pretty big for such a short text.</p>
<p>And there’s also a featurette that can respect word
boundaries called char_wb.</p>
<p>So here, you get the end of one word and the beginning of
the next word, you might want to exclude that if you’re only
interested on the actual words. So the char_wb makes sure
that there’s no white space inside the character n-gram.</p>
</div>
<div class="section" id="imdb-data">
<h2>IMDB Data<a class="headerlink" href="#imdb-data" title="Permalink to this headline">¶</a></h2>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">char_vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">analyzer</span><span class="o">=</span><span class="s2">&quot;char_wb&quot;</span><span class="p">)</span>
<span class="n">X_train_char</span> <span class="o">=</span> <span class="n">char_vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">char_vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">164632</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr_char</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_char</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">X_val_char</span> <span class="o">=</span> <span class="n">char_vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_val</span><span class="p">)</span>
<span class="n">lr_char</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val_char</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.881</span>
</pre></div>
</div>
<p>]</p>
<p>If you want, you can use that for classification.</p>
<p>Here, I use it again on the IMDb data set with 2-5grams. The
vocabulary is quite a bit bigger than if I looked at single
words. But actually, I get a result that is about as good as
the bag of words.</p>
<p>.center[
<img alt=":scale 100%" src="../_images/imdb_char_ngrams.png" />
]</p>
<p>I can also look at the features that are important. The way
this looks like is that it seems pretty redundant. So it
picked up different subparts of the words. So this might not
be ideal, but the accuracy is still good.</p>
<p>Another thing that I found quite interesting when I looked
at this is that some people gave their star rating in the
comment. This is actually leaking the label of the star
rating. This is not something that we saw before but here
looking at his character n-grams, we can see that that’s
something that’s in the dataset.</p>
</div>
<div class="section" id="predicting-nationality-from-name">
<h2>Predicting Nationality from Name<a class="headerlink" href="#predicting-nationality-from-name" title="Permalink to this headline">¶</a></h2>
<p>A more useful application is going back to the European
Parliament. What I’m going to do now is predict nationality
from the name.</p>
<p>.center[
<img alt=":scale 70%" src="../_images/nationality_name_1.png" />
]</p>
<p>.center[
<img alt=":scale 70%" src="../_images/nationality_name_2.png" />
]</p>
<p>Here’s the distribution of the country. This is a very
imbalanced classification.</p>
</div>
<div class="section" id="comparing-words-vs-chars">
<h2>Comparing words vs chars<a class="headerlink" href="#comparing-words-vs-chars" title="Permalink to this headline">¶</a></h2>
<p>.tiny-code[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bow_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(),</span> <span class="n">LogisticRegressionCV</span><span class="p">())</span>
<span class="n">cross_val_score</span><span class="p">(</span><span class="n">bow_pipe</span><span class="p">,</span> <span class="n">text_mem_train</span><span class="p">,</span> <span class="n">y_mem_train</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;f1_macro&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span> <span class="mf">0.231</span><span class="p">,</span>  <span class="mf">0.241</span><span class="p">,</span>  <span class="mf">0.236</span><span class="p">,</span>  <span class="mf">0.28</span> <span class="p">,</span>  <span class="mf">0.254</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">char_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s2">&quot;char_wb&quot;</span><span class="p">),</span> <span class="n">LogisticRegressionCV</span><span class="p">())</span>
<span class="n">cross_val_score</span><span class="p">(</span><span class="n">char_pipe</span><span class="p">,</span> <span class="n">text_mem_train</span><span class="p">,</span> <span class="n">y_mem_train</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;f1_macro&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span> <span class="mf">0.452</span><span class="p">,</span>  <span class="mf">0.459</span><span class="p">,</span>  <span class="mf">0.341</span><span class="p">,</span>  <span class="mf">0.469</span><span class="p">,</span>  <span class="mf">0.418</span><span class="p">])</span>
</pre></div>
</div>
<p>]</p>
<p>I can try to do either with word or character n-gram.</p>
<p>Using an f1 macro, I don’t get a good f1 score. If I use
character n-grams, it’s not super great but it’s still quite
a lot better than if we look at the words because I don’t
need to learn new digital names.</p>
</div>
<div class="section" id="grid-search-parameters">
<h2>Grid-search parameters<a class="headerlink" href="#grid-search-parameters" title="Permalink to this headline">¶</a></h2>
<p>.tiny-code[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Normalizer</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;logisticregression__C&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">],</span>
              <span class="s2">&quot;countvectorizer__ngram_range&quot;</span><span class="p">:</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span>
                                               <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)],</span>
              <span class="s2">&quot;countvectorizer__min_df&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
              <span class="s2">&quot;normalizer&quot;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">Normalizer</span><span class="p">()]</span>
             <span class="p">}</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">make_pipeline</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s2">&quot;char&quot;</span><span class="p">),</span> <span class="n">Normalizer</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">(),</span>
                                  <span class="n">memory</span><span class="o">=</span><span class="s2">&quot;cache_folder&quot;</span><span class="p">),</span>
                    <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;f1_macro&quot;</span>
                   <span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_mem_train</span><span class="p">,</span> <span class="n">y_mem_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.58255198397046815</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;countvectorizer__min_df&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
 <span class="s1">&#39;countvectorizer__ngram_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
 <span class="s1">&#39;logisticregression__C&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">}</span>
</pre></div>
</div>
<p>]</p>
<p>We can do a grid search, though that might take a while on
the bigger dataset.</p>
<p>Things that you want to tune are a regularization of your
models, I always use logistic regression CV to tune it
internally. If I want to tune multiple things at once, I
could use as a pipeline grid search.</p>
<p>Here, this is for character n-grams, but still, I might want
to tune the n-gram range. If I was using bag of words, I
might not look at sequences of 8 words because it will take
very long to compute this and it’s probably not going to be
useful.</p>
<p>Here, I’m using count vectorizer, but I could still
normalize using the L2 normalizer.</p>
<p>Another thing, if you do something like this, you don’t want
to fit each step of the pipeline every time. So some of
these steps for the count vectorizer, if you have a big
dataset, it might be pretty expensive and so you don’t want
to refit this again, just because you’re plugged in the
normalizer or just because you changed the C parameter in
the logistic regression.</p>
<p>Make pipeline has a parameter called memory that allows you
to cash the results. So you just have to give it a non-empty
string and it’ll use that as the full name to catch the
results. If there’s no catch eviction, at some point you
need to delete it, or your hard drive will run full.</p>
<p>Here, you can see that after tuning everything, I actually
get a result that’s much better than what I had before. So
before I had an f1 macro of 0.45 and now I have 0.58.</p>
<p>So min_df=2 and ngram_range: (1,5) and c=10 gives the best
results.</p>
<ul class="simple">
<li><p>Small dataset, makes grid-search faster! (less reliable)</p></li>
</ul>
<p>.center[
<img alt=":scale 100%" src="../_images/grid_search_table.png" />
]</p>
<p>Here, I used pivot tables and Pandas to look into the effect
of the different parameters.</p>
</div>
<div class="section" id="other-features">
<h2>Other features<a class="headerlink" href="#other-features" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Length of text</p></li>
<li><p>Number of out-of-vocabularly words</p></li>
<li><p>Presence / frequency of ALL CAPS</p></li>
<li><p>Punctuation…!? (somewhat captured by char ngrams)</p></li>
<li><p>Sentiment words (good vs bad)</p></li>
<li><p>Whatever makes sense for the task!</p></li>
</ul>
<p>There are other kinds of features that you might want to
look into.</p>
<p>There are actual collections of positive-negative words that
you can download and you can check how many positive words
are in this, how many negative words are in this. So
basically, the model can share information about how
important these words are because a particularly positive
word might appear only very infrequently in your dataset but
you can still learn that positive word means good.</p>
<p>Bag of words is a pretty good baseline. It’s a very simple
approach and often gives reasonable results.</p>
</div>
<div class="section" id="large-scale-text-vectorization">
<h2>Large Scale Text Vectorization<a class="headerlink" href="#large-scale-text-vectorization" title="Permalink to this headline">¶</a></h2>
<p>The last thing I want to mention is how you can scale this
up for really large scale stuff.</p>
<p>Basically just a very slight modification. So for example,
if you want to use training a model on the Twitter stream,
which has tweets coming in all the time, this has a couple
of problems.</p>
<ul class="simple">
<li><p>You don’t want to store all the data</p></li>
<li><p>Your vocabulary might shift over time. You might not be
able to store the whole vocabulary, because the whole
vocabulary of what everybody says on Twitter is very large.</p></li>
</ul>
<p>There’s a trick which allows you to get away without
building a vocabulary.</p>
<p>.center[
<img alt=":scale 90%" src="../_images/bag_of_words.png" />
]</p>
<p>This is what we did before. We tokenize the string, split
onto words, build a vocabulary and then built a sparse
matrix representation. Now we can replace this vocabulary
with just a hash function.</p>
<p>.center[
<img alt=":scale 80%" src="../_images/large_scale_text_vec_2.png" />
]</p>
<p>And we can use any string hash function, that computes half
of the token, then we represent this token just by its hash
number.</p>
<p>Here, I used a hash method. This just hashes to 832,412 and
so this is the number of the feature. Then I can use these
as indices into my sparse matrix encoding. So now I need a
hash function with some upper limits, let’s say 1 billion or
something, and as I get a feature vector of length 1
billion. And then I can use these hashes as indices into
this feature vector.</p>
<p>So I don’t need to build a vocabulary which is pretty nice.</p>
</div>
<div class="section" id="trade-offs">
<h2>Trade-offs<a class="headerlink" href="#trade-offs" title="Permalink to this headline">¶</a></h2>
<p>.left-column[
Pro:</p>
<ul class="simple">
<li><p>Fast</p></li>
<li><p>Works for streaming data</p></li>
<li><p>Low memory footprint
]
.right-column[
Con:</p></li>
<li><p>Can’t interpret results</p></li>
<li><p>Hard to debug</p></li>
<li><p>(collisions are not a problem for model accuracy)</p></li>
</ul>
<p>]</p>
<p>Here are pros and cons listed.</p>
<p>It’s hard to interpret the results because now you have like
a feature vector of size million, but you don’t know what
the features correspond to. Previously we had a vocabulary
and we could say feature 3 corresponds to X and now we don’t
know what it is.</p>
<p>You could try to store the correspondences but that would
sort of remove all the benefits. So that makes it kind of
hard to debug.</p>
<p>There can be hash collisions, which make it harder to
interpret. 2 tokens can hash to the same index. In practice,
that’s not really a problem for accuracy. But it’s a problem
for interpretability because you can’t undo the hash
function so you don’t know why your model learned the thing
it learned. But if you have really big text data or
streaming setting, this is a quite simple and effective
method.</p>
</div>
<div class="section" id="near-drop-in-replacement">
<h2>Near drop-in replacement<a class="headerlink" href="#near-drop-in-replacement" title="Permalink to this headline">¶</a></h2>
<p>.smallest[</p>
<ul class="simple">
<li><p>Careful: Uses l2 normalization by default!
]</p></li>
</ul>
<p>.tiny-code[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">HashingVectorizer</span>
<span class="n">hv</span> <span class="o">=</span> <span class="n">HashingVectorizer</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_val</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">HashingVectorizer</span>
<span class="n">hv</span> <span class="o">=</span> <span class="n">HashingVectorizer</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">hv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_val</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">18750</span><span class="p">,</span> <span class="mi">1048576</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
</pre></div>
</div>
<p>]</p>
<p>This is a drop-in replacement for count vectorizer. It’s the
hashing vectorizer. Here, you can see by default, it
actually uses a hash base of about a million.</p>
<p>The result was about the same. It can be faster, and you get
away without storing the vocabulary</p>
</div>
<div class="section" id="other-libraries">
<h2>Other libraries<a class="headerlink" href="#other-libraries" title="Permalink to this headline">¶</a></h2>
<div class="section" id="nltk">
<h3><a class="reference external" href="https://www.nltk.org/">nltk</a><a class="headerlink" href="#nltk" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Classic, comprehensiv, slightly outdated</p></li>
</ul>
</div>
<div class="section" id="spacy">
<h3><a class="reference external" href="https://spacy.io/">spaCy</a><a class="headerlink" href="#spacy" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>recommended; modern, fast; API still changing in parts.</p></li>
</ul>
</div>
<div class="section" id="gensim">
<h3><a class="reference external" href="https://radimrehurek.com/gensim/">gensim</a><a class="headerlink" href="#gensim" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>focus on topic modeling</p></li>
</ul>
<div class="section" id="w4995-applied-machine-learning">
<h4>W4995 Applied Machine Learning<a class="headerlink" href="#w4995-applied-machine-learning" title="Permalink to this headline">¶</a></h4>
</div>
</div>
</div>
<div class="section" id="lsa-topic-models">
<h2>LSA &amp; Topic Models<a class="headerlink" href="#lsa-topic-models" title="Permalink to this headline">¶</a></h2>
<p>04/13/20</p>
<p>Andreas C. Müller</p>
<p>Today, I’m going to talk about Latent Semantic Analysis and topic models, and a bit of Non-negative matrix factorization.
The overarching theme is extracting information from text corpuses in an
unsupervised way.</p>
<p>FIXME How to apply LDA (and NMF?) to test-data
FIXME WHY / How
FIXME steal from Tim Hoppers talk!! https://www.youtube.com/watch?v=_R66X_udxZQ
FIXME do MCMC LDA (code)?
FIXME explain MCMC?
FIXME classification with logisticregressioncv  instead of fixed C?
FIXME preprocessing for LDA: mention no tfidf.
FIXME Classification with NMF.
FIXME Classification with LDA vs LSA?
FIXME add non-negative constraint on NMF to objective slide?</p>
</div>
<div class="section" id="beyond-bags-of-words">
<h2>Beyond Bags of Words<a class="headerlink" href="#beyond-bags-of-words" title="Permalink to this headline">¶</a></h2>
<p>Limitations of bag of words:</p>
<ul class="simple">
<li><p>Semantics of words not captured</p></li>
<li><p>Synonymous words not represented</p></li>
<li><p>Very distributed representation of documents</p></li>
</ul>
<p>One possible reason to do this is that you want to get a
representation of your text data that’s more semantic.</p>
<p>In bag of words, the semantics of the words are not really
captured at all. Synonymous words are not represented, if
you use a synonym it will be like a completely different
feature and will have nothing to do with another feature.
The representation is very distributed, you have hundreds of
thousands of features and it’s kind of hard to reason about
such a long vector sometimes, in particular, if you have
longer documents.</p>
<p>Today, we’re going to talk about classical unsupervised
methods. And next time, we’re going to talk about word
vectors and word embedding, which also tries to solve the
same problem of getting more semantically meaningful
representations for text data.</p>
</div>
<div class="section" id="topic-models">
<h2>Topic Models<a class="headerlink" href="#topic-models" title="Permalink to this headline">¶</a></h2>
<p>Next, I want to talk a little bit about models that are
somewhat more specific to the text data, or that allows us
maybe to interpret the text data a little bit more clearly.</p>
<p>The idea of topic models is basically that each document is
created as a mixture of topics. Each topic is a distribution
over words. And we want to learn the topics that are
relevant to our corpus and which of these topics contribute
to each individual document.</p>
<p>Topics could be the genre of movie, or the sentiment, or the
comment writer sophistication, or whether English is their
native language or so on. And so each of them is would be
topics. So the topic is not necessarily sorted of semantic
topic, but something that influences the choice of
vocabulary for a given document.</p>
<p>For each document, you assume that several of these topics
are active and others are not and they have very varying
importance. So movies could be mixtures of different genres
and then they could have words from multiple genres.</p>
</div>
<div class="section" id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Each document is created as a mixture of topics</p></li>
<li><p>Topics are distributions over words</p></li>
<li><p>Learn topics and composition of documents simultaneously</p></li>
<li><p>Unsupervised (and possibly ill-defined)</p></li>
</ul>
<p>This is an unsupervised task, so it’s probably ill-defined
and so results are open to interpretation, and also depend
on how exactly you formulate the task.</p>
<p>#Matrix Factorization</p>
<p>.center[
<img alt=":scale 80%" src="../_images/matrix_factorization1.png" />
]</p>
<p>So matrix factorization algorithm works like this. We have
our data matrix, X, which is number of samples times number
of features. And we want to factor it into two matrices, A
and B.</p>
<p>A is number of samples times k and B is k times number of
features.</p>
<p>If we do this in some way, we get for each sample, a new
representation in k-dimensional, each row of k will
correspond to one sample and the columns of B will
correspond to the input features. So they encode how the
entries of A correspond to the original data. Often this is
called latent representation. So, these new features here in
A, somehow represent the rows of X and the features are
encoded in B.</p>
<p>#Matrix Factorization</p>
<p>.center[
<img alt=":scale 80%" src="../_images/matrix_factorization_21.png" />
]
sklearn speak: <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">=</span> <span class="pre">mf.transform(X)</span></code></p>
<p>#PCA</p>
<p>.center[
<img alt=":scale 80%" src="../_images/pca1.png" />
]</p>
<p>The one example of this that we already saw is principal
component analysis. In PCA, B was basically the rotation and
then a projection. And A is just the  projection of X.</p>
<p>This is just one particular factorization you can do. I have
the sign equal, but usually, more generally, you’re trying
to find matrixes A and B so that this is as close to equal
as possible.</p>
<p>In particular, for PCA, what we’re trying to do is we try to
find matrices A and B so that the rows of B are orthogonal,
and we restrict the rank of k. K would be the number of
components and we get PCA, if we restrict the columns of B
to be orthogonal, then the product of matrixes A and B is
most close to X in the least square sense is the principal
component analysis.</p>
</div>
<div class="section" id="latent-semantic-analysis-lsa">
<h2>Latent Semantic Analysis (LSA)<a class="headerlink" href="#latent-semantic-analysis-lsa" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Reduce dimensionality of data.</p></li>
<li><p>Can’t use PCA: can’t subtract the mean (sparse data)</p></li>
<li><p>Instead of PCA: Just do SVD, truncate.</p></li>
<li><p>“Semantic” features, dense representation.</p></li>
<li><p>Easy to compute - convex optimization</p></li>
</ul>
<p>LSA is basically just the same as PCA. The idea is to reduce
the dimensionality of the data to some semantically
meaningful components. The thing is, we can’t easily do PCA
here, because we can’t remove the mean. Remember, this is a
sparse dataset and so the zero is sort of meaningful and if
we shift the zero, like by trying to subtract the mean, we
will get a density dataset that won’t have many zeros
anymore and so we won’t be able to even store it.</p>
<p>There are ways to still compute PCA without explicitly creating
a big dense matrix, but in the traditional LSA, we just
ignore that and do a singular value decomposition of the data.
This is exactly the
same computation as PCA, only, we don’t subtract the mean.</p>
<p>This is very easy to compute. You can do the SVD very
easily. And there’s a unique solution that you can compute.
And then you have something that’s like a dense
representation of the data and hopefully, it’s semantic in
some way.</p>
<p>As with all unsupervised methods, this can be hit or miss a
little bit.</p>
</div>
<div class="section" id="lsa-with-truncated-svd">
<h2>LSA with Truncated SVD<a class="headerlink" href="#lsa-with-truncated-svd" title="Permalink to this headline">¶</a></h2>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">25000</span><span class="p">,</span> <span class="mi">30462</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>
<span class="n">lsa</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">X_lsa</span> <span class="o">=</span> <span class="n">lsa</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">lsa</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">30462</span><span class="p">)</span>
</pre></div>
</div>
<p>]</p>
<p>So the way to do this with scikit-learn is with truncated
SVD. Truncated SVD is basically the implementation of PCA
that doesn’t subtract the mean. We didn’t call it LSA,
because if you do this in the context of text processing,
you can do this whenever you have a sparse dataset, you
would want to do something like PCA but it’s sparse, then
you can use the truncated SVD.</p>
<p>Here, I’m just using the training set, 25,000 documents, I
removed the stop words, and I set min_df=4.</p>
<p>So basically, I removed the most common words, which are
stop words, and I removed the very infrequent words. So I
have a more reasonably sized vocabulary. And then I did a
truncated SVD and extract 100 components. And so the
components I extracted, are 100 X number of features. So I
have 100 vectors, where each feature corresponds to one of
the words in the vocabulary.</p>
<p>.smaller[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">lsa</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
</pre></div>
</div>
<p>]</p>
<p>.center[
<img alt=":scale 70%" src="../_images/lsa_truncated_svd_plot.png" />
]</p>
<p>As we saw with PCA, we can look at the explained variance
ratio.</p>
<p>Here’s is a semi-log scale. And you can see that the first
one or two explain a lot and then rapidly decreases. A lot
is captured in the first 10, and then it goes down.</p>
<p>But still at 100, there’s still a lot of variances left, so
it’ll just probably go down logarithmically as it goes on.
We can also look at the eigenvectors with the highest
singular values.</p>
</div>
<div class="section" id="first-six-eigenvectors">
<h2>First Six eigenvectors<a class="headerlink" href="#first-six-eigenvectors" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 100%" src="../_images/lsa_six_eigvec.png" />
]</p>
<p>Here are the 6 first eigenvectors. I’m showing only the
entries that have the largest magnitude, either positive or
negative.</p>
<p>I’m showing the 10 largest magnitudes entries of each of the
eigenvectors. So again, keep in mind that the sign doesn’t
really mean anything, since its eigenvalues, but sort of the
relative sign means something.</p>
<p>So the first one is sort of what you would usually get.
They’re all positive because all entries in the data matrix
are positive since its bag of words. This is sort of, in a
sense, just translating into the middle of the data,
somewhat similar to trying to model the mean. ‘Movie’ and
‘film’ are obviously the most common words.</p>
<p>The second eigenvector is whether someone uses the word
‘movies’ or ‘films’. You can see that either someone uses
‘film’ and ‘films’ or ‘movie’ and ‘movies’. Basically,
people don’t usually use both of them in the same comment.
It is just interesting that this is the one of the largest
components of variances, whether a person uses this one word
or this other word that is completely synonymous.</p>
<p>Points in direction of mean</p>
</div>
<div class="section" id="scale-before-lsa">
<h2>Scale before LSA<a class="headerlink" href="#scale-before-lsa" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MaxAbsScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MaxAbsScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="n">lsa_scaled</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">X_lsa_scaled</span> <span class="o">=</span> <span class="n">lsa_scaled</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</pre></div>
</div>
<p>There are 2 things that you can do, you can normalize for
the length of the document, or you can scale so that all the
features are on the same scale.</p>
<p>MaxAbsScaler performs the same way as a standard scaler,
only it doesn’t subtract the mean. So it just scales
everything to have the same maximum absolute value. And so
in this sense, I’m basically scaling down film and movie to
have the same importance as the
other words. I can also normalize and get rid of the length
of the document. But I don’t think it actually had an effect
here. And then I can compute the same thing.</p>
<ul class="simple">
<li><p>“Movie” and “Film” were dominating first couple of components. Try to get rid of that effect.</p></li>
</ul>
</div>
<div class="section" id="eigenvectors-after-scaling">
<h2>Eigenvectors after scaling<a class="headerlink" href="#eigenvectors-after-scaling" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 100%" src="../_images/lsa_six_eigvec_scaled.png" />
]</p>
<p>This is the first six eigenvectors of LSA after scaling. The
first one still points towards the direction of the mean. So
I found some components like the third one here, which are
interesting because I interpreted this as like a writer
sophistication. So there are some people that use a lot of
very short words and some people use a lot of very long
words. So people that use words like ‘cinematography’ also
use words like ‘performance’ and ‘excellent’. This also
comes out from some of the other methods.</p>
<p>And so I looked at a couple of these components and it
turned out that the component 1 and the component 3 are
actually related to the good and bad review.</p>
<ul class="simple">
<li><p>Movie and film still important, but not that dominant any more.</p></li>
</ul>
</div>
<div class="section" id="some-components-capture-sentiment">
<h2>Some Components Capture Sentiment<a class="headerlink" href="#some-components-capture-sentiment" title="Permalink to this headline">¶</a></h2>
<p>.smallest[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_lsa_scaled</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_lsa_scaled</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>]</p>
<p>.center[
<img alt=":scale 35%" src="../_images/capture_sentiment.png" />
]</p>
<p>.smallest[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>lr_lsa = LogisticRegression(C=100).fit(X_lsa_scaled[:, :10], y_train)
lr_lsa.score(X_test_lsa_scaled[:, :10], y_test)```
</pre></div>
</div>
<p>0.827</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>```python
lr_lsa.score(X_lsa_scaled[:, :10], y_train)
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.827</span>
</pre></div>
</div>
<p>]</p>
<p>This scatter plot is positive reviews versus negative
reviews. Some of the components actually capture the
semantic attributes we are interested in.</p>
<p>It was completely unsupervised, we didn’t put that in and
this is just a bunch of movie reviews. But these two
components, actually, seem to be pretty related to the
sentiment of the comment writer.</p>
<p>I also run a logistic regression on 10 components and it was
at 83% accuracy.</p>
<p>I thought that was actually pretty good. It was 88% using
the bag of word representation. So you got 88% if you used
30,000 dimensions, and you get 83% if you used 10
dimensions. So it is completely unsupervised compression of
the data. So it’s not as good as the original data set but
if you take 100 components you recover the same accuracy.</p>
<p>So these components definitely capture something that’s
semantic, and it allows us to classify even if we use lower
dimensional representation.</p>
<ul class="simple">
<li><p>Not competitive but reasonable with just 10 components</p></li>
</ul>
</div>
<div class="section" id="nmf-for-topic-models">
<h2>NMF for topic models<a class="headerlink" href="#nmf-for-topic-models" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 80%" src="../_images/nmf1.png" />
]</p>
<p>The first model I want to use for this is NMF.</p>
<p>In NMF, remember, we have a data matrix, X, which is now our
large sparse matrix of word counts and we factorize this
into HxW, H is the hidden or latent representation, and W
represents the weights.</p>
</div>
<div class="section" id="nmf-loss-and-algorithm">
<h2>NMF Loss and algorithm<a class="headerlink" href="#nmf-loss-and-algorithm" title="Permalink to this headline">¶</a></h2>
<p>Frobenius loss / squared loss ($ Y = H W$):</p>
<div class="math notranslate nohighlight">
\[ l_{\text{frob}}(X, Y) = \sum_{i,j} (X_{ij} - Y_{ij})^ 2\]</div>
<p>Kulback-Leibler (KL) divergence:
<span class="math notranslate nohighlight">\($ l_{\text{KL}}(X, Y) = \sum_{i,j} X_{ij} \log\left(\frac{X_{ij}}{Y_{ij}}\right) - X_{ij}  + Y_{ij}\)</span>$</p>
</div>
<div class="section" id="s-t-h-w-all-entries-positive">
<h2>s.t. H, W all entries positive<a class="headerlink" href="#s-t-h-w-all-entries-positive" title="Permalink to this headline">¶</a></h2>
<p>optimization :</p>
<ul class="simple">
<li><p>Convex in either W or H, not both.</p></li>
<li><p>Randomly initialize (PCA?)</p></li>
<li><p>Iteratively update W and H (block coordinate descent)</p></li>
</ul>
<p>–</p>
<p>Transforming data:</p>
<ul class="simple">
<li><p>Given new data <code class="docutils literal notranslate"><span class="pre">X_test</span></code>, fixed <code class="docutils literal notranslate"><span class="pre">W</span></code>, computing <code class="docutils literal notranslate"><span class="pre">H</span></code> requires optimization.</p></li>
</ul>
<p>In literature: often everything transposed</p>
<p>You want X to be close to HxW, that’s the whole goal. There
are 2 common ways in which people measure this.</p>
<p>The first one at the top is called x norm, it’s just least
squares basically. So you look at the least squares
difference element-wise between the entries in the X and the
entries in the product, 〖HW〗_ij. This one is the mostly
common used one and also the default in scikit-learn.</p>
<p>The bottom one is the one which was originally proposed,
which is basically, a version of the KDE, which is usually a
measure that finds similarities between distributions. But
here, it’s applied basically to element-wise entries of this
matrix. The authors use that because then they could drive
nice ways to minimize this.</p>
<p>Minimizing either of these is not convex but if you hold one
of the two fixes, then minimizing the other one is convex.
So you usually iterate doing either gradient descent or
coordinate descent on H and then doing it on W, then you’re
iterate between the two.</p>
</div>
<div class="section" id="id3">
<h2>NMF for topic models<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 95%" src="../_images/nmf_1.png" />
]</p>
<p>When applying this to text, the rows of X, corresponds to
movie reviews, the columns of X correspond to words. So that
means the rows of H, correspond to documents, and the
columns of H correspond to the importance of a given topic
for a given document. The rows in W corresponds to topics,
one row in W is lengths of vocabulary that states the words
that are important for a particular topic. So you might have
a component with like horror, scary, monster and so on.</p>
<p>If the first topic is about horror movies, and the first
movie is a horror movie, then it would be a large first
component. Since everything is positive, and so the additive
components are added together. That possibly makes it a
little bit more interpretable.</p>
<ul class="simple">
<li><p>Each row of W corresponds to one “topic”</p></li>
</ul>
</div>
<div class="section" id="nmf-on-scaled-data">
<h2>NMF on Scaled Data<a class="headerlink" href="#nmf-on-scaled-data" title="Permalink to this headline">¶</a></h2>
<p>.smallest[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nmf_scale</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">nmf_scale</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</pre></div>
</div>
<p>]</p>
<p>.center[
<img alt=":scale 65%" src="../_images/nmf_scaled_plot.png" />
]</p>
<p>Sorted by “most active” components</p>
<p>NMF, as I said, is an iterative optimization so this runs
way longer than the LSA.</p>
<p>Q: What’s the variable ’tol’ do?
A: It’s stopping tolerance.</p>
<p>These are subsets of the columns of W. I’m visualizing the
rows of W and I’m only looking at the ones that have large
absolute values. So the Y values are the weight that this
word has inside this topic. So the word ‘really’ has the
weight of 7 in this first topic.</p>
<p>So obviously, they have entries for all words, but I didn’t
put in a penalty so that they’re sparse. So they’ll have
non-zero entries for all words, but sort of the interesting
ones are the big ones.</p>
<p>The way that I picked these 9 components here out of the 100
components was the ones that have the highest activation in
X. I, then realize it might not be the most interesting
thing. Might be more interesting to look at the ones that
are less peaked.</p>
<p>If you look at more components, you’ll find other
genre-specific components.</p>
<p>One of the things you can do with this is you can find
genre specific reviews by looking at the component that
corresponds to it.</p>
</div>
<div class="section" id="nmf-components-without-scaling">
<h2>NMF components without scaling<a class="headerlink" href="#nmf-components-without-scaling" title="Permalink to this headline">¶</a></h2>
<p><img alt=":scale 70%" src="../_images/nmf_topics_no_scaling.png" /></p>
<p>I did the same thing without scaling.</p>
</div>
<div class="section" id="nmf-with-tfidf">
<h2>NMF with tfidf<a class="headerlink" href="#nmf-with-tfidf" title="Permalink to this headline">¶</a></h2>
<p><img alt=":scale 70%" src="../_images/nmf_topics_tfidf.png" /></p>
<p>You can also run this on TF-IDF. Again, results are somewhat
similar to before. The top-ranked components are now
different. It really only makes sense if you look at all 100
components, but it’s very hard to do. So I picked out just a
couple.</p>
<p>This was with a 100 components. That said, if you use fewer
components, you get something that is much more general.</p>
</div>
<div class="section" id="nmf-with-tfidf-and-10-components">
<h2>NMF with tfidf and 10 components<a class="headerlink" href="#nmf-with-tfidf-and-10-components" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 70%" src="../_images/tfidf_10comp_plot.png" />
]</p>
<p>This is using 9 components. The first one, probably kind of
models the mean and points in the center.</p>
<p>Q: is the task here still to classify between good and bad
reviews?</p>
<p>The application of that I had in the back of my head is I’m
interested in the reviews, good or bad, but this is all
completely unsupervised. So I never put in anything about
these two classes even existing. This is just trying to help
me understand corpus overall.</p>
<p>The main things that I found so far are that it finds things
that are genre specific, some of them are very specific,
like Star Wars and DC comics, some of them are less specific
like horror movies, but then it finds some that are about
sentiments.</p>
<p>The reason why I call these topics is since they are
additive. And so maybe someone would argue that this is not
really a topic model, because it’s not technically a topic
perfect model but it does exactly the same as a topic model
does. It does a positive decomposition. I think that’s
easier to interpret in this context. Because what does it
mean for a document to be not about horror movies.</p>
<p>Less number of components will give you more generic things.
If you use more number of components, there might be generic
components. If you want to force them to be generic, you can
use fewer components. Using fewer components definitely
makes it easy to look at them.</p>
<p>A friend of mine who is an expert on topic models told me to
always use thousands of components, and then go through all
of them and see if something’s interesting.</p>
<p>#Latent Dirichlet Allocation</p>
</div>
<div class="section" id="the-other-lda">
<h2>(the other LDA)<a class="headerlink" href="#the-other-lda" title="Permalink to this headline">¶</a></h2>
<p>(Not Linear Discriminant Analysis)</p>
<p>Next thing I want to talk about is LDA. We are going from
simple models to more complex model.</p>
<p>What people think about if they hear topic model is usually
LDA. Dave Blei in the sets department invented LDA. This is
like an incredibly popular model right now.</p>
</div>
<div class="section" id="the-lda-model">
<h2>The LDA Model<a class="headerlink" href="#the-lda-model" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 90%" src="../_images/lda_model.png" />
]
.smallest[
(stolen from Dave and John)
]</p>
<p>This a probabilistic generative model. The idea is you write
down a generative process of how each document is created.
This is similar to mixture models where basically, for each
point you want to create, you picked one of the Gaussians
and then you sample from it. That’s how we create points in
this distribution.</p>
<p>Similarly, in the LDA model, let’s say, you’re given some
topics and these topics have, like word importance. Given
the topics, the way that you generate a document would be,
first, you draw which of these topics are important, and how
important are they?</p>
<p>Each document is a mixture of topics and then for each word,
you decide which of these topics to you want to draw a word
from. And then you draw a word from that topic, according to
the word importance.</p>
<p>This is clearly not really how text is generated but it’s
sort of reasonable proxy. This is based on the bag of word
model so it only looks at word counts. It doesn’t do that
context at all. You could do it on bigrams, but that’s not
what people usually do.</p>
<p>The idea is how likely is it that a word appears in this
document and basically, for each word individually, you pick
a topic, and then you generate a word from this topic and in
the end, you end up with a big bag of word representation.
The idea is that this a generative process, given corpus of
documents, can I actually do inference in this? Can I find
out what were the topics that generated this model? And what
are the topic proportions for each of the documents? And
what are the word assignments?</p>
<p>So basically, you write down this generative process and
then given your documents, you’re trying to invert it and
figure out what were the topics.</p>
<ul class="simple">
<li><p>Generative probabilistic model (similar to mixture model)</p></li>
<li><p>Bayesian graphical model</p></li>
<li><p>Learning is probabilistic inference</p></li>
<li><p>Non-convex optimization (even harder than mixture models)</p></li>
</ul>
<p>.center[
<img alt=":scale 70%" src="../_images/lda_params.png" />
]</p>
<p>.smallest[</p>
<ul class="simple">
<li><p>For each topic <span class="math notranslate nohighlight">\(k\)</span>, draw <span class="math notranslate nohighlight">\(\phi_k \sim \text{Dirichlet}(\beta), k = 1 ... K\)</span></p></li>
<li><p>For each document <span class="math notranslate nohighlight">\(d\)</span> draw <span class="math notranslate nohighlight">\(\theta_d \sim \text{Dirichlet}(\alpha), d = 1 ... D\)</span></p></li>
<li><p>For each word <span class="math notranslate nohighlight">\(i\)</span> in document <span class="math notranslate nohighlight">\(d\)</span>:</p>
<ul>
<li><p>Draw a topic index <span class="math notranslate nohighlight">\(z_{di} \sim \text{Multinomial}(\theta_d)\)</span></p></li>
<li><p>Draw the observed word <span class="math notranslate nohighlight">\(w_{ij} \sim \text{Multinomial}(\phi_z)\)</span></p></li>
</ul>
</li>
<li><p>(taken from Yang Ruan, Changsi An (http://salsahpc.indiana.edu/b649proj/proj3.html)
]</p></li>
</ul>
<p>You can write down this generative model in flight
notations. I don’t think we looked at the plate notation yet
because I don’t find it very helpful.</p>
<p>So this is a depiction of the graphical model where
basically, each of these circles is a variable. And these
red things are called plates and they mean a variable is
duplicated.</p>
<p>So here, there are 2 parameters you specified beforehand,
which are the hyperparameters, Alpha and Beta.</p>
<p>Then there are K topics, I fixed the number of topics and I
call them K. And for each topic, I have a distribution over
words, like how important is each word in each topic. Then D
goes over documents. So for each document, I have a
distribution over the topics that states that a particular
document is so much about this topic, and so much about
another topic.</p>
<p>So for each document, I have a vector that tells me the
topic proportions. Finally, inside the document, for each
word I have a variable, that tells me which topic the word
belongs to and then given the topic, what is the word.</p>
<p>Here, this outer plate is replicated along all the documents
and then inside each document, for each word, there’s the
inner plate.</p>
<p>But the only thing that I actually observe is basically the
W, which is the word count. So I observed which words
appeared in these documents. I also set alpha and beta as
some prior parameters usually.</p>
<p>What I want to find out is the topic distribution over words
and the documents specific distributions over topics. So I
want to know what the topics are and what topics are a given
document about.</p>
<p>This is very similar to the thing that we had in NMF.</p>
<p>The flight notation graph can be read as
A) The generative process or
B) Dependency structure in the probability distribution.</p>
<p>So basically, the distribution that you can easily think of
is like, what should the work distribution be? And so the
word, I choose it from a discreet of distribution, like
given a particular topic, I take the distribution of words
in this topic and I pick from that distribution.</p>
<p>So this is a categorical distribution where you say, under
this distribution, each word is so and so likely. So this
multinomial distribution, this is how I pick a word given a
topic.</p>
<p>But the distribution in this is Dirichlet data
distributions.</p>
<p>Basically, you want the distribution over distribution, you
want the distribution where the outcome is multinomial
distribution, and for the W, I want multinomial
distributions and so the pi, the outcome of drawing there,
needs to be multinomial distribution.</p>
<p>The national prior over things that give me multinomial
distributions is called Dirichlet.</p>
<ul class="simple">
<li><p>K topics = multinomial distributions over words</p></li>
<li><p>“mixture weights” for each document:</p>
<ul>
<li><p>How important is each topic for this document</p></li>
<li><p>Each document contains multiple topics!</p></li>
</ul>
</li>
</ul>
<div class="section" id="multinomial">
<h3>Multinomial<a class="headerlink" href="#multinomial" title="Permalink to this headline">¶</a></h3>
<p>.smaller[It models the probability of counts for rolling a k-sided die n times.]
.quote_author[wikipedia]</p>
<div class="math notranslate nohighlight">
\[ P(x_1, \dots, x_k |n, p_1, \dots, p_k) = \frac{n!}{x_1! \dots x_k!} p_1^{x_1} \dots p_k^{x^k}\]</div>
<p>–</p>
</div>
<div class="section" id="dirichlet">
<h3>Dirichlet<a class="headerlink" href="#dirichlet" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[ P(x_1, \dots, x_k | \alpha_1, \dots \alpha_k)=\frac{1}{B(\alpha)} x_1^{\alpha_1 - 1} \dots x_k^{\alpha_k - 1} \]</div>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(p(x|\theta)\)</span> is multimnomial (discrete distribution),
then <span class="math notranslate nohighlight">\(p(\theta) = \text{Dirichlet}(...)\)</span> is a conjugate prior.</p></li>
</ul>
<p>This is how it looks in formula. For multinomial, let’s say
I draw n words from a multinomial. There are k possible
words, I draw n, so I draw the same word multiple times, and
the probability of drawing a particular word is P1 to Pk. So
then the probability of a specific draw is P1 to X1, and so
on from Pk to the Xk and then normalized.</p>
<p>Dirichlet is basically the same, only the other way around.
You can see that here, the Ps in multicultural distribution
become the Xs in Dirichlet distribution. Dirichlet looks
exactly the same. Only now, you have a distribution over
these base probabilities.</p>
<p>That makes it very easy to include the data into the prior
information and compute the posterior.</p>
</div>
</div>
<div class="section" id="conjugate-prior">
<h2>Conjugate Prior<a class="headerlink" href="#conjugate-prior" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Prior is called “conjugate” of the posterior has the same form as prior
<span class="math notranslate nohighlight">\($ P(\theta | x) = \frac{p(x | \theta)p(\theta)}{\int_{}^{} p(x | \theta^\prime)p(\theta^\prime)d\theta^\prime} $\)</span></p></li>
</ul>
<p>The reason why we’re using generative prior is because of
the conjugate prior of the multinomial distribution.  Let’s
say we have a prior over our word topic distribution and
then we observe some words belonging to some of the topics,
we have some evidence that these words are drawn from these
topics and then we want to update our word topic prior,
given the information that we saw.</p>
<p>p(x|θ) would be our multinomial distribution, meaning given
the topic, what are the multinomial distribution. And
basically, we want p(θ|x) to be easy to compute. And it’s
easiest to compute if we use the conjugate prior here,
because then posterior will have the same distribution as
the prior.</p>
</div>
<div class="section" id="dirichlet-distributions">
<h2>Dirichlet Distributions<a class="headerlink" href="#dirichlet-distributions" title="Permalink to this headline">¶</a></h2>
<p>.left-column[
PDF:
<span class="math notranslate nohighlight">\(\qquad \frac{1}{B(\alpha)} \Pi_{i=1}^{K}{x_i^{\alpha_i - 1}}\)</span>
]</p>
<p>.right-column[
Mean:
<span class="math notranslate nohighlight">\(\qquad E[X_i] = \frac{\alpha_i}{\sum_k{\alpha_k}}\)</span>
]</p>
<p>.center[
<img alt=":scale 60%" src="../_images/dirichlet_dist.png" />
]</p>
<p>Here are some examples in 3D. Dirichlet distribution is like
a distribution over distributions. So a point drawn from
Dirichlet distribution is a multinomial distribution.</p>
<p>So here, let’s say you have a multinomial with 3 variables,
this lives inside the probability simplex in 3-dimensions.
The dark blue sector is sort of the space all 3 numbers that
sum up to one.</p>
<p>And so now you can try put a prior on this. So we want to
distribution over the simplex of all 3 numbers that add up
to 1. And so what we’re doing for LDA is usually, we make
this sort of symmetric in all directions because we don’t
really have a preference for like, saying one topic is more
important than the other topic, that’s not our prior
knowledge, because they are arbitrary.</p>
<p>It’s pretty likely that all the topics are equally
important, that means I have put math only in the very
center or I can say, I didn’t really know how important each
of the topics is, and then you can get points at the
corners, which means that each document can be about only a
few topics. If you use this, then each document will be
about many topics.</p>
<p>And similarly, these priors use both on the document topic
distribution, and on the topic word distribution. So
similarly, for each topic, you can have each topic being
very broad.</p>
<p>So the third one means broad, you assign probability mass to
most words.  While the first one means you can assign
probability mass only to a couple of the words.</p>
<p>.center[
<img alt=":scale 70%" src="../_images/lda_params.png" />
]</p>
<p>.smallest[</p>
<ul class="simple">
<li><p>For each topic <span class="math notranslate nohighlight">\(k\)</span>, draw <span class="math notranslate nohighlight">\(\phi_k \sim \text{Dirichlet}(\beta), k = 1 ... K\)</span></p></li>
<li><p>For each document <span class="math notranslate nohighlight">\(d\)</span> draw <span class="math notranslate nohighlight">\(\theta_d \sim \text{Dirichlet}(\alpha), d = 1 ... D\)</span></p></li>
<li><p>For each word <span class="math notranslate nohighlight">\(i\)</span> in document <span class="math notranslate nohighlight">\(d\)</span>:</p>
<ul>
<li><p>Draw a topic index <span class="math notranslate nohighlight">\(z_{di} \sim \text{Multinomial}(\theta_d)\)</span></p></li>
<li><p>Draw the observed word <span class="math notranslate nohighlight">\(w_{ij} \sim \text{Multinomial}(\beta_z)\)</span></p></li>
</ul>
</li>
<li><p>(taken from Yang Ruan, Changsi An (http://salsahpc.indiana.edu/b649proj/proj3.html)
]</p></li>
</ul>
</div>
<div class="section" id="two-schools-of-solvers">
<h2>Two Schools (of solvers)<a class="headerlink" href="#two-schools-of-solvers" title="Permalink to this headline">¶</a></h2>
<p>.left-column[
Gibbs Sampling</p>
<ul class="simple">
<li><p>Implements MCMC</p></li>
<li><p>Standard procedure for any probabilistic model</p></li>
<li><p>Very accuracte</p></li>
<li><p>Very slow</p></li>
</ul>
<p>]</p>
<p>.right-column[</p>
<p>Variational Inference</p>
<ul class="simple">
<li><p>Extension of expectation-maximization algorithm</p></li>
<li><p>Deterministic</p></li>
<li><p>fast(er)</p></li>
<li><p>Less accurate solutions</p></li>
<li><p>Championed by Dave Blei</p></li>
</ul>
<p>]</p>
<p>There’s basically 2 schools of thought on how we can best
solve this. These are ways to do inference and generalize
the models.</p>
<p>Gibbs sampling is a form of MCMC, so it’s like a stochastic
simulation. Basically, you simulate process going on for a
long time, you keep sampling and in the end, you get as a
stationary distribution, the real distribution that you’re
interested in.  This is very accurate. You can do this for
any probabilistic model. But it’s very slow because you need
to simulate the process of generating the data for a long
until it lines up with what’s in the data. Gibbs sampling a
form of sampling so it’s like a very random process.</p>
<p>The other one is variational inference. This is more from an
optimization perspective. This is very similar to the
expectation-maximization algorithm that’s used for GMMs,
which is similar to that of the KMeans idea of alternating
between assignment and maximization. This is a deterministic
algorithm, it’s quite a bit faster than Gibbs sampling
usually, but less accurate. The way that it does
approximations is not entirely clear. This is used by Dave
Blei all the time.</p>
</div>
<div class="section" id="pick-a-solver">
<h2>Pick a solver<a class="headerlink" href="#pick-a-solver" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>“Small data” (
&lt;
= 10k? Documents):</p>
<ul>
<li><p>Gibbs sampling (lda package, MALLET in Java)</p></li>
</ul>
</li>
<li><p>“Medium data” (
&lt;
= 1M? Documents):</p>
<ul>
<li><p>Variational Inference (scikit-learn default)</p></li>
</ul>
</li>
<li><p>“Large Data” (&gt;1M? Documents):</p>
<ul>
<li><p>Stochastic Variational Inference</p></li>
<li><p>SVI allows online learning (partial_fit)</p></li>
</ul>
</li>
<li><p>Tensorflow Probability https://www.tensorflow.org/probability</p>
<ul>
<li><p>Tensor-flow based framework for stochastic variational inference.</p></li>
<li><p>Used to be Edward by Dustin Tran (Blei Lab)</p></li>
</ul>
</li>
</ul>
<p>Pick a solver depending on the amount of the documents you
have.</p>
<p>Ida package (Python) and MALLET (Java) works well if you
have very few documents. Otherwise, it gets too slow.</p>
<p>Variational inference or batch variational inference, which
will be the default in scikit-learn in the future. Both are
used on medium sized documents.</p>
<p>For large data, you can do stochastic variation inference,
which is the current default in scikit-learn. But that’s
sort of an approximation to the approximation. It’s
something like doing stochastic gradient descent on the
optimization variational inference.</p>
<p>Partial_fit can be used on streaming data.</p>
<p>If you’re more serious about this, look at the works of
Dustin Tran. He created Edward, I think it’s now
incorporated into TensorFlow. It’s a library for doing
variational inference in all kinds of different graphical
models with GPU acceleration.</p>
<p>Talking about SGD, maybe it’s good if have a lot of data,
maybe it’s worth doing an approximation that allows you to
use all the data, if the other option is to do something
that’s fairly exact, but you have to subsample your data a
lot, then that might give you worse results than just doing
something that’s pretty bad.</p>
<p>SVI is maybe not the greatest optimizer but it allows you to
use large amounts of data, and so in the end to get a better
result.</p>
<p>Variational might be more sensitive to hyper parameters? Or give different results?</p>
<p>.smallest[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">learning_method</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">)</span>
<span class="n">X_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
<p>]</p>
<p>.center[
<img alt=":scale 70%" src="../_images/lda_plot_1.png" />
]</p>
<p>Implementing in scikit-learn is pretty straightforward. The
components are not ordered, and changing number of
components changes everything. Here, I do
learning_method=batch and the other one is
learning_method=online, but since the dataset is not that
big, it’ll just give me sort of worse results.</p>
<p>Everything is positive because these are now the word topic
matrices. It tells me if I picked this topic, what’s the
probability of each of these words and they’re actually on a
log scale. They sum up to 1 in a sense. This is using 10
topics, so this is pretty generic.</p>
<ul class="simple">
<li><p>Very generic, similar to NMF(n_components=10).
TV Series, “family drama”, and “history / war” topics</p></li>
</ul>
<p>.smallest[</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lda100</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_method</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">)</span>
<span class="n">X_lda100</span> <span class="o">=</span> <span class="n">lda100</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
<p>]</p>
<p>.center[
<img alt=":scale 60%" src="../_images/lda_plot_2.png" />
]</p>
<p>This showing 9 out of 100 documents.</p>
<p>.center[
<img alt=":scale 80%" src="../_images/lda_topics.png" />
]</p>
<p>Here, I picked out a couple of topics that I thought were
interesting.</p>
<p>One thing that you can do is try to interpret this. You can
use both this and NMF as a representation for
classification, for example. There’s no guarantee that it’s
a good representation, but it might be a better
representation than the bag of words.</p>
<p>If you do the NMF with 100 components, you actually get a
slightly better result than if you use just a bag of word
vectors. Doing the NMF actually helps you to get some
features that are semantic for the task of classifying good
versus bad reviews.</p>
</div>
<div class="section" id="hyper-parameters">
<h2>Hyper-Parameters<a class="headerlink" href="#hyper-parameters" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> (or <span class="math notranslate nohighlight">\(\theta\)</span>) = doc_topic_prior</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> (or <span class="math notranslate nohighlight">\(\eta\)</span>) = topic_word_prior</p></li>
<li><p>Both dirichlet distributions</p></li>
<li><p>Large value <span class="math notranslate nohighlight">\(\xrightarrow[]{}\)</span> more dispersed</p></li>
<li><p>Small value <span class="math notranslate nohighlight">\(\xrightarrow[]{}\)</span> more concentrated</p></li>
</ul>
<p>.center[
<img alt=":scale 50%" src="../_images/lda_params.png" />
]</p>
<p>I want to talk a little bit more about the specifics for
LDA. There are 2 parameters that you need to tune. Both are
Dirichlet priors over distribution, one over the topic
distribution and one over the document topic distribution.</p>
<p>Large value use means that they’re more dispersed. Small
values mean they’re more concentrated.</p>
</div>
<div class="section" id="rough-overview-of-mcmc-and-gibbs-sampling">
<h2>Rough overview of MCMC and Gibbs sampling<a class="headerlink" href="#rough-overview-of-mcmc-and-gibbs-sampling" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="markov-chain-monte-carlo-mcmc">
<h2>Markov-Chain Monte-Carlo (MCMC)<a class="headerlink" href="#markov-chain-monte-carlo-mcmc" title="Permalink to this headline">¶</a></h2>
<p><strong>Goal</strong>: Sample from complex distribution <span class="math notranslate nohighlight">\(p(X_1, ... X_n)\)</span>.</p>
<p><strong>Monte Carlo algorithm</strong>: “correct in expectation” / “correct on average”</p>
<p><strong>Markov Chain</strong>: a sequence of probability distribution, each depending only on the previous state.</p>
<p><strong>Markov Chain Monte Carlo</strong>: a sequence of probability distributions so that on average you sample from the target distribution.</p>
<p>(The stationary distribution of the Markov chain is the target distribution <span class="math notranslate nohighlight">\(p\)</span>)</p>
</div>
<div class="section" id="gibbs-sampling">
<h2>Gibbs Sampling<a class="headerlink" href="#gibbs-sampling" title="Permalink to this headline">¶</a></h2>
<p><strong>Goal</strong>: Sample from complex distribution <span class="math notranslate nohighlight">\(p(X_1, ... X_n)\)</span>.</p>
</div>
<div class="section" id="assumption-can-sample-from-conditional-p-x-i-x-j-neq-i">
<h2><strong>Assumption</strong>: Can sample from conditional <span class="math notranslate nohighlight">\(p(X_i|X_{j\neq i})\)</span><a class="headerlink" href="#assumption-can-sample-from-conditional-p-x-i-x-j-neq-i" title="Permalink to this headline">¶</a></h2>
<p>Initialize</p>
<p>Start from random initialization <span class="math notranslate nohighlight">\((x^0_1, ..., x^0_n)\)</span></p>
<p>Construct <span class="math notranslate nohighlight">\(x^{k+1}\)</span> from <span class="math notranslate nohighlight">\(x^k\)</span>:</p>
<div class="math notranslate nohighlight">
\[ x^{k+1}_0 \propto  p(X_i| x^{k}_{1}, ..., x^{k}_{n})\]</div>
<div class="math notranslate nohighlight">
\[ x^{k+1}_i \propto  p(X_i|x^{k+1}_0, ..., x^{k+1}_{i-1}, x^{k}_{i+1}, ..., x^{k}_{n})\]</div>
</div>
<div class="section" id="block-gibbs-sampling-for-lda">
<h2>(Block) Gibbs Sampling for LDA<a class="headerlink" href="#block-gibbs-sampling-for-lda" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 50%" src="../_images/lda_params.png" />
]</p>
<p>This is sort of the generative process of the data. And so
what Gibbs sampling does is, we observe the words W and we
are given the priors, alpha, and beta, so now we want to
figure out the word topic indicators and the documents
specific distribution over topics and topic distribution
over words.</p>
<p>The way that Gibbs sampling works is it basically assigns
these randomly and then draws the posterior distribution for
each of these variables, given all the other variables.</p>
<p>So if we know all the words and the topic assignments of the
words, then we can look at what’s the distribution of the
topics over words likely to be.</p>
<p>If we know alpha, we know all the topic distributions for
each document then we can also compute the document specific
distributions over topics. So basically, if you know all the
information that’s connected to give node, you can draw this
given node, given all the other information.</p>
<p>And then you just iterate these.</p>
<p>I’m not sure how to phrase this…given all the other
variables, you can randomly draw one of the variables and
then fix these and randomly draw the other variables. And
this gives you sort of a mark of change, meaning that every
time you draw this, the distribution over the other things
will change. And if you keep drawing and keep drawing, in
the end, you will get to a distribution, which is sort of
the correct posterior. Which just means it’s going to be the
most likely assignment of topics, given to documents.</p>
<p>If you do this, if you fix the document specific
distribution over topics and if you fix the topic
distribution over works, then all of the documents become
independent, because everything that you need to know about
one document is already covered in the documents specific
distribution over topics and the other documents don’t
really matter.</p>
<p>If you do it this way, you can easily paralyze it over
documents, for example.</p>
<p>–
Updates sampled from:</p>
<p><span class="math notranslate nohighlight">\(p(z_{iv} = k \, \vert \, \mathbf{\theta}_i, \phi_k) \propto \exp(\log \theta_{ik} + \log \phi_{k, y_{iv}})\)</span></p>
<p><span class="math notranslate nohighlight">\(p(\theta_i \, \vert \, z_{iv} = k, \phi_k) = \text{Dir}(\alpha + \sum_l \mathbb{I}(z_{il} = k))\)</span></p>
<p><span class="math notranslate nohighlight">\(p(\phi_k \, \vert \, z_{iv} = k, \mathbf{\theta}_i) = \text{Dir}(\beta + \sum_i \sum_l \mathbb{I}(y_{il} = v, z_{il} = k))\)</span></p>
<p>i is index for documents, v is index for words in document, k index for topics.
z is word-topic assignments.</p>
<p><span class="math notranslate nohighlight">\(z_{iv}=k\)</span> means word v in document i is assigned topic k.</p>
<p><span class="math notranslate nohighlight">\(\theta\)</span> is document topic distribution, <span class="math notranslate nohighlight">\(\theta_{ik}\)</span> is the probability of a word in document i to come from topic k.</p>
<p><span class="math notranslate nohighlight">\(y_{iv}\)</span> is the actual word (word-index) of word v in document i.</p>
<p><span class="math notranslate nohighlight">\(\phi_{k, y}\)</span> is the probability of a drawing word y from document k.
<span class="math notranslate nohighlight">\(\phi_{k, y_{iv}}\)</span> is the probability</p>
</div>
<div class="section" id="collapsed-gibbs-sampling-for-lda">
<h2>Collapsed Gibbs Sampling for LDA<a class="headerlink" href="#collapsed-gibbs-sampling-for-lda" title="Permalink to this headline">¶</a></h2>
<p>.center[
<img alt=":scale 50%" src="../_images/lda_params.png" />
]</p>
<p>Sample only <span class="math notranslate nohighlight">\(p(z_i=k|\mathbf{z}_{-i})\)</span></p>
<p>Compute <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(\phi\)</span> from <span class="math notranslate nohighlight">\(\mathbf{z}\)</span></p>
<p>If you don’t want to paralyze it, you can reformulate Gibbs
sampling into collapsed Gibbs sampling, where we only look
at the topic so we can analytically compute away π and theta
and we can look only at z, which is the topic assignments
for each word. And we can only draw the Zs for each word, in
order for each word we draw which topic this word comes
from.</p>
<p>The problem with this is if we integrate this and this (the
smallest and the largest red rectangle) then all of the
different words will be dependent. So we can’t do this in
parallel anymore, we have to go word by word and for each
word we draw, what’s the probability of this word belonging
to this topic given all the other word assignments.</p>
<p>This makes the math a little bit easier, but it means it’s
going to be sequential. Once we computed all these word
topic assignments we can then get the topic distribution
over words and the documents specific distribution over
topics (once we have all the z_is)</p>
<p>Now looking at all documents together (i goes over words and documents).
<span class="math notranslate nohighlight">\(z_{-i}\)</span> is word-topic assignments for all other words for all documents.
Now sequential problem!</p>
</div>
<div class="section" id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Rethinking LDA: Why Priors Matter - Hanna Wallach</p></li>
<li><p>LDA Revisited: Entropy, Prior and Convergence –
Zhang et. al.</p></li>
</ul>
<p>If you want to learn more about this, in particular, about
the practical aspects of this, there are 2 papers that I
really recommend.</p>
<p>Both of these talk more about how to use LDA in practice and
how to tune hyperparameters and so on.</p>
<p>One thing I want to say is that the hyperparameters behave
actually quite different for the variational inference and
the Gibbs sampling. I think the first paper uses Gibbs
sampling, and the second uses inference. Things are very
different between these two approaches, even though you’re
trying to optimize the same probabilistic model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="n">sklearn</span><span class="o">.</span><span class="n">set_config</span><span class="p">(</span><span class="n">print_changed_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="http-ai-stanford-edu-amaas-data-sentiment">
<h2>http://ai.stanford.edu/~amaas/data/sentiment/<a class="headerlink" href="#http-ai-stanford-edu-amaas-data-sentiment" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="example-application-sentiment-analysis-of-movie-reviews">
<h2>Example application: Sentiment analysis of movie reviews<a class="headerlink" href="#example-application-sentiment-analysis-of-movie-reviews" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!tree -L 2 data/aclImdb
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_files</span>

<span class="n">reviews_train</span> <span class="o">=</span> <span class="n">load_files</span><span class="p">(</span><span class="s2">&quot;data/aclImdb/train/&quot;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;neg&#39;</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">])</span>
<span class="c1"># load_files returns a bunch, containing training texts and training labels</span>
<span class="n">text_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">reviews_train</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">reviews_train</span><span class="o">.</span><span class="n">target</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;type of text_train: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">text_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;length of text_train: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;text_train[1]:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text_train</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">text_train</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;&lt;br /&gt;&quot;</span><span class="p">,</span> <span class="sa">b</span><span class="s2">&quot; &quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">text_train</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Samples per class (training):&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">reviews_test</span> <span class="o">=</span> <span class="n">load_files</span><span class="p">(</span><span class="s2">&quot;data/aclImdb/test/&quot;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;neg&#39;</span><span class="p">,</span> <span class="s1">&#39;pos&#39;</span><span class="p">])</span>
<span class="n">text_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">reviews_test</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">reviews_test</span><span class="o">.</span><span class="n">target</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of documents in test data:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Samples per class (test):&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span>
<span class="n">text_test</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;&lt;br /&gt;&quot;</span><span class="p">,</span> <span class="sa">b</span><span class="s2">&quot; &quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">text_test</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="applying-bag-of-words-to-a-toy-dataset">
<h3>Applying bag-of-words to a toy dataset<a class="headerlink" href="#applying-bag-of-words-to-a-toy-dataset" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bards_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;The fool doth think he is wise,&quot;</span><span class="p">,</span>
               <span class="s2">&quot;but the wise man knows himself to be a fool&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">vect</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">bards_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary content:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bag_of_words</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">bards_words</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bag_of_words:&quot;</span><span class="p">,</span> <span class="nb">repr</span><span class="p">(</span><span class="n">bag_of_words</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dense representation of bag_of_words:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
      <span class="n">bag_of_words</span><span class="o">.</span><span class="n">toarray</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vect</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">bag_of_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="bag-of-word-for-movie-reviews">
<h3>Bag-of-word for movie reviews<a class="headerlink" href="#bag-of-word-for-movie-reviews" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of features:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">feature_names</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First 20 features:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features 20010 to 20030:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">20010</span><span class="p">:</span><span class="mi">20030</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Every 2000th feature:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[::</span><span class="mi">2000</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean cross-validation accuracy: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]}</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">param_grid</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best cross-validation score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters: &quot;</span><span class="p">,</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train with min_df:&quot;</span><span class="p">,</span> <span class="nb">repr</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First 50 features:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[:</span><span class="mi">50</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features 20010 to 20030:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">20010</span><span class="p">:</span><span class="mi">20030</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Every 700th feature:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[::</span><span class="mi">700</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">param_grid</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best cross-validation score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id4">
<h3>Stop-words<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">ENGLISH_STOP_WORDS</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of stop words:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ENGLISH_STOP_WORDS</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Every 10th stopword:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">ENGLISH_STOP_WORDS</span><span class="p">)[::</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># specifying stop_words=&quot;english&quot; uses the build-in list.</span>
<span class="c1"># We could also augment it and pass our own.</span>
<span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train with stop words:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">param_grid</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best cross-validation score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="rescaling-the-data-with-tfidf">
<h3>Rescaling the data with TFIDF<a class="headerlink" href="#rescaling-the-data-with-tfidf" title="Permalink to this headline">¶</a></h3>
<p>\begin{equation*}
\text{tfidf}(w, d) = \text{tf} \log\big(\frac{N + 1}{N_w + 1}\big) + 1
\end{equation*}</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
                     <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;logisticregression__C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]}</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best cross-validation score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;tfidfvectorizer&quot;</span><span class="p">]</span>
<span class="c1"># transform the training dataset:</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="c1"># find maximum value for each of the features over dataset:</span>
<span class="n">max_value</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">sorted_by_tfidf</span> <span class="o">=</span> <span class="n">max_value</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
<span class="c1"># get feature names</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features with lowest tfidf:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[</span><span class="n">sorted_by_tfidf</span><span class="p">[:</span><span class="mi">20</span><span class="p">]])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features with highest tfidf:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[</span><span class="n">sorted_by_tfidf</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sorted_by_idf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">idf_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features with lowest idf:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[</span><span class="n">sorted_by_idf</span><span class="p">[:</span><span class="mi">100</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="investigating-model-coefficients">
<h3>Investigating model coefficients<a class="headerlink" href="#investigating-model-coefficients" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">visualize_coefficients</span><span class="p">(</span><span class="n">coefficients</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">n_top_features</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Visualize coefficients of a linear model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    coefficients : nd-array, shape (n_features,)</span>
<span class="sd">        Model coefficients.</span>

<span class="sd">    feature_names : list or nd-array of strings, shape (n_features,)</span>
<span class="sd">        Feature names for labeling the coefficients.</span>

<span class="sd">    n_top_features : int, default=25</span>
<span class="sd">        How many features to show. The function will show the largest (most</span>
<span class="sd">        positive) and smallest (most negative)  n_top_features coefficients,</span>
<span class="sd">        for a total of 2 * n_top_features coefficients.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">coefficients</span> <span class="o">=</span> <span class="n">coefficients</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">coefficients</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># this is not a row or column vector</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;coeffients must be 1d array or column vector, got&quot;</span>
                         <span class="s2">&quot; shape </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">coefficients</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">coefficients</span> <span class="o">=</span> <span class="n">coefficients</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">coefficients</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">feature_names</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of coefficients </span><span class="si">{}</span><span class="s2"> doesn&#39;t match number of&quot;</span>
                         <span class="s2">&quot;feature names </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">coefficients</span><span class="p">),</span>
                                                    <span class="nb">len</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)))</span>
    <span class="c1"># get coefficients with large absolute values</span>
    <span class="n">coef</span> <span class="o">=</span> <span class="n">coefficients</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">positive_coefficients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">coef</span><span class="p">)[</span><span class="o">-</span><span class="n">n_top_features</span><span class="p">:]</span>
    <span class="n">negative_coefficients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">coef</span><span class="p">)[:</span><span class="n">n_top_features</span><span class="p">]</span>
    <span class="n">interesting_coefficients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">negative_coefficients</span><span class="p">,</span>
                                          <span class="n">positive_coefficients</span><span class="p">])</span>
    <span class="c1"># plot them</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span> <span class="k">if</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">&#39;b&#39;</span>
              <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">coef</span><span class="p">[</span><span class="n">interesting_coefficients</span><span class="p">]]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_top_features</span><span class="p">),</span> <span class="n">coef</span><span class="p">[</span><span class="n">interesting_coefficients</span><span class="p">],</span>
            <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
    <span class="n">feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_top_features</span><span class="p">),</span>
               <span class="n">feature_names</span><span class="p">[</span><span class="n">interesting_coefficients</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
               <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Coefficient magnitude&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">visualize_coefficients</span><span class="p">(</span>
    <span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;logisticregression&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span>
    <span class="n">feature_names</span><span class="p">,</span> <span class="n">n_top_features</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="bag-of-words-with-more-than-one-word-n-grams">
<h2>Bag of words with more than one word (n-grams)<a class="headerlink" href="#bag-of-words-with-more-than-one-word-n-grams" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bards_words:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bards_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">bards_words</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">bards_words</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transformed data (dense):</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">bards_words</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">bards_words</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="c1"># running the grid-search takes a long time because of the</span>
<span class="c1"># relatively large grid and the inclusion of trigrams</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;logisticregression__C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
              <span class="s2">&quot;tfidfvectorizer__ngram_range&quot;</span><span class="p">:</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]}</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best cross-validation score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># extract scores from grid_search</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="c1"># visualize heatmap</span>
<span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;logisticregression__C&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">param_grid</span><span class="p">[</span><span class="s1">&#39;tfidfvectorizer__ngram_range&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># extract feature names and coefficients</span>
<span class="n">vect</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;tfidfvectorizer&#39;</span><span class="p">]</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vect</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
<span class="n">coef</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;logisticregression&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">visualize_coefficients</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">n_top_features</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">22</span><span class="p">,</span> <span class="mi">22</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># find 3-gram features</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">feature</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))</span> <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">feature_names</span><span class="p">])</span> <span class="o">==</span> <span class="mi">3</span>
<span class="c1"># visualize only 3-gram features:</span>
<span class="n">visualize_coefficients</span><span class="p">(</span><span class="n">coef</span><span class="o">.</span><span class="n">ravel</span><span class="p">()[</span><span class="n">mask</span><span class="p">],</span>
                                     <span class="n">feature_names</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">n_top_features</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">22</span><span class="p">,</span> <span class="mi">22</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h2>
<p>Compare unigram and bigram models on the 20 newsgroup dataset</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="n">categories</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;alt.atheism&#39;</span><span class="p">,</span>
    <span class="s1">&#39;talk.religion.misc&#39;</span><span class="p">,</span>
    <span class="s1">&#39;comp.graphics&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sci.space&#39;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">remove</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;headers&#39;</span><span class="p">,</span> <span class="s1">&#39;footers&#39;</span><span class="p">,</span> <span class="s1">&#39;quotes&#39;</span><span class="p">)</span>

<span class="n">data_train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span>
                                <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                                <span class="n">remove</span><span class="o">=</span><span class="n">remove</span><span class="p">)</span>

<span class="n">data_test</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span>
                               <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                               <span class="n">remove</span><span class="o">=</span><span class="n">remove</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">data_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="12-feature-selection.html" title="previous page">Automatic Feature Selection</a>
    <a class='right-next' id="next-link" href="14-custom-estimators.html" title="next page">Custom Estimators</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Andreas C. Müller<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>