{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics and scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Model evaluation\n",
    "\n",
    "02/24/20\n",
    "\n",
    "Andreas C. Müller\n",
    "\n",
    "\n",
    "\n",
    "FIXME boston\n",
    "FIXME explain scorer interface vs metrics interface, plotting has scorer interface\n",
    "FIXME ROC curve slide is bad. Illustration of why random is .5\n",
    "FIXME format string\n",
    "FIXME demonstrate that AUC / average precision are rancing metrics\n",
    "FIXME add calibration\n",
    "FIXME remove regression?\n",
    "FIXME top right of pr curve actually maximizes f1\n",
    "FIXME no interpolation on pr curve?\n",
    "FIXME add plotting\n",
    "FIXME macro vs weighted average example\n",
    "FIXME roc auc not good for imbalanced!? show example!!\n",
    "\n",
    "FIXME remove regression, go into more depth on cost-sensitive?\n",
    "FIXME relate to calibration!\n",
    "FIXME How to pick thresholds in multi-class?\n",
    "FIXME Add log-loss to metrics, and show why I don’t like it (.4, .6, 0) is better than (.36, .34, .3)\n",
    "FIXME explain why ROC AUC doesn't depend on class imbalance by calculating example\n",
    "FIXME write down example on slide about recall being zero and AUC being 1?\n",
    "FIXME show example of number nonzero for scorer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review : confusion matrix\n",
    ".center[\n",
    "![:scale 55%](images/confusion_matrix.png)\n",
    "]\n",
    ".larger[\n",
    "$$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$$\n",
    "]\n",
    "\n",
    "\n",
    "Diagonal divided by everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive & Negative are arbitrary\n",
    "### Though often the minority class is considered positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    ".smaller[\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, stratify=data.target, random_state=0)\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(lr.score(X_test, y_test))\n",
    "plot_confusion_matrix(lr, X_test, y_test, cmap='gray_r')\n",
    "```\n",
    "```\n",
    "[[48  5]\n",
    " [ 4 86]]\n",
    "0.94\n",
    "```\n",
    "\n",
    "]\n",
    "\n",
    "![:scale 20%](images/plot_confusion_matrix.png)\n",
    "\n",
    "\n",
    "confusion_matrix and accuracy_score take y_true, y_pred\n",
    "Note that plot_confusion_matrix takes the estimator.\n",
    "We might change that. We'll talk a bit more about that interface later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with Accuracy\n",
    "Data with 90% negatives:\n",
    ".smaller[\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "for y_pred in [y_pred_1, y_pred_2, y_pred_3]:\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "```\n",
    "```\n",
    "0.9\n",
    "0.9\n",
    "0.9\n",
    "```\n",
    "]\n",
    "\n",
    "--\n",
    "\n",
    ".center[\n",
    "![:scale 70%](images/problems_with_accuracy.png)\n",
    "]\n",
    "\n",
    "\n",
    "- Imbalanced classes lead to hard-to-interpret accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall, f-score\n",
    "\n",
    "\n",
    "$$ \\large\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
    "\n",
    "$$\\large\\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$\n",
    "\n",
    "$$\\large\\text{F} = 2 \\frac{\\text{precision} \\cdot\\text{recall}}{\\text{precision}+\\text{recall}}$$\n",
    "\n",
    "\n",
    "Positive Predicted Value (PPV)\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Sensitivity, coverage, true positive rate.\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Harmonic mean of precision and recall\n",
    "\n",
    "\n",
    "\n",
    "All depend on definition of positive and negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Zoo\n",
    "![:scale 100%](images/zoo.png)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the confusion matrix\n",
    ".smallest[\n",
    "```python\n",
    "confusion_matrix(y_true, y_pred)\n",
    "```\n",
    "![:scale 30%](images/confusion_matrix.png)\n",
    "]\n",
    ".left-column[\n",
    ".smallest[\n",
    "```python\n",
    "confusion_matrix(y_true, y_pred,\n",
    "                 normalize='true')\n",
    "```\n",
    "![:scale 80%](images/confusion_matrix_norm_true.png)\n",
    "]\n",
    "]\n",
    ".right-column[\n",
    ".smallest[\n",
    "```python\n",
    "confusion_matrix(y_true, y_pred,\n",
    "                 normalize='pred')\n",
    "```\n",
    "![:scale 80%](images/confusion_matrix_norm_pred.png)\n",
    "]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".smallest[\n",
    "```python\n",
    "classification_report(y_true, y_pred)\n",
    "```\n",
    "]\n",
    ".left-column[\n",
    "![:scale 38%](images/confusion_matrix_col.png)]\n",
    ".smallest[\n",
    ".right-column[\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      1.00      0.95        90\n",
    "           1       0.00      0.00      0.00        10\n",
    "\n",
    "    accuracy                           0.90       100\n",
    "   macro avg       0.45      0.50      0.47       100\n",
    "weighted avg       0.81      0.90      0.85       100\n",
    "\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.89      0.94        90\n",
    "           1       0.50      1.00      0.67        10\n",
    "\n",
    "    accuracy                           0.90       100\n",
    "   macro avg       0.75      0.94      0.80       100\n",
    "weighted avg       0.95      0.90      0.91       100\n",
    "\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.94      0.94        90\n",
    "           1       0.50      0.50      0.50        10\n",
    "\n",
    "    accuracy                           0.90       100\n",
    "   macro avg       0.72      0.72      0.72       100\n",
    "weighted avg       0.90      0.90      0.90       100\n",
    "```\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging strategies\n",
    "\n",
    "$$\\text{macro }\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} R(y_l, \\hat{y}_l)$$\n",
    "\n",
    "$$\\text{weighted } \\frac{1}{n} \\sum_{l \\in L} n_l R(y_l, \\hat{y}_l)$$\n",
    "\n",
    "\n",
    ".smaller[\n",
    "```python\n",
    "print(\"Weighted average: \", recall_score(y_test, y_pred_1, average=\"weighted\"))\n",
    "print(\"Macro average: \", recall_score(y_test, y_pred_1, average=\"macro\"))\n",
    "```\n",
    "```\n",
    "Weighted average: 0.90\n",
    "Macro average: 0.50\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Two common ways to approach multitask is to look at averages\n",
    "over binary metrics. So you can do binary metrics for\n",
    "recall, precision f1 score. But in principle, you could do\n",
    "it for more things. And in scikit-learn has several\n",
    "averaging strategies.\n",
    "\n",
    "There is macro, weighted, micro and samples. You should\n",
    "really not worried about micro samples, which only apply to\n",
    "multi-label prediction. We haven't talked about multi-label\n",
    "prediction and we're not going to talk about multi-label\n",
    "prediction. Multi-level prediction is when each sample can\n",
    "have multiple labels. \n",
    "\n",
    "Macro and weighted are the two options. They are\n",
    "interesting. For example, for recall, we can look at macro\n",
    "average recall, which is the average over all the labels and\n",
    "then the recall for just this label. So basically, here y_l\n",
    "is the true label, y ̂ is the predicted label. The positive\n",
    "class is now label l, and the negative classes is any other\n",
    "label, so this is sort of one versus rest version of recall.\n",
    "\n",
    "\n",
    "You can also do a weighted average, you weight by the number\n",
    "of samples in each class, and then divide by the total\n",
    "number of samples. So again, for each class, I compute\n",
    "recall for that class, and then I weight that by the number\n",
    "of samples. This is actually what classification report\n",
    "reports here.\n",
    "\n",
    "It's kind of interesting that if you do this, then actually\n",
    "looking at precision or recall just by themselves is\n",
    "informative. So if you look at macro average recall, that's\n",
    "actually also called balanced accuracy or if you look at\n",
    "macro average precision, this will be the useful metric by\n",
    "itself, because it looks at the precision of each of the\n",
    "classes. \n",
    "\n",
    "So basically, the choice between macro and weighted is\n",
    "whether you think each class should have the same weight, or\n",
    "each sample should have the same weight. Let’s say, we have\n",
    "recall for one class is 1 and for the other class 0. And for\n",
    "the majority class recall is 1 and for the minority, class\n",
    "recall is 0. For macro average recall, you would get 0.5\n",
    "because you just take the average of the two. If you look at\n",
    "a weighted, then the outcome would be the proportion of the\n",
    "positive class. So if 90% of the data is positive, you will\n",
    "get 90% here. So basically weighted takes the class sizes\n",
    "into account and so the bigger classes count for more. If\n",
    "you're actually interested in the smaller classes, macro is\n",
    "probably better because each class weights the same. Which\n",
    "one is the best really depends on your application?\n",
    "\n",
    "Macro is counted each class the same while weighted is count\n",
    "each sample the same. I did this here on the digit data set,\n",
    "which is basically balanced and so there's no difference. \n",
    "\n",
    "\n",
    "micro vs macro same for other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Accuracy\n",
    ".smaller[\n",
    "```python\n",
    "balanced_accuracy_score(y_t, y_p) == recall_score(y_t, y_p, average='macro')\n",
    "```\n",
    "]\n",
    "\n",
    "$$\\text{balanced_accuracy} = \\frac{1}{2}\\left( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP}\\right )$$\n",
    "\n",
    "- Always 0.5 for chance predictions\n",
    "- Equal to accuracy for balanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mammography Data\n",
    "\n",
    ".smallest[\n",
    ".left-column[\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "## mammography https://www.openml.org/d/310\n",
    "data = fetch_openml('mammography', as_frame=True)\n",
    "X, y = data.data, data.target\n",
    "X.shape\n",
    "```\n",
    "(11183, 6)\n",
    "```python\n",
    "y.value_counts()\n",
    "```\n",
    "```\n",
    "-1    10923\n",
    "1       260\n",
    "```\n",
    "]\n",
    ".right-column[\n",
    ".center[\n",
    "![:scale 100%](images/mammography_data.png)\n",
    "]\n",
    "]\n",
    ".reset-column[\n",
    "```python\n",
    "## make y boolean\n",
    "## this allows sklearn to determine the positive class more easily\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y == '1', random_state=0)\n",
    "```\n",
    "]\n",
    "]\n",
    "\n",
    "\n",
    "I use this mammography data set, which is very imbalanced.\n",
    "This is a data set that has many samples, only six features\n",
    "and it's very imbalanced.\n",
    "\n",
    "The datasets are about mammography data, and whether there\n",
    "are calcium deposits in the breast. They are often mistaken\n",
    "for cancer, which is why it's good to detect them. Since its\n",
    "rigidly low dimensional, we can do a scatter plot. And we\n",
    "can see that these are much skewed distributions and there's\n",
    "really a lot more of one class than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".smallest[\n",
    ".left-column[\n",
    "```python\n",
    "svc = make_pipeline(StandardScaler(),\n",
    "                    SVC(C=100, gamma=0.1))\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_test, y_test)\n",
    "```\n",
    "```\n",
    "0.986\n",
    "```\n",
    "]\n",
    ".right-column[\n",
    "```python\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)\n",
    "```\n",
    "```\n",
    "0.989\n",
    "```\n",
    "]\n",
    "]\n",
    "\n",
    "--\n",
    "\n",
    ".reset-column[\n",
    "]\n",
    ".smallest[\n",
    ".left-column[\n",
    "```\n",
    "              precision   recall  f1-score  support\n",
    "\n",
    "       False       0.99     1.00      0.99     2732\n",
    "        True       0.81     0.53      0.64       64\n",
    "\n",
    "    accuracy                          0.99     2796\n",
    "   macro avg       0.90     0.76      0.82     2796\n",
    "weighted avg       0.98     0.99      0.99     2796\n",
    "```\n",
    "]\n",
    ".right-column[\n",
    "```\n",
    "              precision   recall  f1-score  support\n",
    "\n",
    "       False       0.99     1.00      0.99     2732\n",
    "        True       0.90     0.56      0.69       64\n",
    "\n",
    "    accuracy                          0.99     2796\n",
    "   macro avg       0.94     0.78      0.84     2796\n",
    "weighted avg       0.99     0.99      0.99     2796\n",
    "```\n",
    "\n",
    "]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal setting!\n",
    "- What do I want? What do I care about? \n",
    "- Can I assign costs to the confusion matrix?\n",
    "- What guarantees do we want to give?\n",
    "\n",
    "\n",
    "\n",
    "(precision, recall, or something else)\n",
    "(i.e. a false positive costs me 10 dollars; a false negative, 100 dollars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Thresholds\n",
    ".tiny-code[\n",
    "```python\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       False       0.99      1.00      0.99      2732\n",
    "        True       0.90      0.56      0.69        64\n",
    "\n",
    "    accuracy                           0.99      2796\n",
    "   macro avg       0.94      0.78      0.84      2796\n",
    "weighted avg       0.99      0.99      0.99      2796\n",
    "\n",
    "```\n",
    "```python\n",
    "y_pred = rf.predict_proba(X_test)[:, 1] > .30\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       False       0.99      0.99      0.99      2732\n",
    "        True       0.71      0.64      0.67        64\n",
    "\n",
    "    accuracy                           0.99      2796\n",
    "   macro avg       0.85      0.82      0.83      2796\n",
    "weighted avg       0.99      0.99      0.99      2796\n",
    "\n",
    "```\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall curve\n",
    ".smaller[\n",
    "```python\n",
    "svc = make_pipeline(StandardScaler(), SVC(C=100, gamma=0.1))\n",
    "svc.fit(X_train, y_train)\n",
    "plot_precision_recall_curve(svc, X_test, y_test, name='SVC')\n",
    "```\n",
    "]\n",
    ".center[\n",
    "![:scale 65%](images/precision_recall_curve.png)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side note: Scikit-learn plotting API\n",
    ".smaller[\n",
    "```python\n",
    "prc = plot_precision_recall_curve(svc, X_test, y_test, name='SVC')\n",
    "## plot pops up here\n",
    "prc\n",
    "```\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "```python\n",
    "vars(pr_svc)\n",
    "```\n",
    "```\n",
    "{'precision': array([0.023, 0.023, 0.023, ..., 1.   , 1.   , 1.   ]),\n",
    " 'recall': array([1.   , 0.984, 0.984, ..., 0.031, 0.016, 0.   ]),\n",
    " 'average_precision': 0.6743452407177641,\n",
    " 'estimator_name': 'Pipeline',\n",
    " 'line_': \n",
    ",\n",
    " 'ax_': \n",
    ",\n",
    " 'figure_': \n",
    "}\n",
    "```\n",
    "```python\n",
    "## plot again without recomputing\n",
    "prc.plot()\n",
    "```\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    ".center[\n",
    "### (Receiver Operating Characteristic)\n",
    "]\n",
    ".smaller[\n",
    "```python\n",
    "plot_roc_curve(svc, X_test, y_test, name='SVC')\n",
    "```\n",
    "]\n",
    ".center[\n",
    "![:scale 65%](images/roc_curve_svc.png)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".left-column[\n",
    "## PR-Curve\n",
    "![:scale 100%](images/precision_recall_curve.png)\n",
    "]\n",
    ".right-column[\n",
    "## ROC-Curve\n",
    "![:scale 100%](images/roc_curve_svc.png)\n",
    "]\n",
    ".reset-column[\n",
    "- Share one axis (though transposed!?)\n",
    "- Interpolation is meaningful on ROC curve but not PR curve\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing RF and SVC\n",
    ".smallest[\n",
    "```python\n",
    "pr_svc = plot_precision_recall_curve(svc, X_test, y_test, name='SVC')\n",
    "## if we used computed before, we could just call pr_svc.plot()\n",
    "## using ax=plt.gca() will plot into the existing axes instead of creating  new ones\n",
    "pr_rf = plot_precision_recall_curve(rf, X_test, y_test, ax=plt.gca())\n",
    "\n",
    "```\n",
    "]\n",
    ".center[\n",
    "![:scale 60%](images/rf_vs_svc.png)\n",
    "]\n",
    "\n",
    "\n",
    "We’re going to compare random forest and support vector\n",
    "machine.\n",
    "\n",
    "You can see they're sort of similar. But in some areas, they\n",
    "are different. For example, you can see that the random\n",
    "forest is little bit more stable for very high precision but\n",
    "in some places the SVM is better than random forest and vice\n",
    "versa. So which of the two is better classifier really\n",
    "depends on which area you're interested in. \n",
    "\n",
    "By comparing these two curves, again, sort of a very fine\n",
    "grain thing that you can do manually, if you want to do this\n",
    "for many models, and want to pick the best model. This is\n",
    "maybe not really feasible. So you might want to summarize\n",
    "this in a single number. So one number you could think of\n",
    "is, while I know I definitely want to have a recall of 90%\n",
    "so I'm going to put my threshold here and compare at that\n",
    "threshold. If you don't really have a particular goal yet,\n",
    "you can also consider all thresholds at once, and basically\n",
    "compute the area under this curve. It's not exactly the same\n",
    "but that's sort of what average precision does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision\n",
    ".center[\n",
    "![:scale 100%](images/avg_precision.png)\n",
    "]\n",
    "\n",
    "\n",
    "Average precision basically does a step function integral of\n",
    "this thing. So for each possible threshold, you look at the\n",
    "precision at this threshold times the change in recall. So\n",
    "this is like fitting a step function under the curve and\n",
    "then computing the integral. This allows you to basically\n",
    "take all possible thresholds into account at once. \n",
    "\n",
    "Q: How do I adjust the precision-recall ratio for the random\n",
    "forest? \n",
    "\n",
    "I mean, it says predict_proba. So I can alter threshold that\n",
    "at different values if I want. For a support vector machine,\n",
    "which doesn't have predict_proba, I can also adjust the\n",
    "threshold the decision function.\n",
    "\n",
    "Average precision is a pretty good metric to basically\n",
    "compare models if there are imbalanced classes. \n",
    "\n",
    "\n",
    "Related to area under the precision-recall curve\n",
    "(with step interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Under ROC Curve (aka AUC)\n",
    ".center[\n",
    "![:scale 65%](images/roc_curve_svc_auc.png)\n",
    "]\n",
    "- Always .5 for random/constant prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "And so when you look at the area under the curve, which is\n",
    "called ROC AUC, it's always 0.5 for random or constant\n",
    "predictions. So it's very easy to say, how much better than\n",
    "random are you if you look at ROC AUC. \n",
    "\n",
    "I encourage you to read the paper in the link. \n",
    "\n",
    "One thing is that the ROC curve is usually much smoother and\n",
    "it makes more sense to interpolate it, where the\n",
    "precision-recall curve makes less sense to interpolate us.\n",
    "But the precision-recall curve can sometimes pick up on more\n",
    "fine-grained differences. \n",
    "\n",
    "Even though AUC stands for area under the curve, if you see\n",
    "it in literature always means the area under the receiver\n",
    "operating curve. So AUC always means ROC AUC. \n",
    "\n",
    "AUC is a ranking metric. So it takes all possible thresholds\n",
    "into account, which means that it's independent of the\n",
    "default thresholds. So again, you can have something with\n",
    "very low accuracy but still with perfect ROC AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep in mind: ranking vs predictions\n",
    "```python\n",
    "dec = svc.decision_function(X_test)\n",
    "np.all((dec > 0) == svc.predict(X_test))\n",
    "```\n",
    "```\n",
    "True\n",
    "```\n",
    "```python\n",
    "print(f1_score(y_test, dec > 0))\n",
    "print(average_precision_score(y_test, dec))\n",
    "```\n",
    "```\n",
    "0.641\n",
    "0.635\n",
    "```\n",
    "\n",
    "--\n",
    "\n",
    "```python\n",
    "dec_new = dec - 10\n",
    "print(f1_score(y_test, dec_new > 0))\n",
    "print(average_precision_score(y_test, dec_new))\n",
    "```\n",
    "```\n",
    "0.0\n",
    "0.635\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold and ranking metrics\n",
    ".left-column[\n",
    ".tiny[\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "f1_rf = f1_score(y_test, rf.predict(X_test))\n",
    "print(f\"f1_score of random forest: {f1_rf:.3f}\")\n",
    "f1_svc = f1_score(y_test, svc.predict(X_test))\n",
    "print(f\"f1_score of svc: {f1_svc:.3f}\")\n",
    "```\n",
    "\n",
    "```\n",
    "f1_score of random forest: 0.709\n",
    "f1_score of svc: 0.715\n",
    "```\n",
    "```python\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "ba_rf = balanced_accuracy_score(y_test, rf.predict(X_test))\n",
    "print(f\"Balanced accuracy of random forest: {ba_rf:.3f}\")\n",
    "ba_svc = balanced_accuracy_score(y_test, svc.predict(X_test))\n",
    "print(f\"Balanced accuracy of svc: {ba_svc:.3f}\")\n",
    "```\n",
    "```\n",
    "Balanced accuracy of random forest: 0.765\n",
    "Balanced accuracy of svc: 0.764\n",
    "```\n",
    "]\n",
    "]\n",
    ".right-column[\n",
    ".tiny[\n",
    "```python\n",
    "from sklearn.metrics import average_precision_score\n",
    "ap_rf = average_precision_score(\n",
    "y_test, rf.predict_proba(X_test)[:, 1])\n",
    "    print(f\"Average precision of random forest: {ap_rf:.3f}\")\n",
    "ap_svc = average_precision_score(\n",
    "    y_test, svc.decision_function(X_test))\n",
    "print(f\"Average precision of svc: {ap_svc:.3f}\")\n",
    "```\n",
    "```\n",
    "Average precision of random forest: 0.682\n",
    "Average precision of svc: 0.693\n",
    "```\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_rf = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "print(f\"Area under ROC curve of random forest: {auc_rf:.3f}\")\n",
    "auc_svc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
    "print(f\"Area under ROC curve of svc: {auc_svc:.3f}\")\n",
    "```\n",
    "```\n",
    "Area under ROC curve of random forest: 0.936\n",
    "Area under ROC curve of svc: 0.817\n",
    "```\n",
    "]]\n",
    "\n",
    "\n",
    "Previously, I just have sort of a simple comparison between\n",
    "f1 and average precision. I'm comparing these two models. So\n",
    "if I look at f1 score, it's going to compare basically these\n",
    "two points and the SVC is slightly is better.\n",
    "\n",
    "But F1 score only looks at the default threshold. If I want\n",
    "to look at the whole RC curve, I can use average precision.\n",
    "Even if I look at all possible thresholds, SVC is still\n",
    "better. Average precision is sort of a very sensitive metric\n",
    "that allows you to basically make good decisions even if the\n",
    "classes are very imbalanced and that also takes all possible\n",
    "thresholds into account. \n",
    "\n",
    "One thing that you should keep in mind if you use this is\n",
    "that this does not give you a particular threshold. For\n",
    "example, if you use the default threshold, your accuracy\n",
    "might be zero. This only looks at ranking. \n",
    "\n",
    "The curve and the area under the curve, don't depend on\n",
    "where the default threshold is. So if you predict\n",
    "everything, like for some weird reason, if you predict\n",
    "everything as the minority class, your accuracy will be\n",
    "really bad. But if they are ranked in the right order, so\n",
    "that all the positive classes are ranked higher than the\n",
    "negative classes, then you might still get an area under the\n",
    "curve that's very close to one. So this only looks at\n",
    "ranking. \n",
    "\n",
    "\n",
    "AP only considers ranking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading on evaluation curves\n",
    "\n",
    "- [The Relationship Between Precision-Recall and ROC Curves](https://www.biostat.wisc.edu/~page/rocpr.pdf)\n",
    "- [Precision-Recall-Gain Curves: PR Analysis Done Right](https://papers.nips.cc/paper/5867-precision-recall-gain-curves-pr-analysis-done-right.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of metrics for binary classification\n",
    "\n",
    "Threshold-based:\n",
    "- (balanced) accuracy\n",
    "- precision, recall, f1\n",
    "\n",
    "Ranking:\n",
    "- average precision\n",
    "- ROC AUC\n",
    "\n",
    "\n",
    "\n",
    "So let's briefly summarize the metrics for binary\n",
    "classification. So there are basically two approaches. One\n",
    "looks just at the predictions or look at soft predictions\n",
    "and take multiple thresholds into account. So the ones that\n",
    "I called threshold base, which is for single threshold, the\n",
    "ones that we talked about in our accuracy, which is just a\n",
    "fraction of correctly predicted, precision, recall, and f1.\n",
    "So the issue with accuracy is obviously that it can't\n",
    "distinguish between things that are quite different, for\n",
    "example, for imbalanced data sets, the classifier always\n",
    "predicts majority class often has like very high accuracy\n",
    "but tells you nothing. So in particular, for imbalanced\n",
    "classes, accuracy is a pretty bad measure. Precision and\n",
    "recall together are pretty good measures, though you always\n",
    "need to look at both numbers. One way to look at both\n",
    "numbers at once is the f1 score, though, using the harmonic\n",
    "mean is a little bit arbitrary. But still, only f1 score is\n",
    "a somewhat reasonable score if you only look at the actual\n",
    "predicted classes.\n",
    "\n",
    "For the ranking based losses, I think both average precision\n",
    "and ROC AUC are pretty good choices, ROC AUC, I like it\n",
    "because I know what 0.5 means, while for average precision,\n",
    "it's a little bit more tricky to see what the different\n",
    "orders of magnitude mean, or the different scales mean, but\n",
    "it can be more fine-grained measure. And so if you want to\n",
    "do cross-validation, my go-to is usually ROC AUC. In\n",
    "particular, for imbalance binary datasets, I would usually\n",
    "use ROC AUC. But then make sure that your threshold is\n",
    "sensible, but anything other than accuracy is okay. \n",
    "\n",
    "Just don’t look at only precision or only recall, if you do\n",
    "a grid search for just precision, you will probably get\n",
    "garbage results. \n",
    "\n",
    "\n",
    "add log loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification\n",
    "\n",
    "\n",
    "The next thing I want to talk about is multiclass\n",
    "classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Confusion Matrix\n",
    ".left-column[\n",
    ".tiny-code[\n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "digits = load_digits()\n",
    " ## data is between 0 and 16\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data / 16., digits.target, random_state=0)\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\n",
    "plot_confusion_matrix(lr, X_test, y_test, cmap='gray_r')\n",
    "\n",
    "```\n",
    "```\n",
    "Accuracy: 0.964\n",
    "```\n",
    "![:scale 70%](images/confusion_matrix_digits.png)\n",
    "\n",
    "]\n",
    "]\n",
    ".right-column[\n",
    ".tiny-code[\n",
    "```python\n",
    "print(classification_report(y_test, pred))\n",
    "```\n",
    "```\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       1.00      1.00      1.00        37\n",
    "          1       0.91      0.93      0.92        43\n",
    "          2       0.98      1.00      0.99        44\n",
    "          3       1.00      0.96      0.98        45\n",
    "          4       0.95      0.97      0.96        38\n",
    "          5       0.96      0.96      0.96        48\n",
    "          6       0.98      0.98      0.98        52\n",
    "          7       0.98      0.96      0.97        48\n",
    "          8       0.96      0.90      0.92        48\n",
    "          9       0.92      0.98      0.95        47\n",
    "\n",
    "    accuracy                           0.96       450\n",
    "   macro avg       0.96      0.96      0.96       450\n",
    "weighted avg       0.96      0.96      0.96       450\n",
    "```\n",
    "]\n",
    "]\n",
    "\n",
    "\n",
    "So again, for multiclass classification, you look at the\n",
    "confusion matrix. It's even more telling in a way than it\n",
    "was for binary classification but it's also pretty big. So\n",
    "here, I'm using the digital data set, which has 10 classes,\n",
    "and here's the confusion matrix. The diagonal is everything\n",
    "that's correct, the off-diagonal are mistakes. \n",
    "\n",
    "And you can see very fine-grained which classes were\n",
    "mistaken for which other classes. So this is very nice if\n",
    "you want to look in a very detailed view of the performance\n",
    "of the classifier. But again, you can't really use it for\n",
    "model selection, or not easily. You can also again, look at\n",
    "the classification report, which will give you\n",
    "precision-recall and F score for each for the classes,\n",
    "again, very, very fine grained. So you can see, for example,\n",
    "that Class 0, was predicted while Class 8 is the hardest. \n",
    "\n",
    "It might make more sense to look at in precision average\n",
    "classes, recall over the classes or f1 score for the\n",
    "classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class ROC AUC\n",
    "- Hand & Till, 2001, one vs one\n",
    " $$ \\frac{1}{c(c-1)}\\sum_{j=1}^{c}\\sum_{k \\neq j}^{c} AUC(j,k)$$\n",
    "- Provost & Domingo, 2000, one vs rest\n",
    "$$ \\frac{1}{c}\\sum_{j=1}^{c}p(j) AUC(j,\\text{rest}_j)$$\n",
    "\n",
    "\n",
    "You can do that similar for ROC AUC. It will soon be\n",
    "available in scikit-learn. There are basically two versions\n",
    "of doing multiclass ROC AUC. One is called Hand & Till while\n",
    "the other is called Provost & Domingo. And the first one is\n",
    "basically one versus one where you iterate over all the\n",
    "classes, and then you iterate over all the other classes,\n",
    "then you look at the AUC of one class versus the other\n",
    "class. \n",
    "\n",
    "And this one is basically one versus rest where you look at\n",
    "the AUC of one class versus all the other classes. \n",
    "\n",
    "H&T is not weighted whereas, on P&D, p(j) means number of\n",
    "samples in j, and P&D is weighted. You can also do weighted\n",
    "OvO or unweighted OvR.\n",
    "\n",
    "There are at least 4 different ways to do multi-class AUC.\n",
    "It’s not clear to me how they behave in practice. If you\n",
    "weighted then you count the samples are the same weight. If\n",
    "you do unweight, you give the different classes the same\n",
    "weight. \n",
    "\n",
    "So this is definitely a pretty good metric if you want a\n",
    "ranking metric for multi-class classification.\n",
    "\n",
    "\n",
    "FIXME unify notation with slide on averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of metrics for multiclass classification\n",
    "\n",
    "Threshold-based:\n",
    "- accuracy\n",
    "- precision, recall, f1 (macro average, weighted)\n",
    "\n",
    "Ranking:\n",
    "- OVR ROC AUC\n",
    "- OVO ROC AUC\n",
    "\n",
    "\n",
    "\n",
    "The threshold based ones are sort of the same, only now\n",
    "looking at only precision, or only recall, if you average\n",
    "over all the classes actually makes sense. So you can do a\n",
    "grid search over macro average recall, and it'll give you\n",
    "reasonable results.\n",
    "\n",
    "For ranking, in theory, you could also do OvR or OvO on the\n",
    "precision-recall curve. But I haven’t seen anyone done that\n",
    "or any papers on that. So let's not do that for now. Both\n",
    "can be weighted or unweighted versions of that.\n",
    "\n",
    "Both are great for imbalance problems again, but now you\n",
    "have to pick multiple thresholds and it's unclear how to do\n",
    "that well or at least for me.\n",
    "\n",
    "So let’s say you use OVO ROC and you get a classifier with a\n",
    "high value there, and now you want to make predictions. What\n",
    "thresholds do you use? \n",
    "\n",
    "I don't actually have a good answer. Because you need n\n",
    "minus one different threshold to actually make a decision. \n",
    "\n",
    "So for this reason, probably for multi-class, I might not\n",
    "use the ranking metrics unless you have to be cursed,\n",
    "they're sort of hard to interpret. And I would probably use\n",
    "something like macro average precision or macro average\n",
    "recall or macro average f1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build-in standard metrics\n",
    "- $\\text{R}^2$ : easy to understand scale\n",
    "- MSE : easy to relate to input\n",
    "- Mean absolute error, median absolute error: more robust\n",
    "\n",
    "\n",
    "The built-in standard metrics by default in scikit-learn\n",
    "uses R squared. The other obvious metric is Mean Squared\n",
    "Error. The good thing about R squared is it's around zero\n",
    "and maximum of one. It's very easy to understand the scale.\n",
    "If it's close to one is good. If it's negative, it's really\n",
    "bad. If it's close to zero, it's bad.\n",
    "\n",
    "While the MSE is easier to relate to the input. So the scale\n",
    "of the mean squared error is sort of relates to the scale of\n",
    "the output. You can also use things like mean absolute\n",
    "error, and median absolute error, which is more robust. \n",
    "\n",
    "If you square things, then outliers have a very big impact.\n",
    "So if we don't square things, then outliers have less of an\n",
    "impact. So often people prefer to use mean absolute error to\n",
    "limit the impact of outliers. If being low is good, you need\n",
    "to have a neg before and if being high is good you won't be\n",
    "needing to it be neg. \n",
    "\n",
    "I think these are actually the only metrics that are in\n",
    "scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute vs Relative: MAPE\n",
    "$$ \\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n}\\left|\\frac{y-\\hat{y}}{y}\\right|$$\n",
    "\n",
    ".center[\n",
    "![:scale 50%](images/mape.png)\n",
    "]\n",
    "\n",
    "\n",
    "There's another metric I want to talk about called MAPE.\n",
    "Which comes up in particular in forecasting. MAPE stands for\n",
    "Mean Absolute Percentage Error. MAPE captures is the\n",
    "percentage error. It divides the residual by the true value.\n",
    "Here for each individual data point, we’re dividing by the\n",
    "true value. \n",
    "\n",
    "Basically, what that means is that if a data point is large,\n",
    "we care less about how well we predict it. Basically, we\n",
    "allow larger error bars around high values. And I tried to\n",
    "plot this here, this is sort of for the one feature, LSTAT,\n",
    "to have like a reasonable x-axis since that helps us\n",
    "visualize this. And so here is the true dataset in blue and\n",
    "the predictive dataset in orange predicted using all\n",
    "features. And so in the bottom right, you can see it that we\n",
    "are under predicting by about 10. And this gives us a MAPE\n",
    "of 36%.\n",
    "\n",
    "On the top left, we are under predicting by even more, but\n",
    "the MAPE is smaller. So even though the absolute error on\n",
    "the top is bigger than the absolute error on the bottom, the\n",
    "relative error in the top is much smaller than the relative\n",
    "error in the bottom. \n",
    "\n",
    "This is a very commonly used measure in forecasting. Be\n",
    "careful when to use it because sometimes it might be not\n",
    "super intuitive. In particular, if something is not defined,\n",
    "if anything is zero, and if something is very close to zero,\n",
    "you need to perfectly predict it otherwise, you have unbound\n",
    "error. So this is usually used for things that are greater\n",
    "than one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction plots\n",
    ".wide-left-column[\n",
    ".smaller[\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    boston.data, boston.target)\n",
    "ridge = Ridge(normalize=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "pred = ridge.predict(X_test)\n",
    "plt.plot(pred, y_test, 'o')\n",
    "```\n",
    "]]\n",
    ".narrow-right-column[\n",
    "![:scale 100%](images/regression_boston.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "I think one of the best debugging tools is prediction plots,\n",
    "where you plot the predicted versus the true outcome. This\n",
    "is on the Boston dataset here, I just do a ridge regression\n",
    "model. And so here, I plot the predicted outcome by the\n",
    "ridge regression on the test data set versus the true\n",
    "outcome. And so ideally, it should be the same, they it\n",
    "should be on the diagonal. \n",
    "\n",
    "But you can see that there's like, a couple things wrong\n",
    "with it. So for example, there are a couple of things where\n",
    "the true prediction is 50K, but I under predict severely.\n",
    "There’s a lot of underprediction going on. In particular,\n",
    "for low values, I’m predicting too high and for high values,\n",
    "I’m predicting too low.\n",
    "\n",
    "Looking at this you can see different trends and see if\n",
    "there are any trends or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Plots\n",
    ".left-column[![:scale 50%](images/regression_boston.png)\n",
    "\n",
    "\n",
    "![:scale 60%](images/regression_boston_2.png)]\n",
    ".right-column[\n",
    "![:scale 100%](images/regression_boston_3.png)]\n",
    "\n",
    "\n",
    "A different way to look at this is to basically rotate that\n",
    "45 degrees. \n",
    "\n",
    "So here we are looking at the true versus true minus\n",
    "predicted. This allows you even more easily to see if there\n",
    "are trends, and you can see that basically, we're over\n",
    "predicting for low values and we're under predicting for\n",
    "high values.\n",
    "\n",
    "You can also look at the histogram of the residuals and you\n",
    "can see that they’re mostly centered around zero, which is\n",
    "good so there's probably no systematic bias but you can see\n",
    "that there are some values where we underpredict by quite a\n",
    "lot. These are nice because you can do these plots no matter\n",
    "what dimension your data set is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target vs Feature\n",
    "![:scale 100%](images/target_vs_feature.png)\n",
    "\n",
    "\n",
    "If you want to look at in more detail, you can look at per\n",
    "feature plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual vs Feature\n",
    "![:scale 100%](images/residual_vs_feature.png)\n",
    "\n",
    "\n",
    "\n",
    "Here, I'm plotting y test minus the prediction against each\n",
    "of the features and see if there are particular aspects of\n",
    "the feature that are not captured. For example, you can see\n",
    "here that for a low LSTAT, we are underpredicting more than\n",
    "for a high LSTAT. That shows that there might be trends in\n",
    "these features that we're not capturing. \n",
    "\n",
    "The X-axis is features. For each data point in the test\n",
    "dataset, I look at the value of this feature, and I look at\n",
    "the residual, which is the true value minus the prediction. \n",
    "\n",
    "Ideally, this should all be sort of horizontal because the\n",
    "errors that the model makes should be independent of all of\n",
    "the features. If the error that the model makes is not\n",
    "independent of the feature that means there's some aspect of\n",
    "the feature that the model didn't capture. \n",
    "\n",
    "This is basically the dependence of the target on the\n",
    "feature that is not captured by the prediction. And you can\n",
    "meditate that and then at some point it'll make sense.\n",
    "\n",
    "These are all more and more fine grain plots to understand\n",
    "the errors that you're making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking metrics\n",
    "\n",
    "- Accuracy rarely what you want\n",
    "- Problems are rarely balanced\n",
    "- Find the right criterion for the task\n",
    "- OR pick a substitude, but at least think about it\n",
    "- Emphasis on recall or precision?\n",
    "- Which classes are the important ones?\n",
    "\n",
    "\n",
    "\n",
    "In summary, for both multiclass and binary, you should\n",
    "really think about what your metrics are. And unless your\n",
    "problem is balanced, and even maybe then don't use accuracy.\n",
    "Accuracy is really bad. Problems are rarely balanced, in a\n",
    "real world and usually there heavily unbalanced unless\n",
    "someone artificially balances them. So it's really important\n",
    "to think about what is the right criterion for a particular\n",
    "task, and then you can optimize that criterion. \n",
    "\n",
    "So as I said, that can be like having costs associated with\n",
    "the confusion matrix, emphasizing precision or recall,\n",
    "setting specific goals such as have recall of at least X\n",
    "percent, or have precision of at least X percent and for\n",
    "multi-class deciding whether are all the classes the same\n",
    "importance, or all the samples the same importance, what are\n",
    "the important classes, even for binary, which one is the\n",
    "positive class? \n",
    "\n",
    "So whenever you talk about precision and recall, it’s\n",
    "important to think about which one is the positive class and\n",
    "how will changing the positive class change these two\n",
    "values? So and then finally, obviously, am I going to use\n",
    "the default threshold? Why am I going to use the default\n",
    "threshold? Do I use a ranking based metric? Or is there a\n",
    "reason why I would want to use a ranking based metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using metrics in cross-validation\n",
    ".smaller[\n",
    "```python\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "## default scoring for classification is accuracy\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "print(\"default scoring \", cross_val_score(rf, X, y))\n",
    "\n",
    "## providing scoring=\"accuracy\" doesn't change the results\n",
    "explicit_accuracy =  cross_val_score(rf, X, y,  scoring=\"accuracy\")\n",
    "print(\"explicit accuracy scoring \", explicit_accuracy)\n",
    "\n",
    "ap =  cross_val_score(rf, X, y, scoring=\"average_precision\")\n",
    "print(\"average precision\", ap)\n",
    "```\n",
    "```\n",
    "default scoring  [0.93  0.947 0.991 0.974 0.973]\n",
    "explicit accuracy scoring  [0.93  0.947 0.991 0.974 0.973]\n",
    "average precision [0.992 0.973 0.999 0.995 0.999]\n",
    "\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "So once you pick them, it's very easy to use any of these\n",
    "metrics in scikit-learn. So they're all functions inside the\n",
    "metrics module, but usually, you're going to want to use\n",
    "them in for cross-validation, or grid search. And then it's\n",
    "even easier. There's argument called scoring. It takes a\n",
    "string or a callable. And so here, if you give it a string,\n",
    "it'll just do the right thing automatically.\n",
    "\n",
    "By default its accuracy. So if you don't do anything, it's\n",
    "the same as if you provide accuracy. But you can also set\n",
    "ROC AUC, and it's going to use ROC AUC. So the thing is ROC\n",
    "AUC actually needs probabilities right or a decision\n",
    "function, so behind the scenes, it does the right thing and\n",
    "gets out the decision function for the positive class to\n",
    "compute the ROC AUC correctly.\n",
    "\n",
    "Using better a metric than accuracy is as simple as saying\n",
    "scoring equal to something. And it’s exactly the same as in\n",
    "grid search CV. So if you want to do a grid search CV or\n",
    "randomized search CV, you can set scoring equal to ROC AUC\n",
    "or average precision or recall macro.\n",
    "\n",
    "\n",
    "Same for GridSearchCV\n",
    "\n",
    "\n",
    "Will make GridSearchCV.score use your metric!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Metrics\n",
    "```\n",
    "from sklearn.model_selection import cross_validate\n",
    "res = cross_validate(RandomForestClassifier(), X, y,\n",
    "                     scoring=[\"accuracy\", \"average_precision\",\n",
    "                     \"recall_macro\"],\n",
    "                     return_train_score=True, cv=5)\n",
    "pd.DataFrame(res)\n",
    "```\n",
    ".tiny[\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "      \n",
    "fit_time\n",
    "score_time\n",
    "test_accuracy\n",
    "train_accuracy\n",
    "test_average_precision\n",
    "train_average_precision\n",
    "test_recall_macro\n",
    "train_recall_macro\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "0\n",
    "0.171312\n",
    "0.016973\n",
    "0.921053\n",
    "1.0\n",
    "0.992115\n",
    "1.0\n",
    "0.918277\n",
    "1.0\n",
    "\n",
    "    \n",
    "1\n",
    "0.133090\n",
    "0.017563\n",
    "0.938596\n",
    "1.0\n",
    "0.972293\n",
    "1.0\n",
    "0.923190\n",
    "1.0\n",
    "\n",
    "    \n",
    "2\n",
    "0.110232\n",
    "0.015200\n",
    "0.982456\n",
    "1.0\n",
    "0.999098\n",
    "1.0\n",
    "0.981151\n",
    "1.0\n",
    "\n",
    "    \n",
    "3\n",
    "0.110211\n",
    "0.015076\n",
    "0.956140\n",
    "1.0\n",
    "0.992924\n",
    "1.0\n",
    "0.950397\n",
    "1.0\n",
    "\n",
    "    \n",
    "4\n",
    "0.124239\n",
    "0.019509\n",
    "0.973451\n",
    "1.0\n",
    "0.998858\n",
    "1.0\n",
    "0.974011\n",
    "1.0\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in scoring\n",
    "```python\n",
    "from sklearn.metrics import SCORERS\n",
    "print(\"\\n\".join(sorted(SCORERS.keys())))\n",
    "```\n",
    ".smaller[\n",
    "```\n",
    "accuracy                       adjusted_mutual_info_score     adjusted_rand_score\n",
    "average_precision              balanced_accuracy              completeness_score\n",
    "explained_variance             f1                             f1_macro\n",
    "f1_micro                       f1_samples                     f1_weighted\n",
    "fowlkes_mallows_score          homogeneity_score              jaccard\n",
    "jaccard_macro                  jaccard_micro                  jaccard_samples\n",
    "jaccard_weighted               max_error                      mutual_info_score\n",
    "neg_brier_score                neg_log_loss                   neg_mean_absolute_error\n",
    "neg_mean_gamma_deviance        neg_mean_poisson_deviance      neg_mean_squared_error\n",
    "neg_mean_squared_log_error     neg_median_absolute_error      neg_root_mean_squared_error\n",
    "normalized_mutual_info_score   precision                      precision_macro\n",
    "precision_micro                precision_samples              precision_weighted\n",
    "r2                             recall                         recall_macro\n",
    "recall_micro                   recall_samples                 recall_weighted\n",
    "roc_auc                        roc_auc_ovo                    roc_auc_ovo_weighted\n",
    "roc_auc_ovr                    roc_auc_ovr_weighted           v_measure_score\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here's a list of all the built-in scores, you can look at\n",
    "the documentation, there's really a lot of them. So these\n",
    "are for classification or regression or clustering. If you\n",
    "look in the documentation, you'll actually see which ones\n",
    "are for classification, which one is binary only, which ones\n",
    "are multiclass, which ones are regression and which ones are\n",
    "for clustering.\n",
    "\n",
    "The thing in scikit-learn, whatever you use for scoring\n",
    "greater needs to be better the way it's written. So you\n",
    "can’t use mean_squared_error for doing a grid search,\n",
    "because the grid search assumes greater is better so you\n",
    "have to use negative mean squared error. So for regression\n",
    "or log loss, you use mean squared error. For classification,\n",
    "you need to use the neg log loss one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Scorer interface (vs the metrics interface)\n",
    "```python\n",
    "## metric function interface:\n",
    "y_probs = rf.predict_proba(X_test)\n",
    "ap_rf = average_precision_score(y_test, y_probs[:, 1])\n",
    "y_pred = rf.predict(X_test)\n",
    "ba_rf = balanced_accuracy_score(y_test, y_pred)\n",
    "## scorer interface:\n",
    "from sklearn.metrics import get_scorer\n",
    "ap_scorer = get_scorer('average_precision')\n",
    "ap_rf = ap_scorer(rf, X_test, y_test)\n",
    "ab_scorer = get_scorer('balanced_accuracy')\n",
    "ba_rf = ab_scorer(rf, X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing you your own callable\n",
    "- Takes estimator, X, y\n",
    "- Returns score – higher is better (always!)\n",
    "```python\n",
    "def accuracy_scoring(est, X, y):\n",
    "        return (est.predict(X) == y).mean()\n",
    "```\n",
    "\n",
    "\n",
    "You can also provide your own metric, for example, if you\n",
    "want to do multiclass ROC AUC, you can provide a callable as\n",
    "scoring instead of a string. For any of the built-in ones,\n",
    "you can just provide a string.\n",
    "\n",
    "In this case, I’ve re-implemented accuracy. And the\n",
    "arguments for this needs to be an estimator, x - which is\n",
    "the test data and y - which is the test data true labels or\n",
    "whatever data you want to score. To re-implement accuracy,\n",
    "you have to call predict on the test data, check whether\n",
    "it's equal to y, and then compute the mean. This is actually\n",
    "a very powerful framework so you can do a lot of things with\n",
    "it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can access the model!\n",
    ".tiny-code[\n",
    "\n",
    "```python\n",
    "def nonzero(est, X, y):\n",
    "    return np.sum(est.coef_ != 0)\n",
    "\n",
    "param_grid = {'alpha': np.logspace(-5, 0, 10)}\n",
    "## scoring can be string, single scorer, list or dict\n",
    "grid = GridSearchCV(Lasso(), param_grid, return_train_score=True,\n",
    "                    scoring={'r2': 'r2', 'num_nonzero': nonzero}, refit='r2')\n",
    "grid.fit(X_train, y_train)\n",
    "a = results.plot('param_alpha', 'mean_train_r2') \n",
    "b = results.plot('param_alpha', 'mean_test_r2', ax=plt.gca())\n",
    "ax2 = plt.gca().twinx()\n",
    "results.plot('param_alpha', 'mean_train_num_nonzero', ax=ax2, c='k')\n",
    "```\n",
    "\n",
    "]\n",
    ".center[\n",
    "![:scale 50%](images/lasso_alpha_triazine.png)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "The main thing that I want to illustrate here is that you\n",
    "can have access to the model. So you can write a function\n",
    "for grid search that can do anything with the model it\n",
    "wants. So you can do deep introspection into the model and\n",
    "use that for your model selection. And then I can set up set\n",
    "scoring and then my callable and then I can run grid search\n",
    "as I usually would. Only now it's select a bigger C because\n",
    "bigger C give fewer support vectors. \n",
    "\n",
    "Question is why is the default accuracy? \n",
    "\n",
    "I guess one reason is, we can't change it anymore. Because\n",
    "everybody assumes it is and if it would be anything else,\n",
    "people would be very confused because that's sort of the\n",
    "most natural metric that people think about, like the number\n",
    "of correctly classified samples. Otherwise, in the first\n",
    "lecture, I would have to explain ROC AUC to you if it was\n",
    "ROC AUC and it would be harder to understand for people.\n",
    "\n",
    "Question is if I don't have probabilities can I compute ROC\n",
    "and yes, it does not depend probabilities at all. It's just\n",
    "I go through all possible thresholds. It's really just a\n",
    "ranking. I go through all possible thresholds and look at if\n",
    "I've used this threshold and look at what's the precision?\n",
    "What's the recall? Or what's the false positive rate, what's\n",
    "the true positive rate, and you only need a ranking. \n",
    "\n",
    "Basically, I only need to be able to sort the points and\n",
    "then for each possible threshold, I need to call the\n",
    "precision and recall. And so as long as it can sort the\n",
    "points, it doesn't matter. I can use any monotonous\n",
    "transformation of the decision function and it will still be\n",
    "the same because it only considers the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "import sklearn\n",
    "sklearn.set_config(print_changed_only=True)\n",
    "mpl.rcParams['legend.numpoints'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"data/bank-campaign.csv\")\n",
    "X = data.drop(\"target\", axis=1).values\n",
    "y = data.target.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "pred_most_frequent = dummy_majority.predict(X_test)\n",
    "print(\"predicted labels: %s\" % np.unique(pred_most_frequent))\n",
    "print(\"score: %f\" % dummy_majority.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\n",
    "pred_tree = tree.predict(X_test)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "pred_dummy = dummy.predict(X_test)\n",
    "print(\"dummy score: %f\" % dummy.score(X_test, y_test))\n",
    "\n",
    "logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "pred_logreg = logreg.predict(X_test)\n",
    "print(\"logreg score: %f\" % logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_test, pred_logreg)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(logreg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most frequent class:\")\n",
    "print(confusion_matrix(y_test, pred_most_frequent))\n",
    "print(\"\\nDummy model:\")\n",
    "print(confusion_matrix(y_test, pred_dummy))\n",
    "print(\"\\nDecision tree:\")\n",
    "print(confusion_matrix(y_test, pred_tree))\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(confusion_matrix(y_test, pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"f1 score most frequent: %.2f\" % f1_score(y_test, pred_most_frequent, pos_label=\"yes\"))\n",
    "print(\"f1 score dummy: %.2f\" % f1_score(y_test, pred_dummy, pos_label=\"yes\"))\n",
    "print(\"f1 score tree: %.2f\" % f1_score(y_test, pred_tree, pos_label=\"yes\"))\n",
    "print(\"f1 score logreg: %.2f\" % f1_score(y_test, pred_logreg, pos_label=\"yes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_most_frequent,\n",
    "                            target_names=[\"no\", \"yes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_tree,\n",
    "                            target_names=[\"no\", \"yes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_logreg,\n",
    "                            target_names=[\"no\", \"yes\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking uncertainty into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall curves and ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "\n",
    "# create a similar dataset as before, but with more samples to get a smoother curve\n",
    "X, y = make_blobs(n_samples=8000, centers=2, cluster_std=[7.0, 2], random_state=22, shuffle=False)\n",
    "X, y = X[:4500], y[:4500]\n",
    "\n",
    "# build an imbalanced synthetic dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)\n",
    "\n",
    "pr_svc = plot_precision_recall_curve(svc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0, max_features=2)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# RandomForestClassifier has predict_proba, but not decision_function\n",
    "pr_rf = plot_precision_recall_curve(rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both in the same axes\n",
    "ax = plt.gca()\n",
    "pr_rf.plot(ax=ax)\n",
    "pr_svc.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"f1_score of random forest: %f\" % f1_score(y_test, rf.predict(X_test)))\n",
    "print(\"f1_score of svc: %f\" % f1_score(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "ap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "ap_svc = average_precision_score(y_test, svc.decision_function(X_test))\n",
    "print(\"average precision of random forest: %f\" % ap_rf)\n",
    "print(\"average precision of svc: %f\" % ap_svc)\n",
    "print(\"average precision of svc: %f\" % pr_svc.average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristics (ROC) and AUC\n",
    "\\begin{equation}\n",
    "\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "roc_svc = plot_roc_curve(svc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_svc.plot()\n",
    "roc_rf = plot_roc_curve(rf, X_test, y_test, ax=roc_svc.ax_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "svc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\n",
    "print(\"AUC for Random Forest: %f\" % rf_auc)\n",
    "print(\"AUC for SVC: %f\" % svc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using evaluation metrics in model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# default scoring for classification is accuracy\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "print(\"default scoring \", cross_val_score(rf, X, y))\n",
    "# providing scoring=\"accuracy\" doesn't change the results\n",
    "explicit_accuracy =  cross_val_score(rf, X, y,  scoring=\"accuracy\")\n",
    "print(\"explicit accuracy scoring \", explicit_accuracy)\n",
    "ap =  cross_val_score(rf, X, y, scoring=\"average_precision\")\n",
    "print(\"average precision\", ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "res = cross_validate(RandomForestClassifier(), X, y,\n",
    "                     scoring=[\"accuracy\", \"average_precision\", \"recall_macro\"],\n",
    "                     return_train_score=True, cv=5)\n",
    "pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import SCORERS\n",
    "print(sorted(SCORERS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "data = pd.read_csv(\"data/bank-campaign.csv\")\n",
    "\n",
    "# back to the bank campaign\n",
    "X = data.drop(\"target\", axis=1).values\n",
    "y = data.target.values == \"no\"\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=.1, test_size=.1, random_state=0)\n",
    "\n",
    "param_grid = {'logisticregression__C': [0.0001, 0.01, 0.1, 1, 10]}\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "\n",
    "# using AUC scoring instead:\n",
    "grid = GridSearchCV(model, param_grid=param_grid,\n",
    "                    scoring=[\"roc_auc\", 'average_precision', 'accuracy'],\n",
    "                    refit='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"\\nGrid-Search with AUC\")\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best cross-validation score (AUC):\", grid.best_score_)\n",
    "print(\"Test set AUC: %.3f\" % grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(grid.cv_results_)\n",
    "res[['mean_test_roc_auc', 'mean_test_accuracy', 'mean_test_average_precision']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Load the adult dataset from ``data/adult.csv`` (or pick another dataset), and split it into training and test set.\n",
    "Apply grid-search to the training set, searching for the best C for Logistic Regression using AUC.\n",
    "Plot the ROC curve and precision-recall curve of the best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
