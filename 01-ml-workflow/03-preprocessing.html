

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Preprocessing and Scaling &#8212; Applied Machine Learning in Python</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Categorical Variables" href="04-categorical-variables.html" />
    <link rel="prev" title="Supervised learning" href="02-supervised-learning.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Applied Machine Learning in Python</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../index.html">1. Welcome</a>
  </li>
  <li class="">
    <a href="../00-introduction/00-introduction.html">2. Introduction</a>
  </li>
  <li class="active">
    <a href="00-ml-workflow.html">3. The Machine Learning Workflow</a>
  <ul class="nav sidenav_l2">
    <li class="">
      <a href="01-data-loading.html">3.1 Data Loading and Basic Preprocessing</a>
    </li>
    <li class="">
      <a href="02-supervised-learning.html">3.2 Supervised learning</a>
    </li>
    <li class="active">
      <a href="">3.3 Preprocessing and Scaling</a>
    </li>
    <li class="">
      <a href="04-categorical-variables.html">3.4 Categorical Variables</a>
    </li>
    <li class="">
      <a href="08-imputation.html">3.5 Missing Values</a>
    </li>
    <li class="">
      <a href="10-model-validation-and-tuning.html">3.6 Model evaluation</a>
    </li>
    <li class="">
      <a href="12-pipelines-gridsearch.html">3.7 Combining Pipelines and Model Evaluation</a>
    </li>
  </ul>
  </li>
  <li class="">
    <a href="../02-supervised-learning/index.html">4. Supervised Learning Algorithms</a>
  </li>
  <li class="">
    <a href="../03-unsupervised-learning/index.html">5. Unsupervised Learning Algorithms</a>
  </li>
  <li class="">
    <a href="../04-model-evaluation/index.html">6. Model Evaluation</a>
  </li>
  <li class="">
    <a href="../05-advanced-topics/index.html">7. Advanced Topics</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../_sources/01-ml-workflow/03-preprocessing.ipynb.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.ipynb</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Edit this page -->
        

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
            <div class="dropdown-buttons">
                
                <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/01-ml-workflow/03-preprocessing.ipynb"><button type="button" class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip" data-placement="left"><img class="binder-button-logo" src="../_static/images/logo_binder.svg" alt="Interact on binder">Binder</button></a>
                
                
                
            </div>
        </div>
        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#the-importance-of-scaling" class="nav-link">The importance of scaling</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#ways-to-scale-data" class="nav-link">Ways to Scale Data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#scaling-the-test-data" class="nav-link">Scaling the test data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#knn-with-scaling-on-breast-cancer-data" class="nav-link">KNN with Scaling on Breast Cancer data</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#picking-the-right-scaler" class="nav-link">Picking the right scaler</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#heavy-tailed-and-power-law-data" class="nav-link">Heavy-tailed and power-law data</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#thinking-with-pipelines" class="nav-link">Thinking with Pipelines</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#accessing-pipeline-steps" class="nav-link">Accessing Pipeline steps</a>
        </li>
    
            </ul>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="preprocessing-and-scaling">
<h1>Preprocessing and Scaling<a class="headerlink" href="#preprocessing-and-scaling" title="Permalink to this headline">¶</a></h1>
<div class="section" id="the-importance-of-scaling">
<h2>The importance of scaling<a class="headerlink" href="#the-importance-of-scaling" title="Permalink to this headline">¶</a></h2>
<p>We saw in chapter TODO that the breast cancer dataset contains features with many different magnitudes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="c1"># specifying &quot;as_frame=True&quot; returns the data as a dataframe in addition to a numpy array</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">cancer_df</span> <span class="o">=</span> <span class="n">cancer</span><span class="o">.</span><span class="n">frame</span>
<span class="n">cancer_features</span> <span class="o">=</span> <span class="n">cancer_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;target&#39;</span><span class="p">)</span>
<span class="n">cancer_features</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x20f0aad8948&gt;
</pre></div>
</div>
<img alt="../_images/03-preprocessing_3_1.png" src="../_images/03-preprocessing_3_1.png" />
</div>
</div>
<p>When we applied the Nearest Neighbors algorithm in the previous chapter, we didn’t pay any mind to that, and still got quite good results. Arguably, we just got lucky.
In fact, the scale of the data is tremendously important when using any distance based algorith, such as nearest neighbors, as the euclidean distance will but much larger emphasis on features that have larger magnitude.
This is illustrated in Figure TODO, which shows the features ‘worst area’ against ‘worst concave points’. You can see that the x and y axis have vastly different scales. In fact, if we <code class="docutils literal notranslate"><span class="pre">set_aspect('equal')</span></code> we would only see a line (try it!).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">cancer_df</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;worst area&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;worst concave points&#39;</span><span class="p">,</span>
                            <span class="n">c</span><span class="o">=</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">colormap</span><span class="o">=</span><span class="s1">&#39;bwr&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1">#TODO xlabel?</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/03-preprocessing_5_0.png" src="../_images/03-preprocessing_5_0.png" />
</div>
</div>
<p>Now if we apply 5-nearest neighbors on the data the way it is, the algorithm will basically ignore the y-axis, because distances on the y-axis are at most 0.3, while distances on the x-axis are in the hundreds or thousands.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/03-preprocessing_7_0.png" src="../_images/03-preprocessing_7_0.png" />
</div>
</div>
<p>This was probably not what we had in mind, and if we rescale the data, so that both features have a similar range (say between 0 and 1), the result is much different:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/03-preprocessing_9_0.png" src="../_images/03-preprocessing_9_0.png" />
</div>
</div>
<p>The difference between these two figures is simply whether the data was scaled to a common range or not, and clearly the outcome is dramatically different. The effect is not as large for all machine learning models, but many models do require scaling your data.</p>
<p>An example I think is illustrative of why this is important is when dealing with units. Imagine you have a dataset consisting of a persons height, and how far they live from their workplace. For each of these, you can pick a unit. Now say you picked cm (or inches) for both of them. Then the distance to their workplace would be a very large number, so without scaling, KNeighborsClassifier or any other distance-based algorithm would pay much more attention to it. Now say you decided it’s strange to measure such long distances in cm (or inches), and you decide to use kilometers (or miles). Now, the height will have a larger magnitude than the distance to work, and a distance base algorithm will give more weight to that feature.
Clearly the unit you are choosing should not impact the machine learning model, and usually you don’t a-priory have information about which features are important-that’s what you want the model to figure out. Bringing all features on the same scale allows the model that, without biasing it in one direction or another in an arbitrary way.</p>
<p>Now let’s look at some ways to scale your data with scikit-learn.</p>
</div>
<div class="section" id="ways-to-scale-data">
<h2>Ways to Scale Data<a class="headerlink" href="#ways-to-scale-data" title="Permalink to this headline">¶</a></h2>
<p>There are several ways to scale your data, shown in figure TODO below.
Each of these methods is implemented in a Python class in scikit-learn.
One of the most common ways to scale data is to ensure the data has zero mean and unit variance after scaling (also known as standardization or sometimes z-scoring), which is implemented in the <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>.</p>
<p><img alt=":scale 80%" src="../_images/scaler_comparison_scatter.png" /></p>
<p>Let’s have a look on how to use the StandardScaler on the breast cancer dataset. As with our classification models, we first import and instantiate the class:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="c1"># While the standard scaler has some options,</span>
<span class="c1"># those are rarely used and we usually instantiate it with the default parameters:</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Similar to fitting a model, we can fit our scaling to the data using the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method on the training set. This simply estimates mean and standard deviation. You could think of scaling as a very simple unsupervised learning procedure, which certainly doesn’t require the target <code class="docutils literal notranslate"><span class="pre">y</span></code>, so it is enough to pass our feature <code class="docutils literal notranslate"><span class="pre">X_train</span></code> to the fit method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>StandardScaler()
</pre></div>
</div>
</div>
</div>
<p>Now, <code class="docutils literal notranslate"><span class="pre">scaler</span></code> has stored the mean and standard deviation of our data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">mean_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">scale_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>[1.41226643e+01 1.91988498e+01 9.18850235e+01 6.54919484e+02
 9.55561972e-02 1.02506714e-01 8.74703427e-02 4.77440869e-02
 1.80024413e-01 6.26067840e-02 4.02111737e-01 1.20767089e+00
 2.86367911e+00 4.01327254e+01 7.03638028e-03 2.53726854e-02
 3.22836148e-02 1.18490845e-02 2.04911479e-02 3.78268850e-03
 1.62118592e+01 2.55068779e+01 1.06886784e+02 8.73720657e+02
 1.31201831e-01 2.47729484e-01 2.67697533e-01 1.12653077e-01
 2.87796948e-01 8.33459390e-02]
[3.53058849e+00 4.22578613e+00 2.42759136e+01 3.56022552e+02
 1.39543507e-02 5.14081284e-02 7.85193143e-02 3.78165371e-02
 2.67869023e-02 7.21878787e-03 2.86199676e-01 5.43678655e-01
 2.09469043e+00 4.79685520e+01 3.09086708e-03 1.82308744e-02
 2.99782397e-02 6.27826746e-03 7.99741486e-03 2.68195906e-03
 4.77599313e+00 6.01990200e+00 3.30360621e+01 5.64580149e+02
 2.31962297e-02 1.49219846e-01 1.98739169e-01 6.43530007e-02
 6.15045058e-02 1.75602959e-02]
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Attributes that are estimated from the training data are named with trailing underscores, such as <code class="docutils literal notranslate"><span class="pre">mean_</span></code> in scikit-learn. This is to distinguish them for parameter that are passed to the model at instantiation time, such as <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> in <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code>.</p>
</div>
<p>To apply the scaling, we can use the <code class="docutils literal notranslate"><span class="pre">transform</span></code> method, which will subtract the mean and divide by the standard deviation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>(426, 30)
(426, 30)
</pre></div>
</div>
</div>
</div>
<p>It’s important to note here that the return value of <code class="docutils literal notranslate"><span class="pre">transform</span></code> for any scikit-learn transformation is a numpy array, independent of whether the input <code class="docutils literal notranslate"><span class="pre">X_train</span></code> was a numpy array or a pandas DataFrame:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;numpy.ndarray&#39;&gt;
</pre></div>
</div>
</div>
</div>
<p>This means we no longer have the column names associated with the data after scaling, though we can just take the column names from the input in this case, as scaling doesn’t change the order of columns.
So if we prefer to have a dataframe again, we could create one like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># You can convert your data back to a dataframe after scaling, though that&#39;s usually not needed</span>
<span class="c1"># it can make plotting a bit easier, though.</span>
<span class="n">X_train_scaled_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To illustrate the effect of scaling, let’s look at the boxplot of the scaled data, and compare it to Figure TODO again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_scaled_df</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x20f0f2524c8&gt;
</pre></div>
</div>
<img alt="../_images/03-preprocessing_26_1.png" src="../_images/03-preprocessing_26_1.png" />
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For many preprocessing operations such as scaling, it’s very common to fit the model on the training dataset, and then transform the training datset.
As this is such a common pattern, there is a shortcut to do both of these at once, which will save you some typing, but might also allow a more efficient computation, and is called <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code>.
So we could equivalently write the above code as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
<p>Another common patter, which uses the fact that <code class="docutils literal notranslate"><span class="pre">fit</span></code> returns the object itself, is this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
<p>However, this doesn’t make use of potential computational shortcuts that are possible when computing <code class="docutils literal notranslate"><span class="pre">fit</span></code> and <code class="docutils literal notranslate"><span class="pre">transform</span></code> together in <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code>.</p>
<p>Scikit-learn guarantees that the output of <code class="docutils literal notranslate"><span class="pre">.fit(X_train).transform(X_train)</span></code> is the same as ```.fit_transform(X_train)`` up to numerical precision issues.</p>
</div>
<p>Another very common wayto scale data is to specify a common minimum and maximum for all features, which is implemented in <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code>. The <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> can scale to aribrary ranges, though the default of scaling to the range between 0 and 1 usually works just fine. Another common option is to scale between -1 and 1.
Figure TODO shows two more scalers, the <code class="docutils literal notranslate"><span class="pre">RobustScaler</span></code> and the <code class="docutils literal notranslate"><span class="pre">Normalizer</span></code>. The <code class="docutils literal notranslate"><span class="pre">RobustScaler</span></code> works quite similar to the <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> in that it shifts and scales each feature.
However, as opposed to using mean and variance like StandardScaler, the <code class="docutils literal notranslate"><span class="pre">RobustScaler</span></code> uses median and <a class="reference external" href="https://en.wikipedia.org/wiki/Interquartile_range">interquartile ranges</a>. In other words, <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> ensures that the median of the data is zero, and that 50% of the samples is between -0.5 and 0.5 for each feature.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is called ‘robust’ because these measures are known as ‘robust statistics’, in that they are robust against the presence of outliers.
The mean and variance used in StandardScaler are not robust in that way: a single data point, if it’s far enough away from the rest of the data, can have unlimited impact on mean and variance.
The same is not true for median and interquartile range, which are not impacted by single samples. In practice it might be a good idea to inspect your data and look for outliers and their meaning manually, and despite its better statistical properties, <code class="docutils literal notranslate"><span class="pre">RobustScaler</span></code> is used much less than <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>.</p>
</div>
<p>Finally, there is also the <code class="docutils literal notranslate"><span class="pre">Normalizer</span></code>, which is a bit of an odd one out in that it operates on each row independently, while all the other scalers operate on each column independently.
The normalizer divides each row by its length, either using euclidean distances (aka the L2 norm, using <code class="docutils literal notranslate"><span class="pre">norm='l2'</span></code>, the default) or by using the manhatten distance (aka the L1 norm, or sum of absolute values by using <code class="docutils literal notranslate"><span class="pre">norm='l1'</span></code>).
Both of these transformations can be seen as projecting all data points onto the unit ball, i.e. the circle with radius 1 (the circle with radius 1 in the l1 norm is a diamond).
For the L2 norm, this means normalizing away the length of the vector, and only keeping the direction. For the L1 norm, this has a very intuitive interpretation if all numbers are positive: it keeps the relative proportions, while discarding the overall sum. Both of these normalizations are often used with count data, as in the following example:
Assume each row corresponds a visitor that frequents a shopping mall, and each column corresponds to the different shops they are visiting:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">shoppers</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;groceries&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="s1">&#39;fashion&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;hardware&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]})</span>
<span class="n">shoppers</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>groceries</th>
      <th>fashion</th>
      <th>hardware</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>10</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20</td>
      <td>200</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10</td>
      <td>0</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Looking at distances in the original space, Shopper 0 is most similar to Shopper 2, as the counts are most similar. However, it might make more sense to have Shopper 0 be more similar to Shopper 2, as the both mostly show for fashion.
the Normalizer accomplishes that by normalizing by the total number of visits (when using the l1 distance):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Normalizer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">shoppers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>array([[0.        , 0.83333333, 0.16666667],
       [0.09090909, 0.90909091, 0.        ],
       [0.76923077, 0.        , 0.23076923]])
</pre></div>
</div>
</div>
</div>
<p>However, overall, using the <code class="docutils literal notranslate"><span class="pre">Normalizer</span></code> is used much less common than using the <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> or <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code>.</p>
</div>
<div class="section" id="scaling-the-test-data">
<h2>Scaling the test data<a class="headerlink" href="#scaling-the-test-data" title="Permalink to this headline">¶</a></h2>
<p>So far, we have only scaled the training data. To apply our model to any new data, including the test set, we clearly need to scale that data as well.
To apply the scaling to any other data, simply call <code class="docutils literal notranslate"><span class="pre">transform</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>What this does is that it subtracts the <em>training set mean</em> and divides by the <em>training set standard deviation</em>. In other words, it doesn’t compute these statistics for the test set, but it uses the statistics computed on the training set when calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>. This difference is critically important as illustrated in Figure TODO, which shows a two-dimensional dataset. Here, training data is shown in blue and test data is shown in red.
We apply a <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> to the data, which results in the minimum and maximum on the training set being exactly 0 and 1 respectively, for each of the features. I.e. the blue points have minimum and maximum values of 0 and 1.
Then, we applied the same scaling to achieve these values to the test set. As a result, Figure TODO b) looks exactly the same as Figure TODO a), with the only difference being the tick marks.
Note how the minimum and maximum on the test set are not 0 and 1 in this figure.</p>
<p>Now, imagine we treated the test set in the same way we treated the training set, i.e. if we would call <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code> on it and re-estimate the scale of the data.
That would lead to the test data being scaled in a way that the minimum and maximum are 0 and 1 respectively for each feature, as shown in Figure TODO c). Not that now the figure looks different, and the position of the red points has changed when relating them to the blue points. A model trained on the blue data can now no longer be applied to the red data, as the red data was transformed in a different way.</p>
<p>To summarize, <strong>only ever fit any model or transformation on the training data, never call <code class="docutils literal notranslate"><span class="pre">fit</span></code> or <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code> on the  test data.</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some people prefer to scale the data before splitting it into training and test set. While that might sometimes simplify the code, I would recommend against it.
Your goal will be to apply the model in production to new data, and you want your test data to stand in for this new data that you haven’t observed yet. However, this new data will not be part of the set you use to compute your rescaling. If you included your test data in the scaling, that means that your new data is treated differently from the training set, which defeats the purpose of the training set.
In practice, this is unlikely to have a large impact, as computing mean and standard deviation is relatively stable on well-behaved datasets. However, I recommend to adhere to best practices, and split off the test set before doing <em>any</em> processing.</p>
</div>
<p><img alt=":scale 100%" src="../_images/no_separate_scaling.png" /></p>
</div>
<div class="section" id="knn-with-scaling-on-breast-cancer-data">
<h2>KNN with Scaling on Breast Cancer data<a class="headerlink" href="#knn-with-scaling-on-breast-cancer-data" title="Permalink to this headline">¶</a></h2>
<p>Now, let’s recap the whole process of scaling a dataset and then using it in Nearest Neighbors classification:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="c1"># load the data, split off the target</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">cancer_df</span> <span class="o">=</span> <span class="n">cancer</span><span class="o">.</span><span class="n">frame</span>
<span class="n">cancer_features</span> <span class="o">=</span> <span class="n">cancer_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;target&#39;</span><span class="p">)</span>
<span class="c1"># Split the data into training and test set</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer_features</span><span class="p">,</span> <span class="n">cancer_df</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>

<span class="c1"># For comparison, run KNN again without scaling:</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy without scaling: &quot;</span><span class="p">,</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># Scale the training data using StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="c1"># Fit a knn model on the scaled data</span>
<span class="n">knn_scaled</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">knn_scaled</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Scale the test data</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy with scaling: &quot;</span><span class="p">,</span> <span class="n">knn_scaled</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>accuracy without scaling:  0.9300699300699301
accuracy with scaling:  0.986013986013986
</pre></div>
</div>
</div>
</div>
<p>We can see that we could improve our model quite a bit simply by scaling the data!</p>
</div>
<div class="section" id="picking-the-right-scaler">
<h2>Picking the right scaler<a class="headerlink" href="#picking-the-right-scaler" title="Permalink to this headline">¶</a></h2>
<p>We already discussed how the scalers differ a bit above. I think it’s a relatively save default to use <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> after doing some exploratory analysis.
However, if there is very clear minimum and maximum values, then a <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> might be more appropriate. For example if your feature is a 10 point likert scale, at <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> might be more natural than a <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>.
Similarly, when looking at image intensities that are often between 0 and 255, a <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> seem more natural.
On the other hand, if your data looks more Gaussian distributed, or even has long tails, using the <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> seems like a bad idea, as changing the split of the data can easily change the position of the maximum:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate a numpy random state with a fixed seed</span>
<span class="n">rs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">23</span><span class="p">)</span>
<span class="c1"># generate 100 normally distributed values</span>
<span class="n">normal1</span> <span class="o">=</span> <span class="n">rs</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># another 100 normally distributed values</span>
<span class="n">normal2</span> <span class="o">=</span> <span class="n">rs</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># scale the first normal dataset</span>
<span class="n">normal1_mm</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">normal1</span><span class="p">)</span>
<span class="n">normal1_ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">normal1</span><span class="p">)</span>

<span class="c1"># scale the second normal dataset</span>
<span class="n">normal2_mm</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">normal2</span><span class="p">)</span>
<span class="n">normal2_ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">normal2</span><span class="p">)</span>

<span class="c1"># constrained_layout ensures there is no overlapping figures and ticks</span>
<span class="c1"># similar to plt.tight_layout() but better</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Normal data&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal1</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;draw 1&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal2</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;draw 2&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;After min-max scaling&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal1_mm</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal2_mm</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;After standard scaling&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal1_ss</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal2_ss</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Sample from a dataset with a fixed minimum and maximum value</span>
<span class="c1"># in this case we use beta-distributed data</span>

<span class="n">uniform1</span> <span class="o">=</span> <span class="n">rs</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">uniform2</span> <span class="o">=</span> <span class="n">rs</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># scale the first uniform dataset</span>
<span class="n">uniform1_mm</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">uniform1</span><span class="p">)</span>
<span class="n">uniform1_ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">uniform1</span><span class="p">)</span>

<span class="c1"># scale the second uniform dataset</span>
<span class="n">uniform2_mm</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">uniform2</span><span class="p">)</span>
<span class="n">uniform2_ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">uniform2</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Fixed range data&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">uniform1</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;draw 1&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">uniform2</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;draw 2&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;After min-max scaling&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">uniform1_mm</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">uniform2_mm</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;After standard scaling&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">uniform1_ss</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">uniform2_ss</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>(array([9., 1., 1., 3., 0., 6.]),
 array([-1.01613112, -0.61158548, -0.20703985,  0.19750578,  0.60205142,
         1.00659705,  1.41114268]),
 &lt;a list of 6 Patch objects&gt;)
</pre></div>
</div>
<img alt="../_images/03-preprocessing_43_1.png" src="../_images/03-preprocessing_43_1.png" />
</div>
</div>
<p>Figure TODO looks at histograms for two different synthetic data distributions, normally distributed data on the left, and data with a fixed range on the right.
For both distributions, we draw two datasets. In expectations, these datasets (with histograms in orange and blue respectively) should be indentical, as they are drawn from the same underlying distribution.
If we look on the right hand side, the normal data looks distored after min-max scaling: the orange distribution looks wider than the blue one. That is because computing the minmum and maximum from normally distributed data is not very robust. On the other hand, estimating the mean and standard deviation is very stable, so standard scaling shows the distributions to be similar.</p>
<p>for the distribution on the left hand side, minimum and maximum are well-defined, so min-max scaling essentially leaves the data untouched, while mean and standard deviation are unstable for this distribution, and so after standard scaling, the distributions seem to be different.</p>
<p>These synthetic samples were chosen to emphasize this effect, in particular I used 100 and 20 samples respectively. As the dataset becomes larger, estimating mean and standard deviation becomes more stable, and generally using <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> should be fine.</p>
<div class="section" id="heavy-tailed-and-power-law-data">
<h3>Heavy-tailed and power-law data<a class="headerlink" href="#heavy-tailed-and-power-law-data" title="Permalink to this headline">¶</a></h3>
<p>One area where there are potential issues with using both <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> and <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> is heavy-tailed data such as data that follows a power law. We already saw an example of such data, the income data from the lending club dataset.
Income is often model with the <a class="reference external" href="https://en.wikipedia.org/wiki/Pareto_distribution">Pareto distribution</a>, which has the somewhat suprising property of having invinite mean and variance in some cases. Therefore mean and variance are not good measures to describe this data and can’t be estimated robustly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">23</span><span class="p">)</span>
<span class="n">sample1</span> <span class="o">=</span> <span class="n">rs</span><span class="o">.</span><span class="n">pareto</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">sample2</span> <span class="o">=</span> <span class="n">rs</span><span class="o">.</span><span class="n">pareto</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample 1 mean:&quot;</span><span class="p">,</span> <span class="n">sample1</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample 2 mean:&quot;</span><span class="p">,</span> <span class="n">sample2</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>Sample 1 mean: 1638.0589394545575
Sample 2 mean: 282.50804144173316
</pre></div>
</div>
</div>
</div>
<p>Here we can see that the mean can vary widely over different samples from the same distribution. Power-law distributions are also not readily plotted using histograms:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample1</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;draw 1&#39;</span><span class="p">)</span>
<span class="c1"># we&#39;re reusing the bins determined from the first sample</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample2</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;draw 2&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x210176273c8&gt;
</pre></div>
</div>
<img alt="../_images/03-preprocessing_47_1.png" src="../_images/03-preprocessing_47_1.png" />
</div>
</div>
<p>A common trick for both features and targets that follow a power law is to apply the log transform, which both makes scaling and plotting easier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sample1</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;draw 1&#39;</span><span class="p">)</span>
<span class="c1"># we&#39;re reusing the bins determined from the first sample</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sample2</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;draw 2&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x21017e45688&gt;
</pre></div>
</div>
<img alt="../_images/03-preprocessing_49_1.png" src="../_images/03-preprocessing_49_1.png" />
</div>
</div>
<p>Overall, there is no golden rule for determining the preprocessing, and often it is a good idea to evalute several alternatives, and use the one that results in the most accurate model.</p>
</div>
</div>
<div class="section" id="thinking-with-pipelines">
<h2>Thinking with Pipelines<a class="headerlink" href="#thinking-with-pipelines" title="Permalink to this headline">¶</a></h2>
<p>Preprocessing your data is required in nearly every application, and the pattern we’ve seen in TODO is very common.
For that reason, scikit-learn has some tools to make your life easier, in particular the <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> class which allows you to chain preprocessing steps and models together.
Let’s take a look at our example using the <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> and <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> on the breast cancer dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Scale the training data using StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="c1"># Fit a knn model on the scaled data</span>
<span class="n">knn_scaled</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">knn_scaled</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Scale the test data</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy with scaling: &quot;</span><span class="p">,</span> <span class="n">knn_scaled</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>accuracy with scaling:  0.986013986013986
</pre></div>
</div>
</div>
</div>
<p>Using a <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> we can write this equivalently as:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="c1"># build a pipeline with two steps, the scaler and the classifier</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="c1"># fit both steps on the training data:</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># score the test dataset:</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy with scaling: &quot;</span><span class="p">,</span> <span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>accuracy with scaling:  0.986013986013986
</pre></div>
</div>
</div>
</div>
<p>As you can see, we got the result, with much less code. Using pipelines allows you to encapsulate your whole processing workflow into a single Python object. This makes it much easier to avoid bugs, such as leaking information or forgetting to apply the same preprocessing steps to the test data that you applied to the training data.
In this instance we used <code class="docutils literal notranslate"><span class="pre">make_pipeline</span></code> which is a convenience function to create an object of the <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> class with very little code. The <code class="docutils literal notranslate"><span class="pre">make_pipeline</span></code> function takes an arbitrary number of arguments (I had passed two), which then become the <em>steps</em> of the pipeline.
I can equivalently create the same pipeline by directly using the <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> class. The <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> class takes a list of steps, each of which is specified as a tuple made out of a string and a scikit-learn model or preprocessor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="c1"># directly use the pipeline class</span>
<span class="c1"># A bunch more parenthesis here than when using make_pipeline</span>
<span class="n">pipe2</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                  <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">))])</span>
</pre></div>
</div>
</div>
</div>
<p>The string is the <em>name</em> of the step, which you can see when displaying the pipeline in Jupyter:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pipe2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>Pipeline(steps=[(&#39;scaler&#39;, StandardScaler()),
                (&#39;classifier&#39;, KNeighborsClassifier())])
</pre></div>
</div>
</div>
</div>
<p>If you use the <code class="docutils literal notranslate"><span class="pre">make_pipeline</span></code> function, the names are assigned automatically, using the lower-cased class name:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pipeline we created using make_pipeline above</span>
<span class="n">pipe</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),
                (&#39;kneighborsclassifier&#39;, KNeighborsClassifier())])
</pre></div>
</div>
</div>
</div>
<p>A pipeline can have arbitrary many steps which are executed in order. Each step is a scikit-learn <em>Estimator</em>, which is the base class and iterface for all the models and processing steps in scikit-learn. So far we’ve only seen the <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> and the scalers, but soon we’ll meet many more.</p>
<div class="margin sidebar">
<p class="sidebar-title">Meta-Estimators</p>
<p>In scikit-learn, objects that are created by putting together one or more existing Estimator are called <em>meta-estimators</em>.
The Pipeline is the first meta-estimator that we encountered, but we’ll see many more, as they are an essential part of scikit-learn.</p>
</div>
<p>All steps except the last step need to be <em>transformers</em> meaning they need to have a <code class="docutils literal notranslate"><span class="pre">transform</span></code> method that can produce a new version of the dataset, such as the scalers do. We can’t change the order to have the <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> first, because it is not a <code class="docutils literal notranslate"><span class="pre">Transformer</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span> <span class="n">StandardScaler</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ne">TypeError</span><span class="p">:</span> <span class="n">All</span> <span class="n">intermediate</span> <span class="n">steps</span> <span class="n">should</span> <span class="n">be</span> <span class="n">transformers</span> <span class="ow">and</span> <span class="n">implement</span> <span class="n">fit</span> <span class="ow">and</span> <span class="n">transform</span> <span class="ow">or</span> <span class="n">be</span> <span class="n">the</span> <span class="n">string</span> <span class="s1">&#39;passthrough&#39;</span> <span class="s1">&#39;KNeighborsClassifier()&#39;</span> <span class="n">doesn</span><span class="s1">&#39;t</span>

</pre></div>
</div>
<p>The last step can be anything. Very often it’s a supervised model like a classifier or regressor, but we can also just build a pipeline out of a scaler (though this is not very interesting as it does the exact same thing as the scaler by itself):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># build a pipeline just consisting of a standard scaler</span>
<span class="n">boring_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">())</span>
<span class="c1"># fit the pipeline</span>
<span class="n">boring_pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="c1"># this pipeline has a transform as the last step is a transformer</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">boring_pipe</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Often there’s multiple processing steps, such as dealing with categorical data or missing values, tools for which we’ll see in the next couple of sections. In that cases there’s multiple transformers before the final model.
But so what happens inside the pipeline? Let’s look at figure TODO, which shows a schematic using two transformers which I called <code class="docutils literal notranslate"><span class="pre">T1</span></code> and <code class="docutils literal notranslate"><span class="pre">T2</span></code> and a classifier that’s creatively called <code class="docutils literal notranslate"><span class="pre">Classifier</span></code>.</p>
<p><img alt=":scale 70%" src="../_images/pipeline.png" /></p>
<p>When you call the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method of the pipeline, the training data (called <code class="docutils literal notranslate"><span class="pre">X</span></code> in the diagram) is passed to the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method of the first step, <code class="docutils literal notranslate"><span class="pre">T1</span></code>, together with the target <code class="docutils literal notranslate"><span class="pre">y</span></code> (if any was passed).
The transformer <code class="docutils literal notranslate"><span class="pre">T1</span></code> is fitted and then used to produce the transformed version of <code class="docutils literal notranslate"><span class="pre">X</span></code> called <code class="docutils literal notranslate"><span class="pre">X1</span> <span class="pre">=</span> <span class="pre">T1.transform(X)</span></code>. Then, <code class="docutils literal notranslate"><span class="pre">X1</span></code> is used as training data for the next step, here <code class="docutils literal notranslate"><span class="pre">T2</span></code>, so the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method of <code class="docutils literal notranslate"><span class="pre">T2</span></code> is called with <code class="docutils literal notranslate"><span class="pre">X1</span></code> and the original target <code class="docutils literal notranslate"><span class="pre">y</span></code>, <code class="docutils literal notranslate"><span class="pre">T2</span></code> is fit, and then used to produce a transformed version of <code class="docutils literal notranslate"><span class="pre">X1</span></code> called <code class="docutils literal notranslate"><span class="pre">X2</span></code>. Finally, the last step of the pipeline, <code class="docutils literal notranslate"><span class="pre">Classifier</span></code> is fit using <code class="docutils literal notranslate"><span class="pre">X2</span></code> and the target <code class="docutils literal notranslate"><span class="pre">y</span></code>.
That is the end of the call to <code class="docutils literal notranslate"><span class="pre">pipe.fit</span></code>, and it is exactly what you would do if you’d want to apply two preprocessing steps in order and then fit a classifier.</p>
<p>When calling <code class="docutils literal notranslate"><span class="pre">predict</span></code> on the pipeline, say on some new data <code class="docutils literal notranslate"><span class="pre">X'</span></code>, fist, <code class="docutils literal notranslate"><span class="pre">X'</span></code> is passed to <code class="docutils literal notranslate"><span class="pre">T1.transform</span></code>, producing <code class="docutils literal notranslate"><span class="pre">X'1</span></code>, which is passed to <code class="docutils literal notranslate"><span class="pre">T2.transform</span></code> to produce <code class="docutils literal notranslate"><span class="pre">X'2</span></code>, which is the passed to <code class="docutils literal notranslate"><span class="pre">Classifier.predict</span></code> to produce predictions <code class="docutils literal notranslate"><span class="pre">y'</span></code>. Again, this is exactly what you would do if you would want to apply two preprocessing steps during prediction.</p>
<p>The pipeline always has a <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, as do all scikit-learn <code class="docutils literal notranslate"><span class="pre">Estimators</span></code>. The <code class="docutils literal notranslate"><span class="pre">pipeline</span></code> also has all the methods the <em>last step</em> of the pipeline has. So if the last step is a classifier and has <code class="docutils literal notranslate"><span class="pre">predict</span></code> and <code class="docutils literal notranslate"><span class="pre">score</span></code> methods, then the pipeline will also have <code class="docutils literal notranslate"><span class="pre">predict</span></code> and <code class="docutils literal notranslate"><span class="pre">score</span></code> methods. If the last step is a Transformer, then the pipeline will have a <code class="docutils literal notranslate"><span class="pre">transform</span></code> method instead.</p>
<div class="section" id="accessing-pipeline-steps">
<h3>Accessing Pipeline steps<a class="headerlink" href="#accessing-pipeline-steps" title="Permalink to this headline">¶</a></h3>
<p>Sometimes you fit a pipeline, say the one for the breast cancer dataset above, and then later want to have a closer look at the components. Say you want to find the mean that was computed in the scaler, or say you want to just use the scaler without using the classifier. All of these are easily possible with the scikit-learn pipeline, and there’s several ways to do them. The easiest way is to use square parenthesis to access steps either by name or by index:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># build and fit the pipeline</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># extract the first step using the index in the pipeline</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">scaler</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>StandardScaler()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># extract the first step using the name (lower-case classname when using make_pipeline)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">[</span><span class="s1">&#39;standardscaler&#39;</span><span class="p">]</span>
<span class="n">scaler</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>StandardScaler()
</pre></div>
</div>
</div>
</div>
<p>From there we can easily access any attributes, like the mean:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span><span class="p">[</span><span class="s1">&#39;standardscaler&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>array([1.41818357e+01, 1.95210094e+01, 9.23325587e+01, 6.57887324e+02,
       9.62686620e-02, 1.04557254e-01, 8.93357700e-02, 4.94196714e-02,
       1.80226761e-01, 6.28116901e-02, 4.01481455e-01, 1.23022582e+00,
       2.84531643e+00, 3.97439883e+01, 7.02482864e-03, 2.53715399e-02,
       3.20286876e-02, 1.18411643e-02, 2.02751127e-02, 3.82257746e-03,
       1.63210000e+01, 2.60004695e+01, 1.07655540e+02, 8.82566432e+02,
       1.32349577e-01, 2.57062465e-01, 2.76123444e-01, 1.15864911e-01,
       2.89140845e-01, 8.43960094e-02])
</pre></div>
</div>
</div>
</div>
<p>We can also use the <code class="docutils literal notranslate"><span class="pre">named_steps</span></code> attribute of the pipeline, which allows accessing the steps using <code class="docutils literal notranslate"><span class="pre">.</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="o">.</span><span class="n">standardscaler</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>StandardScaler()
</pre></div>
</div>
</div>
</div>
<p>This only works if the name of the step is a valid Python name (so it doesn’t work if you named your step <code class="docutils literal notranslate"><span class="pre">'standard-scaler'</span></code>). It’s a bit longer than using <code class="docutils literal notranslate"><span class="pre">[]</span></code>, but it allows you do use auto-complete in Jupyter, which can be nice if you want to see what attributes are available.</p>
<p>Pipelines are an essential part of working with scikit-learn: They make your life much easier, your code shorter, and encourage best practices. We will use them throughout the book, and we will see later that not using them will actually lead to mistakes in some cases.</p>
</div>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="02-supervised-learning.html" title="previous page">Supervised learning</a>
    <a class='right-next' id="next-link" href="04-categorical-variables.html" title="next page">Categorical Variables</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Andreas C. Müller<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>