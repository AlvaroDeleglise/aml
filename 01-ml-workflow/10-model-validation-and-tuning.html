

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Model evaluation &#8212; Applied Machine Learning in Python</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="More on Pipelines" href="12-pipelines-gridsearch.html" />
    <link rel="prev" title="Missing Values" href="08-imputation.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Applied Machine Learning in Python</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../index.html">1. Welcome</a>
  </li>
  <li class="">
    <a href="../00-introduction/00-introduction.html">2. Introduction</a>
  </li>
  <li class="active">
    <a href="00-ml-workflow.html">3. The Machine Learning Workflow</a>
  <ul class="nav sidenav_l2">
    <li class="">
      <a href="01-data-loading.html">3.1 Data Loading and Basic Preprocessing</a>
    </li>
    <li class="">
      <a href="02-supervised-learning.html">3.2 Supervised learning</a>
    </li>
    <li class="">
      <a href="03-preprocessing.html">3.3 Preprocessing and Scaling</a>
    </li>
    <li class="">
      <a href="04-categorical-variables.html">3.4 Categorical Variables</a>
    </li>
    <li class="">
      <a href="08-imputation.html">3.5 Missing Values</a>
    </li>
    <li class="active">
      <a href="">3.6 Model evaluation</a>
    </li>
    <li class="">
      <a href="12-pipelines-gridsearch.html">3.7 More on Pipelines</a>
    </li>
  </ul>
  </li>
  <li class="">
    <a href="../02-supervised-learning/index.html">4. Supervised Learning Algorithms</a>
  </li>
  <li class="">
    <a href="../03-unsupervised-learning/index.html">5. Unsupervised Learning Algorithms</a>
  </li>
  <li class="">
    <a href="../04-model-evaluation/index.html">6. Model Evaluation</a>
  </li>
  <li class="">
    <a href="../05-advanced-topics/index.html">7. Advanced Topics</a>
  </li>
</ul>
</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../_sources/01-ml-workflow/10-model-validation-and-tuning.ipynb.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.ipynb</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Source interaction buttons -->
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
            <div class="dropdown-buttons sourcebuttons">
                <a class="repository-button" href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left" title="Source repository"><i class="fab fa-github"></i>repository</button></a>
                <a class="issues-button" href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F01-ml-workflow/10-model-validation-and-tuning.html&body=Your%20issue%20content%20here."><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left" title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
                
            </div>
        </div>
        

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
            <div class="dropdown-buttons">
                
                <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/01-ml-workflow/10-model-validation-and-tuning.ipynb"><button type="button" class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip" data-placement="left"><img class="binder-button-logo" src="../_static/images/logo_binder.svg" alt="Interact on binder">Binder</button></a>
                
                
                
            </div>
        </div>
        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#the-threefold-split" class="nav-link">The Threefold-split</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#implementing-the-threefold-split" class="nav-link">Implementing the threefold split</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#k-fold-cross-validation" class="nav-link">K-Fold Cross-validation</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#cross-validation-with-scikit-learn" class="nav-link">Cross-validation with scikit-learn</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#grid-search-with-cross-validation" class="nav-link">Grid-Search with Cross-Validation</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#gridsearchcv" class="nav-link">GridSearchCV</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#the-best-estimator" class="nav-link">The best_estimator_</a>
        </li>
    
            </ul>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#nested-cross-validation" class="nav-link">Nested Cross-Validation</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#summary" class="nav-link">Summary</a>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="model-evaluation">
<h1>Model evaluation<a class="headerlink" href="#model-evaluation" title="Permalink to this headline">¶</a></h1>
<p>So far our model evaluation was relatively simplistic, using a split into training and test data as shown in Figure TODO.</p>
<p><img alt=":scale 100%" src="../_images/train_test_split_new.png" /></p>
<p>This is a common scheme, but has several limitations that we’ll address now.
The first issue is model selection. As we discussed before, many models have hyper-parameters that we need to specify, such as <code class="docutils literal notranslate"><span class="pre">k_neighbors</span></code> in the <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code>.
We also might want to choose between different families of models or different algorithms. For simplicity, let’s focus on the case of a single hyper-parameter, but there’s no difference between that and selecting a different algorithm in principle.
A natural way to find a good parameter to use would be to try different values, i.e. fit a model for each value of the hyper-parameter on the training set, and evaluate it on the test set.
It seems that the hyper-parameters performing best on the test set should be a good choice. That’s true, however, with a caveat that’s illustrated in figure TODO.
<img alt=":scale 80%" src="../_images/overfitting_validation_set_3.png" /></p>
<p>todo explain figure more; should say test set in the figure?</p>
<p>The figure assumes we know what the “real” generalization performance of each hyper-parameter setting would be, a quantity that’s only theoretically knowable, but the quantity that we’re actually interested. What we have is the performance on the test set, which can be understood as a noisy version of the true generalization ability of the model.
Now if we pick the best performing hyper-parameters, as indicated by the red dot, it provides a good estimate of what the optimum hyper-parameter value should be, in other words on the x-axis, the red dot is close to the maximum of the idealized generalization performance (the orange line).
However, because we took the maximum of a noisy value, the actual accuracy at this point, i.e. the y axis value of the red dot, is overly optimistic. While the test set accuracy is an unbiased estimate of generalization, i.e. it’s unbiased on average, taking the maximum over all hyper-parameters results in an optimistic bias.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Bias in validation and multiple testing
The issue described above of using the test set both for finding the optimum hyper-parameters and for estimating the accuracy of these parameters can be linked to the concept of multiple testing errors in statistics.
The underlying idea is that if I try out many different things (say many hypothesis or many hyper-parameters), then at some point, I’ll get a ‘good’ result by accident. In a process involving randomness and uncertainty, trying 100 times and getting a good result is not the same as trying once and getting a good result. Luckily the fix for our issue is much easier than the fix for multiple hypothesis testing.</p>
</div>
<p>So while we now know a good setting for our hyper-parmeter, we have no way of estimating how well the model with this hyper-parameter actually performs. If we want to use this model in production, that’s a pretty big issue.
However, it has a pretty simple solution: using an additional hold-out set, as shown in Figure todo.</p>
<div class="section" id="the-threefold-split">
<h2>The Threefold-split<a class="headerlink" href="#the-threefold-split" title="Permalink to this headline">¶</a></h2>
<p><img alt=":scale 100%" src="../_images/train_test_validation_split.png" /></p>
<p>This use of three separate sets, a training set for model building, a validation set for model selection, and a test set for final model evaluation, is probably the most commonly used method for model selection and evaluation, and (apart from some modifications that we’ll discuss below), it’s a best practice that you should use whenever possible. The result is illustrated in Figure todo.
<img alt=":scale 80%" src="../_images/overfitting_validation_set_4.png" /></p>
<p>We now use the validation set to find the value (i.e. x-axis location) of the optimum hyper-parameter, and the test set to find the corresponding y-value. Because we haven’t used the test set for estimating the best hyper-parameter, the test set provides an unbiased estimate of generalization performance when following this method.
One aspect of this is critically important, though: the test set only provides an unbiased result if we are using it once. We apply this selection procedure several times, compare several test-set results, and pick the best-performing model, we end up with the situation we started with, and our estimate will be biased. Therefore, <strong>you should be really careful in when to use the test set</strong>, and ideally set it aside and <strong>use it only once, after you decided on the final model you want to use</strong>.</p>
<p><a class="reference external" href="20http://robotics.stanford.edu/~ang/papers/cv-final.pdf">Preventing Overfitting in cross-validation - Ng 1997</a>]</p>
</div>
<div class="section" id="implementing-the-threefold-split">
<h2>Implementing the threefold split<a class="headerlink" href="#implementing-the-threefold-split" title="Permalink to this headline">¶</a></h2>
<p>Before we go into more detail of tools for doing model-evaluation with scikit-learn, let’s do the procedure <em>by foot</em> first, to clarify the process.
Here is a simple implementation of using the three-fold split strategy for selecting the number of <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> in <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> on the TODO iris dataset.
We split our dataset into three parts, by first talking 25% as the test set, and then taking 25% of the remainder as validation set (so about 19% of the original data).
Then we define the candidate values of the parameter we want to adjust. This often requires knowledge of the algorithm and potentially the dataset.
Here, we’re using a range from 1 to 14 in steps of 2. For <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code>, often uneven numbers are used to avoid ties. The upper range is picked somewhat arbitrarily, but we certainly wouldn’t want to use more than 50, the number of samples in each class in this dataset.</p>
<div class="margin sidebar">
<p class="sidebar-title">Grid Search</p>
<p>The process of trying every value of interest is often called <strong>grid search</strong> in the machine learning context, and basically constitutes a brute-force search for the optimum value.
The name grid search stems from the fact that when searching over multiple hyper-parameters, usually all possible combinations are tried, which corresponds to a grid of hyper-parameters.</p>
</div>
<p>Then, we build a model for each value of <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> in our list, evaluate it on the validation set, and store the result. Finally, we find the value that gave us the best result.</p>
<p>Now we have basically made it to computing the red dot in figure TODO, and we only need to evaluate the purple dot.
The easiest way to do this would be to just predict on the test set, which is certainly possible. In partice, we often prefer rebuilding the model using the training data together with the validation data.
After we found the best value of <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> the validation set is no longer useful, and so we can just use it as more data to build our model with. So we train a new model, using the best value of <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> as determined by the validation set, use as training data both the original training set and the validation set, and evaluate the model on the test set.
This provides us with an unbiased estimate of how well this final model will generalize.</p>
<p>The step of retraining a model using both the training and validation set is optional, and if model training is very expensive, or we can assume the training dataset is large enough for our model, we might skip this step. Here, we have very little data, however, and we should use as much as we can get. We’ll also see more reasons to use this retraining technique later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="c1"># Load the data and split it into three parts</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># first split off test set</span>
<span class="n">X_trainval</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
<span class="c1"># then split of validation set</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>

<span class="c1"># create a list to hold validation scores for each</span>
<span class="c1"># hyper-parameter setting</span>
<span class="n">val_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># Specify a list of values we want to try for n_neighbors</span>
<span class="c1"># This might require some knowledge of the dataset and the model</span>
<span class="c1"># or potentially some trial-and-error</span>
<span class="n">neighbors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># for each potential value of n_neighbors</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">neighbors</span><span class="p">:</span>
    <span class="c1"># build a model</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c1"># score validation set accuracy</span>
    <span class="n">val_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
<span class="c1"># using max tells us the best score</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;best validation score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">val_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.3</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># with argmax we can find the best value of n_neighbors used</span>
<span class="n">best_n_neighbors</span> <span class="o">=</span> <span class="n">neighbors</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">val_scores</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;best n_neighbors:&quot;</span><span class="p">,</span> <span class="n">best_n_neighbors</span><span class="p">)</span>
<span class="c1"># Now,</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">best_n_neighbors</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;test-set score: </span><span class="si">{</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>best validation score: 1.0
best n_neighbors: 3
test-set score: 0.974
</pre></div>
</div>
</div>
</div>
<p>Using a three-fold split into training, validation and test set is essential whenever you’re evaluating more than one model-which is always.
However, there’s another aspect we might want to improve, which is the reliance on the particular splits.
If we change the random state in the splitting (todo?) we might end up with different results. Ideally we want the parameters we pick and our assessment of generalization ability not to be impacted by
the initial splitting of the data. In fact, having large variety between outcomes depending on the data split is probably a bad sign and means our model is not very robust.
In some cases, when the data is particularly small, such as for the iris dataset that we’re using here, this can be hard to overcome, but there’s still one tool
that’s invaluable to have in your toolbox to make model evaluation more robust: cross-validation.</p>
</div>
<div class="section" id="k-fold-cross-validation">
<h2>K-Fold Cross-validation<a class="headerlink" href="#k-fold-cross-validation" title="Permalink to this headline">¶</a></h2>
<p>The idea of cross-validation is to replace the split into training and validation data by multiple different splits.
In general, you can think of cross-validation as a method for splitting data into two parts repeatedly; most commonly
it is applied to the training/validation split, but it could also be applied to splitting off the test data.</p>
<p><img alt=":scale 80%" src="../_images/cross_validation_new.png" /></p>
<div class="margin sidebar">
<p class="sidebar-title">Splitting strategies</p>
<p>There are many other splitting strategies that we’ll discuss in chapter todo. For now, let’s stick to 5-fold or 10-fold cross-validation.</p>
</div>
<p>The most common variant of cross-validation is called k-fold cross validation and illustrated in figure todo.
The first step is to split the data into k disjoint parts, often <span class="math notranslate nohighlight">\(k=5\)</span> or <span class="math notranslate nohighlight">\(k=10\)</span>, which are called folds, usually of about equal size.</p>
<div class="margin sidebar">
<p class="sidebar-title">todo</p>
<p>Max uses the word fold differently</p>
</div>
<p>For each fold, a split of the data is made where this fold serves as validation data (TODO label in image), and the rest of the data serves as training data,
as illustrated in Figure TODO. This means if we use 5-fold cross-validation, we will split our data into five parts,
and then have five different training/validation splits. We build a model for each of these splits of the data, using the training part do build the model,
and the validation part to evaluate it. The outcome is (in the case of five folds) five different accuracy values.
We can then aggregate these and compute a mean or median, or use them to estimate the variance over the different splits.</p>
<p>The benefit over using a single split is that the method is more robust; each data point in the initial dataset is in the validation set exactly once,
while with a single split, only some of the data appears in the validation data. This reduces the variance of the accuracy measurement that is produced.
Using cross-validation can also potentially lead to a more effective use of data: when using five-fold cross-validation, each split can use 80% of the data as training set,
and when using ten-fold cross-validation, it’s even 90%. Still, all of the data is used as validation data in exactly one of the splits.</p>
<p>The main disadvantage of using cross-validation is the computational cost; five-fold cross-validation requires building five models on 80% of the data, so it’s about 5 times slower than using a single split.
The other, potentially less drastic disadvantage is that k-fold cross-validation doesn’t produce a model, it produced k models. If you want to make predictions on new data, it’s somewhat unclear how to do this.
The most obvious choice is retraining on the whole dataset, as we did above, but there are other solutions, which we’ll discuss in chapter todo.</p>
<div class="section" id="cross-validation-with-scikit-learn">
<h3>Cross-validation with scikit-learn<a class="headerlink" href="#cross-validation-with-scikit-learn" title="Permalink to this headline">¶</a></h3>
<p>There are two functions in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> that you can use to perform cross-validation, the <code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code> function and the <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code> function. The <code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code> function has been in scikit-learn for a long time and has a very simple interface, while the <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code> function was added later, is a bit more powerful and provides more options. However, they have a very similar interface, and are easy to use. Let’s apply them to evaluating <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> on the <code class="docutils literal notranslate"><span class="pre">iris</span></code> dataset. As I mentioned above, cross-validation is usually used to replace the inner split into training and validation set, and so we’ll apply it on <code class="docutils literal notranslate"><span class="pre">X_trainval</span></code> and <code class="docutils literal notranslate"><span class="pre">y_trainval</span></code> from TODO.
Both functions, <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code> and <code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code> take the estimator you want to evaluate, so in our case <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code>, the data, for us <code class="docutils literal notranslate"><span class="pre">X_trainval</span></code>, the target <code class="docutils literal notranslate"><span class="pre">y_trainval</span></code> and optionally the number of folds to use, say 5 or 10.
The result of <code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code> is simply an array with one entry per split, corresponding to the accuracy on the particular validation set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">cross_validate</span>
<span class="c1"># cv=10 uses ten-fold cross-validation</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span> <span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">scores</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>array([0.91666667, 0.91666667, 0.90909091, 0.90909091, 1.        ,
       0.90909091, 1.        , 1.        , 1.        , 0.90909091])
</pre></div>
</div>
</div>
</div>
<p>Now let’s try <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code>. The result of <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code> is a dictionary, though it’s easier to display if we convert it to a pandas DataFrame:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="c1"># we could pass cv here as well, but we&#39;re using the default of 5</span>
<span class="n">cv_result</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span> <span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cv_result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.001995</td>
      <td>0.913043</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.001995</td>
      <td>0.913043</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.001995</td>
      <td>0.954545</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.000997</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.000995</td>
      <td>0.954545</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">As</span> <span class="n">you</span> <span class="n">can</span> <span class="n">see</span><span class="p">,</span> <span class="n">the</span> <span class="n">result</span> <span class="n">also</span> <span class="n">contains</span> <span class="n">the</span> <span class="n">time</span> <span class="n">it</span> <span class="n">took</span> <span class="n">to</span> <span class="n">fit</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">and</span> <span class="n">make</span> <span class="n">the</span> <span class="n">predictions</span><span class="o">.</span> <span class="n">We</span> <span class="n">can</span> <span class="n">also</span> <span class="n">compute</span> <span class="n">the</span> <span class="n">training</span> <span class="nb">set</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">which</span> <span class="n">can</span> <span class="n">be</span> <span class="n">helpful</span> <span class="k">for</span> <span class="n">assessing</span> <span class="n">model</span> <span class="n">complexity</span><span class="p">:</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cv_result</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span> <span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cv_result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000191</td>
      <td>0.001017</td>
      <td>0.913043</td>
      <td>0.966292</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000998</td>
      <td>0.000997</td>
      <td>0.913043</td>
      <td>0.955056</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000000</td>
      <td>0.000998</td>
      <td>0.954545</td>
      <td>0.977778</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000000</td>
      <td>0.000997</td>
      <td>1.000000</td>
      <td>0.955556</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000000</td>
      <td>0.000997</td>
      <td>0.954545</td>
      <td>0.966667</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can even return the fitted estimated if we like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cv_result</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span> <span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cv_result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>estimator</th>
      <th>test_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000</td>
      <td>0.001995</td>
      <td>KNeighborsClassifier()</td>
      <td>0.913043</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000000</td>
      <td>0.001019</td>
      <td>KNeighborsClassifier()</td>
      <td>0.913043</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000996</td>
      <td>0.000999</td>
      <td>KNeighborsClassifier()</td>
      <td>0.954545</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000976</td>
      <td>0.000998</td>
      <td>KNeighborsClassifier()</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000000</td>
      <td>0.001994</td>
      <td>KNeighborsClassifier()</td>
      <td>0.954545</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Both, <code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code> and <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code> can compute other evaluation metrics, not only accuracy. We’ll discuss that in detail in chapter TODO.</p>
</div>
</div>
<div class="section" id="grid-search-with-cross-validation">
<h2>Grid-Search with Cross-Validation<a class="headerlink" href="#grid-search-with-cross-validation" title="Permalink to this headline">¶</a></h2>
<p>Now let’s come back to doing model selection as we did in todo, but this time we’re using cross-validation instead of a single split. The overall idea is illustrated in Figure todo.
We still keep the initial split into training and test set, but then, instead of using a single split into training and validation set, we run cross-validation for each parameter setting.
We then record the mean score, where the mean is averaged over the k (here 10) splits in the cross-validation. After we evaluated all candidate parameters, we find the one with the best mean accuracy.
Keep in mind that this score does not correspond to a single model; there is no best model here. What we select is the hyper-parameter that’s best <strong>on average</strong> over the 10 splits.
Then we build a new model, using the hyper-parameters that performed best on average in cross-validation, on the full training dataset (called X_trainval below).
Finally, we evaluate this model on the test dataset.</p>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>todo should it be X_train or X_trainval?</p>
</div>
<p><img alt="" src="../_images/grid_search_cross_validation_new.png" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># We split our data into only two parts</span>
<span class="c1"># Usually we&#39;d call these X_train and X_test</span>
<span class="c1"># We use X_trainval to emphasize that it takes the same role as X_trainval above</span>
<span class="n">X_trainval</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>

<span class="n">cross_val_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># again we define our candiate values</span>
<span class="n">neighbors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">neighbors</span><span class="p">:</span>
    <span class="c1"># we define the model with hyper-parameters</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="c1"># Instead of fitting a single model, we perform cross-validation</span>
    <span class="c1"># fitting 10 models, and returning 10 scores</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="c1"># we record the average score over the 10 folds</span>
    <span class="n">cross_val_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>

<span class="c1"># We can compute the best mean score</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;best cross-validation score: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">cross_val_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.3</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># and corresponding best parameters</span>
<span class="n">best_n_neighbors</span> <span class="o">=</span> <span class="n">neighbors</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cross_val_scores</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;best n_neighbors: </span><span class="si">{</span><span class="n">best_n_neighbors</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">best_n_neighbors</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_trainval</span><span class="p">,</span> <span class="n">y_trainval</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;test-set score: </span><span class="si">{</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>best cross-validation score: 0.974
best n_neighbors: 13
test-set score: 1.000
</pre></div>
</div>
</div>
</div>
<p>Using this combination of brute-force grid-search with cross-validation and a hold-out test set is a gold standard method for model comparison and parameter tuning in machine learning.
The main downside of this procedure it it’s computational cost: the above example tries seven different values for <code class="docutils literal notranslate"><span class="pre">k_neighbors</span></code>, each on ten different splits of the data, plus a final model, meaning we build 71 models in total during this process.
If we had another hyper-parameter, say with 5 different values of interest, we would typically try all possible combinations, leasing to 351 models being build!</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One of my biggest pet-peeves in machine learning is people conflating the use of cross-validation with the use of grid search
These are very distinct concepts and shouldn’t be used interchangeably: cross-validation is a technique to robustly evaluate a particular model on a particular dataset. In essence, it’s a slightly more robust version of just splitting your data into two sets. Grid-search is a technique to tune the hyper-parameters of a particular model by brute-force search. Often each candidate is evaluated using cross-validation, though that’s not necessary. You could also evaluate each candidate with a single split into training and validation set, for example.
So while cross-validation is often used within a grid search, you can also do cross-validation outside of a grid search, and you can do a grid search without using cross-validation.</p>
</div>
<p>The overall process is illustrated in figure TODO. We start off by specifying the hyper-parameters (todo figure parameters) we want to evaluate, which in principle can also include which model we are using.
We split our dataset into training and test set. For each hyper-parameter candidate, we run grid-search on the training set, which yields a score for each split, and a mean score over all the splits.
Using the mean validation scores, we select the best hyper-parameter value, and retrain a model on the whole training data. Then, we evaluate this final model on the test dataset.</p>
<p><img alt=":scale 80%" src="../_images/gridsearch_workflow.png" /></p>
<p>Because this is such a common pattern, there is a helper class for this
in scikit-learn, called GridSearchCV, which does most of these steps
for you.</p>
</div>
<div class="section" id="gridsearchcv">
<h2>GridSearchCV<a class="headerlink" href="#gridsearchcv" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> class is another meta-estimator, that can take any scikit-learn model and tune hyper-parameters for you using cross-validation.
Here is the example from todo using GridSearchCV. The hyper-parameter grid is specified as a dictionary, where the keys are the names of the parameters in the estimator (here the single hyper-parameter <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code>) and the values are all the candidate values of the hyper-parameter that we want to evaluate. The <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> class is then estimated with the estimator we want to tune and the hyper-parameter dictionary. We can also specify how to perform cross-validation (<code class="docutils literal notranslate"><span class="pre">cv=10</span></code>) and we will compute the training scores for evaluation purposes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="c1"># split the data as before</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="c1"># define the parameter grid, key must be exactly the same as name of the parameter</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span>  <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">)}</span>
<span class="c1"># Instantiate GridSearchCV - this sets up the grid-search</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                   <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># Calling fit executes the search and retrains the final model</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># show some information on the search:</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;best mean cross-validation score: </span><span class="si">{</span><span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;best parameters: </span><span class="si">{</span><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># do the final evaluation</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;test-set score: </span><span class="si">{</span><span class="n">grid</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>best mean cross-validation score: 0.9643939393939392
best parameters: {&#39;n_neighbors&#39;: 5}
test-set score: 0.895
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title"></p>
<p>The distinction between the test set score and the <code class="docutils literal notranslate"><span class="pre">best_score_</span></code> attribute is a bit subtle but important: The <code class="docutils literal notranslate"><span class="pre">best_score_</span></code> is an average cross-validation score, and it is optimistic, as illustrated in Figure todo. The test-set score is an unbiased estimate, but only as long as you don’t use it to select a model.</p>
</div>
<p>You can see that the whole process is quite simple using <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>. The great thing about grid search being implemented as a meta-estimator is that after you instantiated the <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> object, you can use it like any other scikit-learn model: it has the standard interface of <code class="docutils literal notranslate"><span class="pre">fit</span></code>, <code class="docutils literal notranslate"><span class="pre">predict</span></code> and <code class="docutils literal notranslate"><span class="pre">score</span></code> which will use the model that was retrained using the best hyper-parameter setting. St this point you don’t need to worry about the fact that it is tuning parameters internally.
However, given that we should only reserve the test-set for a final evaluation, it can be a good idea to look at the search results before using the test set. The attribute <code class="docutils literal notranslate"><span class="pre">best_score_</span></code> contains the mean validation score over all splits of the best hyper-parameter setting, and the best hyper-parameter is stored in <code class="docutils literal notranslate"><span class="pre">best_params_</span></code>.
If the <code class="docutils literal notranslate"><span class="pre">best_score_</span></code> is lower than what you expected or what you need for your application, there is no point in using the test set, as the mean score will be an optimistic estimate, as discussed above todo.
One aspect you might want to look into for the best parameter is whether it is on the boundary of the search space you specified. If the best value found is at the edge of the values I tried, maybe the true optimum is outside the range and I should extend the range.
We can look into the results in even more detail using the <code class="docutils literal notranslate"><span class="pre">cv_results_</span></code> attribute, which is a dictionary with a structure somewhat similar to the result of <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code>. However, in <code class="docutils literal notranslate"><span class="pre">cv_results_</span></code>, each row corresponds to a hyper-parameter setting, and contains the scores for all the splits:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># cv_results_ is a dict, but DataFrames are nicer</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>
<span class="n">results</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>Index([&#39;mean_fit_time&#39;, &#39;std_fit_time&#39;, &#39;mean_score_time&#39;, &#39;std_score_time&#39;,
       &#39;param_n_neighbors&#39;, &#39;params&#39;, &#39;split0_test_score&#39;, &#39;split1_test_score&#39;,
       &#39;split2_test_score&#39;, &#39;split3_test_score&#39;, &#39;split4_test_score&#39;,
       &#39;split5_test_score&#39;, &#39;split6_test_score&#39;, &#39;split7_test_score&#39;,
       &#39;split8_test_score&#39;, &#39;split9_test_score&#39;, &#39;mean_test_score&#39;,
       &#39;std_test_score&#39;, &#39;rank_test_score&#39;, &#39;split0_train_score&#39;,
       &#39;split1_train_score&#39;, &#39;split2_train_score&#39;, &#39;split3_train_score&#39;,
       &#39;split4_train_score&#39;, &#39;split5_train_score&#39;, &#39;split6_train_score&#39;,
       &#39;split7_train_score&#39;, &#39;split8_train_score&#39;, &#39;split9_train_score&#39;,
       &#39;mean_train_score&#39;, &#39;std_train_score&#39;],
      dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>0     {&#39;n_neighbors&#39;: 1}
1     {&#39;n_neighbors&#39;: 3}
2     {&#39;n_neighbors&#39;: 5}
3     {&#39;n_neighbors&#39;: 7}
4     {&#39;n_neighbors&#39;: 9}
5    {&#39;n_neighbors&#39;: 11}
6    {&#39;n_neighbors&#39;: 13}
Name: params, dtype: object
</pre></div>
</div>
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title">TODO</p>
<p>Should we use cv=5 to print less ehere?</p>
</div>
<p>Each column has the mean accuracy scores for the test set and training set (as we specified <code class="docutils literal notranslate"><span class="pre">return_train_score=True</span></code>), as well as the individual score for each split.
If we have a single hyper-parameter, the result can be nicely visualized using a line-plot. One way is to show the mean and standard deviation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="c1"># we use std for error bars.</span>
<span class="c1"># Some people might perfer the 95% confidence interval or the standard error of the mean</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;param_n_neighbors&#39;</span><span class="p">,</span> <span class="s1">&#39;mean_test_score&#39;</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="s1">&#39;std_test_score&#39;</span><span class="p">)</span>
<span class="c1"># reuse the axes from the first plot (ax=ax)</span>
<span class="n">results</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;param_n_neighbors&#39;</span><span class="p">,</span> <span class="s1">&#39;mean_train_score&#39;</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="s1">&#39;std_train_score&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x1e6d7204988&gt;
</pre></div>
</div>
<img alt="../_images/10-model-validation-and-tuning_30_1.png" src="../_images/10-model-validation-and-tuning_30_1.png" />
</div>
</div>
<p>As you can see, there is quite a wide standard deviation on the validation sets in the different splits, which is somewhat unsuprising given the small size of the dataset.
We can also visualize each split as an individual curve, which might make trends easier to spot (though you shouldn’t read too much into these results - this is the iris dataset after all):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># list all the columns for train scores and test scores respectively</span>
<span class="c1"># easier to just generate them instead of trying to slice them or match them</span>
<span class="n">test_score_cols</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;split</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">_test_score&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
<span class="n">train_score_cols</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;split</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">_train_score&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;param_n_neighbors&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">train_score_cols</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">3</span><span class="p">)</span> 
<span class="n">results</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;param_n_neighbors&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">test_score_cols</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span> 

<span class="c1"># Get lines for the legend. This is a bit hacky</span>
<span class="c1"># but better than having 20 entries in the legend</span>
<span class="n">train_line</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_children</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">test_line</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_children</span><span class="p">()[</span><span class="mi">10</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="n">train_line</span><span class="p">,</span> <span class="n">test_line</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;train set&quot;</span><span class="p">,</span> <span class="s2">&quot;test set&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x1e6dad0ccc8&gt;
</pre></div>
</div>
<img alt="../_images/10-model-validation-and-tuning_32_1.png" src="../_images/10-model-validation-and-tuning_32_1.png" />
</div>
</div>
<div class="margin sidebar">
<p class="sidebar-title">CAVEAT TODO Move to model evaluation chapter</p>
<p>The result of</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<p>and</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<p>is always identical.
However, the result of</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<p>and</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<p>is not, as the first uses the metric used in GridSearchCV, while the second uses the score method of the best estimator (i.e. accuracy or R^2).</p>
</div>
<div class="section" id="the-best-estimator">
<h3>The <code class="docutils literal notranslate"><span class="pre">best_estimator_</span></code><a class="headerlink" href="#the-best-estimator" title="Permalink to this headline">¶</a></h3>
<p>There is another attribute of <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> that is commonly used, though maybe used too much, which is called <code class="docutils literal notranslate"><span class="pre">best_estimator_</span></code>. This attribute stores the model that was refit on the whole training data, and is the model that is used when calling <code class="docutils literal notranslate"><span class="pre">predict</span></code> and <code class="docutils literal notranslate"><span class="pre">score</span></code>. For most use-cases, you can just call <code class="docutils literal notranslate"><span class="pre">predict</span></code> and <code class="docutils literal notranslate"><span class="pre">score</span></code> on the <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> object directly, and there is no need to use the <code class="docutils literal notranslate"><span class="pre">best_estimator_</span></code> attribute. However, if you want to access specific attributes of the models, say coefficients in a linear model, you can do that using <code class="docutils literal notranslate"><span class="pre">best_estimator_</span></code>.</p>
</div>
</div>
<div class="section" id="nested-cross-validation">
<h2>Nested Cross-Validation<a class="headerlink" href="#nested-cross-validation" title="Permalink to this headline">¶</a></h2>
<p>As mentioned above, it’s most common to use cross-validation for the training/validation part of the training/validation/test split.
However, we can also use it for both splits, which results in what is known as <em>nested cross-validation</em>.
Nested cross-validation is easy to implement with scikit-learn, but not commonly used, for three reasons:</p>
<ul class="simple">
<li><p>It’s computationally expensive as it adds another for-loop to the grid-search process.</p></li>
<li><p>It doesn’t result in a single model, as cross-validation doesn’t produce a single model, and so it’s hard to productionize.</p></li>
<li><p>It’s conceptually somewhat hard to understand what it computes.</p></li>
</ul>
<p>The first of these should be quite clear: imagine we add an outer cross-validation loop to the example of todo. This would be expressed with scikit-learn like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># note that we don&#39;t call train_test_split</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span>  <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">)}</span>
<span class="c1"># instantiate grid search</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                   <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># perform cross-validation on the grid-search estimator</span>
<span class="c1"># where each individual fit will internally perform cross-validation</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>estimator</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.262298</td>
      <td>0.000998</td>
      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>
      <td>0.966667</td>
      <td>0.975000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.255318</td>
      <td>0.001002</td>
      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>
      <td>1.000000</td>
      <td>0.966667</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.256428</td>
      <td>0.000997</td>
      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>
      <td>0.933333</td>
      <td>0.966667</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.254881</td>
      <td>0.000993</td>
      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>
      <td>0.966667</td>
      <td>0.983333</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.250332</td>
      <td>0.000997</td>
      <td>GridSearchCV(cv=10, estimator=KNeighborsClassi...</td>
      <td>1.000000</td>
      <td>0.966667</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Before, we had 7 hyper-parameter settings, and 10 cross-validation folds, plus a final evaluation, leading to 71 models being build.
If we use five-fold cross-validation on this whole process, this will mean we’re building 355 models, which will clearly take much longer.</p>
<p>Also, the outcome is now five different scores, one for each split. However, these don’t correspond to a particular model, as within this outter loop, grid-search led to different optimum parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">best_params_</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;estimator&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>[{&#39;n_neighbors&#39;: 9},
 {&#39;n_neighbors&#39;: 7},
 {&#39;n_neighbors&#39;: 3},
 {&#39;n_neighbors&#39;: 7},
 {&#39;n_neighbors&#39;: 11}]
</pre></div>
</div>
</div>
</div>
<p>So we don’t end up with a model we can immediately use on new data. One way to get around this is to make predictions from each individual GridSearchCV model, and average the results TODO is this good here?, but that results in a somewhat complicated and hard-to-understand model.</p>
<p>Finally, it’s a bit hard to grasp what the result of the grid-search represents. It is an estimate of how well a given model generalizes if the hyper-parameters are adjusted with grid search. This can be interesting when comparing algorithms while taking into account hyper-parameter tuning: “Is algorithm A better than algorithm B on this dataset if both are properly tuned?”. It can also be useful for seeing if there is signal in the data at all in settings where the dataset is very small todo cite Gael?</p>
<p>However, for most every-day data science use-cases, the workflow illustrated in figures todo and todo is likely better suited for your needs.</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>Selecting hyper-parameters and selecting between different models is a cruicial component of building successful machine learning models. However, comparing models naively on the test set can lead to overly optimistic expectations about the generalization ability of a model. Therefore, it’s critical to use a three-step process of training, validation and testing on separate subsets of the data. This can be achieved either by splitting the data into disjoint training, validation and test set, which is quite efficient but dependant on the particular split of the data, or by using cross-validation instead of a single split for the training/validation split.
Cross-validation is more robust but also more time-consuming than using single splits.
Cross-validation is often combined with the brute-force grid-search approach to tuning hyper-parmeters, which can be easily implemented in scikit-learn using <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>.</p>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="08-imputation.html" title="previous page">Missing Values</a>
    <a class='right-next' id="next-link" href="12-pipelines-gridsearch.html" title="next page">More on Pipelines</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Andreas C. Müller<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>