{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Pipelines\n",
    "We already saw how pipelines can make our live easier in chapter todo. However, when using model evaluation tools such as cross_validate and GridSearchCV, using pipelines becomes essential for obtaining valid results.\n",
    "Also, the use of pipelines in GridSearchCV allows for a variety of powerful use-cases. We'll explore both of these in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data leakage: a common error\n",
    "Let's start with an error that's commonly made when using cross-validation, which is to leak information from the validation parts of the data.\n",
    "This is an error that has been made, not only countless times by beginning data scientists, but in several published scientific research articles.\n",
    "When doing any preprocessing, it is essential that the preprocessing happens within cross-validation, not outside of it.\n",
    "While we haven't seen the details of feature selection yet, it provides and excellent example, and so we'll quickly go over it.\n",
    "\n",
    "### Automatic univariate feature selection\n",
    "When working with high dimensional datasets, it can be beneficial to work with only a subset of the features. This will reduce the computational burden, increase interpretability, and in some cases can even improve generalization performance.\n",
    "There are several methods for automating this process, which we will discuss in depth in chapter todo. One of the simplest methods of automatic feature selection is using univariate statistics to rank features.\n",
    "Univariate means we are looking only at one feature at a time, and evaluate its relationship with the target, often with a simple statistical measure such as an F test or t-test.\n",
    "We can then rank all the features by the strength of their response (or alternatively by how significant their association with the target was) and select the ones deemed most important.\n",
    "A version of this is implemented in the ``SelectPercentile`` transformer in scikit-learn, which allows you to keep a fixed percentage of the existing features.\n",
    "This can be a quick and easy way to subselect features from a very wide dataset and is commonly used. Here is a quick example on the breast cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# load the dataset and split it into training and test set\n",
    "X, y  = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a standard pipeline out of scaler and classifier\n",
    "pipe_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "# Fit and evaluate as a baseline\n",
    "pipe_knn.fit(X_train, y_train)\n",
    "pipe_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pipeline subselecting 20% of the features according to univariate statistics\n",
    "# Order of scaling and selection does not matter in this case\n",
    "pipe_select = make_pipeline(StandardScaler(), SelectPercentile(percentile=20), KNeighborsClassifier())\n",
    "# Fit the pipeline\n",
    "pipe_select.fit(X_train, y_train)\n",
    "# slice off the classifier, look at shape of transformed data:\n",
    "pipe_select[:-1].transform(X_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, of the 30 original features, ``SelectPercentile`` only kept 20%, meaning 6. Now let's evaluate the whole pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_select.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance using only 20% of the features is actually identical to the performance when using all the features, but might be much more interpretable.\n",
    "We can see which features were selected by TODO.\n",
    "\n",
    "Now, that we have familiarized ourselves with how SelectPercentile works (at least in general terms), let's look at the example mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO hide\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(42)\n",
    "X = rng.normal(size=(100, 10000))\n",
    "y = rng.normal(size=(100,)) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say someone gave you a binary classification dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10000) (100,)\n",
      "[53 47]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)\n",
    "# count appearances of 0 and 1 in y\n",
    "print(np.bincount(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very wide, meaning it has many features, compared to the number of samples. This is quite common in sensor networks or in biomedical data for example.\n",
    "Given the small size of the dataset, we might want to use cross-validation to assess performance, instead of using a single train-test split.\n",
    "One might start like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 500)\n"
     ]
    }
   ],
   "source": [
    "# select most informative 5% of features\n",
    "select = SelectPercentile(percentile=5)\n",
    "select.fit(X, y)\n",
    "X_selected = select.transform(X)\n",
    "print(X_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset seems much more managable at 500 features (which are arguably still a lot), and we can evaluate our model with ``cross_val_score``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# run cross-validation with the subselected features\n",
    "cross_val_score(KNeighborsClassifier(), X_selected, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "If a model looks too good to be true, an experienced data scientist ususally looks for the mistake. Often it's a case of information leakage,\n",
    "so if you ever observe very high accuracy, you might do well to be skeptical at first.\n",
    "```\n",
    "\n",
    "It looks like it's our lucky day: we created a model that classifies our dataset perfectly across all folds. From this evaluation, we might be quite certain we found a good model.\n",
    "However, we made a mistake: we applied the feature selection procedure outside of the cross-validation. We should apply it inside the cross-validation instead.\n",
    "In scikit-learn, we can easily do that using a pipeline (as we did above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45, 0.5 , 0.5 , 0.5 , 0.7 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(SelectPercentile(percentile=5), KNeighborsClassifier())\n",
    "# run cross-validation on the original dataset using the pipeline\n",
    "cross_val_score(pipe, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the proper evaluation technique, our results change drastically: our model is around chance performance for a balanced dataset as this, in other words, we might conclude that the model didn't learn anything.\n",
    "Where does this dramatic difference come from? When we called ``fit`` on ``SelectPercentile`` before the cross-validation, it had access to the full dataset, which includes the training and test parts for each of the splits. This means it could extract information from all parts of the data, even those that we meant to use as validation set during cross-validation. This is a classical example of information leakage, and a good reason to always use pipelines!\n",
    "\n",
    "To make the difference in the computation a bit more apparent, I wrote down a more explicit version of the same computation, not using ``cross_val_score`` or ``Pipeline`` (we're using ``KFold`` here which is a way to get the indices to perform K-fold cross-validation, we'll see this in more detail in TODO):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{list-table}\n",
    "---\n",
    "header-rows: 1\n",
    "---\n",
    "* - preprocessing before cross validation\n",
    "  - preprocessing within cross validation\n",
    "* - ```python\n",
    "    # BAD!\n",
    "    select = SelectPercentile(percentile=5)\n",
    "    select.fit(X, y)  # includes the cv test parts!\n",
    "    X_sel = select.transform(X)\n",
    "    scores = []\n",
    "    for train, test in KFold().split(X, y):\n",
    "        knn = KNeighborsClassifier().fit(X_sel[train], y[train])\n",
    "        score = knn.score(X_sel[test], y[test])\n",
    "        scores.append(score)\n",
    "    ```\n",
    "  - ```python\n",
    "    # GOOD!\n",
    "    scores = []\n",
    "    select = SelectPercentile(percentile=5)\n",
    "    for train, test in KFold().split(X, y):\n",
    "        select.fit(X[train], y[train])\n",
    "        X_sel_train = select.transform(X[train])\n",
    "        knn = KNeighborsClassifier().fit(X_sel_train, y[train])\n",
    "        X_sel_test = select.transform(X[test])\n",
    "        score = knn.score(X_sel_test, y[test])\n",
    "        scores.append(score)\n",
    "    ```\n",
    "* - equivalent to:\n",
    "    ```python\n",
    "    select = SelectPercentile(percentile=5)\n",
    "    X_selected = select.fit_transform(X)\n",
    "    scores = cross_val_score(KNeighborsClassifier(), X, y)\n",
    "    ```\n",
    "  - ```python\n",
    "    pipe = make_pipeline(SelectPercentile(percentile=5),\n",
    "                         KNeighborsClassifier()\n",
    "    scores = cross_val_score(pipe, X, y)\n",
    "    ```\n",
    "    \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline and GridSearchCV\n",
    ".small-padding-top[\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor())\n",
    "param_grid = {'kneighborsregressor__n_neighbors': range(1, 10)}\n",
    "grid = GridSearchCV(knn_pipe, param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "```\n",
    "{'kneighborsregressor__n_neighbors': 7}\n",
    "0.60\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "These names are important for using pipelines with\n",
    "gridsearch. Recall that for using GridSearchCV you need to\n",
    "specify a parameter grid as a dictionary, where the keys are\n",
    "the parameter names. If you are using a pipeline inside\n",
    "GridSearchCV, you need to specify not only the parameter\n",
    "name, but also the step name – because multiple steps could\n",
    "have a parameter with the same name.\n",
    "\n",
    "The way to do this is to use the stepname, then two\n",
    "underscores, and then the parameter name, as the key for the\n",
    "param_grid dictionary.\n",
    "\n",
    "You can see that the best_params_ will have this same\n",
    "format.\n",
    "\n",
    "This way you can tune the parameters of all steps in a\n",
    "pipeline at once!\n",
    "\n",
    "And you don’t have to worry about leaking information, since\n",
    "all transformations are contained in the pipeline.\n",
    "\n",
    "You should always use pipelines for preprocessing. Not only\n",
    "does it make your code shorter, it also makes it less likely\n",
    "that you have bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going wild with Pipelines\n",
    "```python\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    diabetes.data, diabetes.target, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(),\n",
    "    Ridge())\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                    n_jobs=-1, return_train_score=True)\n",
    "grid.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going wilder with Pipelines\n",
    "```python\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('regressor', Ridge())])\n",
    "\n",
    "param_grid = {'scaler': [StandardScaler(), MinMaxScaler(),\n",
    "                         'passthrough'],\n",
    "              'regressor': [Ridge(), Lasso()],\n",
    "              'regressor__alpha': np.logspace(-3, 3, 7)}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going wildest with Pipelines\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('regressor', Ridge())])\n",
    "\n",
    "# check out searchgrid for more convenience\n",
    "param_grid = [{'regressor': [DecisionTreeRegressor()],\n",
    "               'regressor__max_depth': [2, 3, 4],\n",
    "               'scaler': ['passthrough']},\n",
    "              {'regressor': [Ridge()],\n",
    "               'regressor__alpha': [0.1, 1],\n",
    "               'scaler': [StandardScaler(), MinMaxScaler(),\n",
    "                          'passthrough']}\n",
    "             ]\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sklearn\n",
    "sklearn.set_config(print_changed_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.972027972027972"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# load and split the data\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "# compute minimum and maximum on the training data\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "# rescale training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "svm = SVC()\n",
    "# learn an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "# scale test data and score the scaled data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "svm.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pipelines in Grid-searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross-validation accuracy: 0.9812311901504789\n",
      "test set score:  0.972027972027972\n",
      "best parameters:  {'svm__C': 1, 'svm__gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best cross-validation accuracy:\", grid.best_score_)\n",
    "print(\"test set score: \", grid.score(X_test, y_test))\n",
    "print(\"best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not using Pipelines vs feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(seed=0)\n",
    "X = rnd.normal(size=(100, 10000))\n",
    "y = rnd.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "\n",
    "select = SelectPercentile(score_func=f_regression,\n",
    "                          percentile=5)\n",
    "select.fit(X, y)\n",
    "X_selected = select.transform(X)\n",
    "print(X_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9057953065239822"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "np.mean(cross_val_score(Ridge(), X_selected, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2465542238495281"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([(\"select\", SelectPercentile(score_func=f_regression, percentile=5)),\n",
    "                 (\"ridge\", Ridge())])\n",
    "np.mean(cross_val_score(pipe, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The General Pipeline Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    X_transformed = X\n",
    "    for step in self.steps[:-1]:\n",
    "        # iterate over all but the final step\n",
    "        # fit and transform the data\n",
    "        X_transformed = step[1].fit_transform(X_transformed, y)\n",
    "    # fit the last step\n",
    "    self.steps[-1][1].fit(X_transformed, y)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    X_transformed = X\n",
    "    for step in self.steps[:-1]:\n",
    "        # iterate over all but the final step\n",
    "        # transform the data\n",
    "        X_transformed = step[1].transform(X_transformed)\n",
    "    # predict using the last step\n",
    "    return self.steps[-1][1].predict(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convenient Pipeline creation with ``make_pipeline``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "# standard syntax\n",
    "pipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
    "# abbreviated syntax\n",
    "pipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('minmaxscaler', MinMaxScaler()), ('svc', SVC(C=100))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_short.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('standardscaler-1', StandardScaler()),\n",
       " ('pca', PCA(n_components=2)),\n",
       " ('standardscaler-2', StandardScaler())]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), PCA(n_components=2),\n",
    "                     StandardScaler())\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing step attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 30)\n"
     ]
    }
   ],
   "source": [
    "# fit the pipeline defined above to the cancer dataset\n",
    "pipe.fit(cancer.data)\n",
    "# extract the first two principal components from the \"pca\" step\n",
    "components = pipe.named_steps.pca.components_\n",
    "print(components.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler-1', StandardScaler()),\n",
       "                ('pca', PCA(n_components=2))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing attributes in grid-searched pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('logisticregression',\n",
       "                                        LogisticRegression(max_iter=1000))]),\n",
       "             param_grid={'logisticregression__C': [0.01, 0.1, 1, 10, 100]})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=4)\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('logisticregression', LogisticRegression(C=1, max_iter=1000))])\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, max_iter=1000)\n",
      "LogisticRegression(C=1, max_iter=1000)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_.named_steps.logisticregression)\n",
    "print(grid.best_estimator_['logisticregression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43570655 -0.34266946 -0.40809443 -0.5344574  -0.14971847  0.61034122\n",
      "  -0.72634347 -0.78538827  0.03886087  0.27497198 -1.29780109  0.04926005\n",
      "  -0.67336941 -0.93447426 -0.13939555  0.45032641 -0.13009864 -0.10144273\n",
      "   0.43432027  0.71596578 -1.09068862 -1.09463976 -0.85183755 -1.06406198\n",
      "  -0.74316099  0.07252425 -0.82323903 -0.65321239 -0.64379499 -0.42026013]]\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_.named_steps.logisticregression.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43570655 -0.34266946 -0.40809443 -0.5344574  -0.14971847  0.61034122\n",
      "  -0.72634347 -0.78538827  0.03886087  0.27497198 -1.29780109  0.04926005\n",
      "  -0.67336941 -0.93447426 -0.13939555  0.45032641 -0.13009864 -0.10144273\n",
      "   0.43432027  0.71596578 -1.09068862 -1.09463976 -0.85183755 -1.06406198\n",
      "  -0.74316099  0.07252425 -0.82323903 -0.65321239 -0.64379499 -0.42026013]]\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_['logisticregression'].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-searching preprocessing steps and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    diabetes.data, diabetes.target, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(),\n",
    "    Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('polynomialfeatures',\n",
       "                                        PolynomialFeatures()),\n",
       "                                       ('ridge', Ridge())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'polynomialfeatures__degree': [1, 2, 3],\n",
       "                         'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                    n_jobs=-1, return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_polynomialfeatures__degree</th>\n",
       "      <th>param_ridge__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009374</td>\n",
       "      <td>2.238970e-03</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'polynomialfeatures__degree': 1, 'ridge__alph...</td>\n",
       "      <td>0.433550</td>\n",
       "      <td>0.486062</td>\n",
       "      <td>0.514033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512238</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>5</td>\n",
       "      <td>0.579986</td>\n",
       "      <td>0.565378</td>\n",
       "      <td>0.556566</td>\n",
       "      <td>0.547340</td>\n",
       "      <td>0.547173</td>\n",
       "      <td>0.559289</td>\n",
       "      <td>0.012349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007978</td>\n",
       "      <td>8.919101e-04</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'polynomialfeatures__degree': 1, 'ridge__alph...</td>\n",
       "      <td>0.433578</td>\n",
       "      <td>0.486053</td>\n",
       "      <td>0.514045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512254</td>\n",
       "      <td>0.049385</td>\n",
       "      <td>4</td>\n",
       "      <td>0.579986</td>\n",
       "      <td>0.565378</td>\n",
       "      <td>0.556566</td>\n",
       "      <td>0.547340</td>\n",
       "      <td>0.547173</td>\n",
       "      <td>0.559289</td>\n",
       "      <td>0.012349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012566</td>\n",
       "      <td>6.752189e-03</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'polynomialfeatures__degree': 1, 'ridge__alph...</td>\n",
       "      <td>0.433841</td>\n",
       "      <td>0.485966</td>\n",
       "      <td>0.514161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512406</td>\n",
       "      <td>0.049413</td>\n",
       "      <td>3</td>\n",
       "      <td>0.579982</td>\n",
       "      <td>0.565377</td>\n",
       "      <td>0.556563</td>\n",
       "      <td>0.547339</td>\n",
       "      <td>0.547166</td>\n",
       "      <td>0.559285</td>\n",
       "      <td>0.012349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003186</td>\n",
       "      <td>1.924576e-03</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'polynomialfeatures__degree': 1, 'ridge__alph...</td>\n",
       "      <td>0.435508</td>\n",
       "      <td>0.485451</td>\n",
       "      <td>0.514905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513376</td>\n",
       "      <td>0.049622</td>\n",
       "      <td>2</td>\n",
       "      <td>0.579731</td>\n",
       "      <td>0.565309</td>\n",
       "      <td>0.556384</td>\n",
       "      <td>0.547307</td>\n",
       "      <td>0.546763</td>\n",
       "      <td>0.559099</td>\n",
       "      <td>0.012352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001994</td>\n",
       "      <td>7.599534e-07</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'polynomialfeatures__degree': 1, 'ridge__alph...</td>\n",
       "      <td>0.441858</td>\n",
       "      <td>0.487365</td>\n",
       "      <td>0.516466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515968</td>\n",
       "      <td>0.048215</td>\n",
       "      <td>1</td>\n",
       "      <td>0.577828</td>\n",
       "      <td>0.564338</td>\n",
       "      <td>0.554608</td>\n",
       "      <td>0.546770</td>\n",
       "      <td>0.543759</td>\n",
       "      <td>0.557461</td>\n",
       "      <td>0.012428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.009374  2.238970e-03         0.000798        0.000399   \n",
       "1       0.007978  8.919101e-04         0.001197        0.000399   \n",
       "2       0.012566  6.752189e-03         0.001595        0.000489   \n",
       "3       0.003186  1.924576e-03         0.000598        0.000489   \n",
       "4       0.001994  7.599534e-07         0.000598        0.000488   \n",
       "\n",
       "  param_polynomialfeatures__degree param_ridge__alpha  \\\n",
       "0                                1              0.001   \n",
       "1                                1               0.01   \n",
       "2                                1                0.1   \n",
       "3                                1                  1   \n",
       "4                                1                 10   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'polynomialfeatures__degree': 1, 'ridge__alph...           0.433550   \n",
       "1  {'polynomialfeatures__degree': 1, 'ridge__alph...           0.433578   \n",
       "2  {'polynomialfeatures__degree': 1, 'ridge__alph...           0.433841   \n",
       "3  {'polynomialfeatures__degree': 1, 'ridge__alph...           0.435508   \n",
       "4  {'polynomialfeatures__degree': 1, 'ridge__alph...           0.441858   \n",
       "\n",
       "   split1_test_score  split2_test_score  ...  mean_test_score  std_test_score  \\\n",
       "0           0.486062           0.514033  ...         0.512238        0.049383   \n",
       "1           0.486053           0.514045  ...         0.512254        0.049385   \n",
       "2           0.485966           0.514161  ...         0.512406        0.049413   \n",
       "3           0.485451           0.514905  ...         0.513376        0.049622   \n",
       "4           0.487365           0.516466  ...         0.515968        0.048215   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                5            0.579986            0.565378   \n",
       "1                4            0.579986            0.565378   \n",
       "2                3            0.579982            0.565377   \n",
       "3                2            0.579731            0.565309   \n",
       "4                1            0.577828            0.564338   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            0.556566            0.547340            0.547173   \n",
       "1            0.556566            0.547340            0.547173   \n",
       "2            0.556563            0.547339            0.547166   \n",
       "3            0.556384            0.547307            0.546763   \n",
       "4            0.554608            0.546770            0.543759   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0          0.559289         0.012349  \n",
       "1          0.559289         0.012349  \n",
       "2          0.559285         0.012349  \n",
       "3          0.559099         0.012352  \n",
       "4          0.557461         0.012428  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame(grid.cv_results_)\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.pivot_table(res, index=['param_polynomialfeatures__degree', 'param_ridge__alpha'],\n",
    "               values=['mean_train_score', 'mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>param_ridge__alpha</th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.010</th>\n",
       "      <th>0.100</th>\n",
       "      <th>1.000</th>\n",
       "      <th>10.000</th>\n",
       "      <th>100.000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_polynomialfeatures__degree</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.559289</td>\n",
       "      <td>0.559289</td>\n",
       "      <td>0.559285</td>\n",
       "      <td>0.559099</td>\n",
       "      <td>0.557461</td>\n",
       "      <td>0.536270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666707</td>\n",
       "      <td>0.666268</td>\n",
       "      <td>0.664697</td>\n",
       "      <td>0.661747</td>\n",
       "      <td>0.650237</td>\n",
       "      <td>0.605903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.974622</td>\n",
       "      <td>0.958083</td>\n",
       "      <td>0.927772</td>\n",
       "      <td>0.883468</td>\n",
       "      <td>0.823504</td>\n",
       "      <td>0.719794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "param_ridge__alpha                 0.001     0.010     0.100     1.000    \\\n",
       "param_polynomialfeatures__degree                                           \n",
       "1                                 0.559289  0.559289  0.559285  0.559099   \n",
       "2                                 0.666707  0.666268  0.664697  0.661747   \n",
       "3                                 0.974622  0.958083  0.927772  0.883468   \n",
       "\n",
       "param_ridge__alpha                 10.000    100.000  \n",
       "param_polynomialfeatures__degree                      \n",
       "1                                 0.557461  0.536270  \n",
       "2                                 0.650237  0.605903  \n",
       "3                                 0.823504  0.719794  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['mean_train_score'].unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>param_ridge__alpha</th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.010</th>\n",
       "      <th>0.100</th>\n",
       "      <th>1.000</th>\n",
       "      <th>10.000</th>\n",
       "      <th>100.000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_polynomialfeatures__degree</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512238</td>\n",
       "      <td>0.512254</td>\n",
       "      <td>0.512406</td>\n",
       "      <td>0.513376</td>\n",
       "      <td>0.515968</td>\n",
       "      <td>0.507934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.374003</td>\n",
       "      <td>0.379897</td>\n",
       "      <td>0.392917</td>\n",
       "      <td>0.428173</td>\n",
       "      <td>0.471838</td>\n",
       "      <td>0.492605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-51.023088</td>\n",
       "      <td>-16.452330</td>\n",
       "      <td>-5.166146</td>\n",
       "      <td>-1.194386</td>\n",
       "      <td>0.060621</td>\n",
       "      <td>0.315993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "param_ridge__alpha                  0.001      0.010     0.100     1.000    \\\n",
       "param_polynomialfeatures__degree                                             \n",
       "1                                  0.512238   0.512254  0.512406  0.513376   \n",
       "2                                  0.374003   0.379897  0.392917  0.428173   \n",
       "3                                -51.023088 -16.452330 -5.166146 -1.194386   \n",
       "\n",
       "param_ridge__alpha                 10.000    100.000  \n",
       "param_polynomialfeatures__degree                      \n",
       "1                                 0.515968  0.507934  \n",
       "2                                 0.471838  0.492605  \n",
       "3                                 0.060621  0.315993  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['mean_test_score'].unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'polynomialfeatures__degree': 1, 'ridge__alpha': 10}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_['polynomialfeatures'].get_feature_names(diabetes.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3580342762914789"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3556784125736949"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()), ('regressor', Ridge())])\n",
    "\n",
    "param_grid = {'scaler': [StandardScaler(), MinMaxScaler(), 'passthrough'],\n",
    "              'regressor': [Ridge(), Lasso()],\n",
    "              'regressor__alpha': np.logspace(-3, 3, 7)}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid,\n",
    "                    cv=RepeatedKFold(n_splits=10, n_repeats=10))\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5041191056200575"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regressor': Lasso(), 'regressor__alpha': 1.0, 'scaler': StandardScaler()}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "param_grid = [{'regressor': [DecisionTreeRegressor()],\n",
    "               'regressor__max_depth': [2, 3, 4]},\n",
    "              {'regressor': [Ridge()],\n",
    "               'regressor__alpha': [0.1, 1]}\n",
    "             ]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sklearndev]",
   "language": "python",
   "name": "conda-env-sklearndev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
