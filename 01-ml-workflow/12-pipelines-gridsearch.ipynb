{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Pipelines\n",
    "We already saw how pipelines can make our live easier in chapter todo. However, when using model evaluation tools such as cross_validate and GridSearchCV, using pipelines becomes essential for obtaining valid results.\n",
    "Also, the use of pipelines in GridSearchCV allows for a variety of powerful use-cases. We'll explore both of these in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data leakage: a common error\n",
    "Let's start with an error that's commonly made when using cross-validation, which is to leak information from the validation parts of the data.\n",
    "This is an error that has been made, not only countless times by beginning data scientists, but in several published scientific research articles.\n",
    "When doing any preprocessing, it is essential that the preprocessing happens within cross-validation, not outside of it.\n",
    "While we haven't seen the details of feature selection yet, it provides and excellent example, and so we'll quickly go over it.\n",
    "\n",
    "### Automatic univariate feature selection\n",
    "When working with high dimensional datasets, it can be beneficial to work with only a subset of the features. This will reduce the computational burden, increase interpretability, and in some cases can even improve generalization performance.\n",
    "There are several methods for automating this process, which we will discuss in depth in chapter todo. One of the simplest methods of automatic feature selection is using univariate statistics to rank features.\n",
    "Univariate means we are looking only at one feature at a time, and evaluate its relationship with the target, often with a simple statistical measure such as an F test or t-test.\n",
    "We can then rank all the features by the strength of their response (or alternatively by how significant their association with the target was) and select the ones deemed most important.\n",
    "A version of this is implemented in the ``SelectPercentile`` transformer in scikit-learn, which allows you to keep a fixed percentage of the existing features.\n",
    "This can be a quick and easy way to subselect features from a very wide dataset and is commonly used. Here is a quick example on the breast cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# load the dataset and split it into training and test set\n",
    "X, y  = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a standard pipeline out of scaler and classifier\n",
    "pipe_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "# Fit and evaluate as a baseline\n",
    "pipe_knn.fit(X_train, y_train)\n",
    "pipe_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pipeline subselecting 20% of the features according to univariate statistics\n",
    "# Order of scaling and selection does not matter in this case\n",
    "pipe_select = make_pipeline(StandardScaler(), SelectPercentile(percentile=20), KNeighborsClassifier())\n",
    "# Fit the pipeline\n",
    "pipe_select.fit(X_train, y_train)\n",
    "# slice off the classifier, look at shape of transformed data:\n",
    "pipe_select[:-1].transform(X_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, of the 30 original features, ``SelectPercentile`` only kept 20%, meaning 6. Now let's evaluate the whole pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_select.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance using only 20% of the features is actually identical to the performance when using all the features, but might be much more interpretable.\n",
    "We can see which features were selected by TODO.\n",
    "\n",
    "Now, that we have familiarized ourselves with how SelectPercentile works (at least in general terms), let's look at the example mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO hide\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(42)\n",
    "X = rng.normal(size=(100, 10000))\n",
    "y = rng.normal(size=(100,)) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say someone gave you a binary classification dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10000) (100,)\n",
      "[53 47]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)\n",
    "# count appearances of 0 and 1 in y\n",
    "print(np.bincount(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very wide, meaning it has many features, compared to the number of samples. This is quite common in sensor networks or in biomedical data for example.\n",
    "Given the small size of the dataset, we might want to use cross-validation to assess performance, instead of using a single train-test split.\n",
    "One might start like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 500)\n"
     ]
    }
   ],
   "source": [
    "# select most informative 5% of features\n",
    "select = SelectPercentile(percentile=5)\n",
    "select.fit(X, y)\n",
    "X_selected = select.transform(X)\n",
    "print(X_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset seems much more managable at 500 features (which are arguably still a lot), and we can evaluate our model with ``cross_val_score``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# run cross-validation with the subselected features\n",
    "cross_val_score(KNeighborsClassifier(), X_selected, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "If a model looks too good to be true, an experienced data scientist ususally looks for the mistake. Often it's a case of information leakage,\n",
    "so if you ever observe very high accuracy, you might do well to be skeptical at first.\n",
    "```\n",
    "\n",
    "It looks like it's our lucky day: we created a model that classifies our dataset perfectly across all folds. From this evaluation, we might be quite certain we found a good model.\n",
    "However, we made a mistake: we applied the feature selection procedure outside of the cross-validation. We should apply it inside the cross-validation instead.\n",
    "In scikit-learn, we can easily do that using a pipeline (as we did above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45, 0.5 , 0.5 , 0.5 , 0.7 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(SelectPercentile(percentile=5), KNeighborsClassifier())\n",
    "# run cross-validation on the original dataset using the pipeline\n",
    "cross_val_score(pipe, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the proper evaluation technique, our results change drastically: our model is around chance performance for a balanced dataset as this, in other words, we might conclude that the model didn't learn anything.\n",
    "Where does this dramatic difference come from? When we called ``fit`` on ``SelectPercentile`` before the cross-validation, it had access to the full dataset, which includes the training and test parts for each of the splits. This means it could extract information from all parts of the data, even those that we meant to use as validation set during cross-validation. This is a classical example of information leakage, and a good reason to always use pipelines!\n",
    "\n",
    "To make the difference in the computation a bit more apparent, I wrote down a more explicit version of the same computation, not using ``cross_val_score`` or ``Pipeline`` (we're using ``KFold`` here which is a way to get the indices to perform K-fold cross-validation, we'll see this in more detail in TODO):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{list-table}\n",
    "---\n",
    "header-rows: 1\n",
    "---\n",
    "* - preprocessing before cross validation\n",
    "  - preprocessing within cross validation\n",
    "* - ```python\n",
    "    # BAD!\n",
    "    select = SelectPercentile(percentile=5)\n",
    "    select.fit(X, y)  # includes the cv test parts!\n",
    "    X_sel = select.transform(X)\n",
    "    scores = []\n",
    "    for train, test in KFold().split(X, y):\n",
    "        knn = KNeighborsClassifier().fit(X_sel[train], y[train])\n",
    "        score = knn.score(X_sel[test], y[test])\n",
    "        scores.append(score)\n",
    "    ```\n",
    "  - ```python\n",
    "    # GOOD!\n",
    "    scores = []\n",
    "    select = SelectPercentile(percentile=5)\n",
    "    for train, test in KFold().split(X, y):\n",
    "        select.fit(X[train], y[train])\n",
    "        X_sel_train = select.transform(X[train])\n",
    "        knn = KNeighborsClassifier().fit(X_sel_train, y[train])\n",
    "        X_sel_test = select.transform(X[test])\n",
    "        score = knn.score(X_sel_test, y[test])\n",
    "        scores.append(score)\n",
    "    ```\n",
    "* - equivalent to:\n",
    "    ```python\n",
    "    select = SelectPercentile(percentile=5)\n",
    "    X_selected = select.fit_transform(X)\n",
    "    scores = cross_val_score(KNeighborsClassifier(), X, y)\n",
    "    ```\n",
    "  - ```python\n",
    "    pipe = make_pipeline(SelectPercentile(percentile=5),\n",
    "                         KNeighborsClassifier()\n",
    "    scores = cross_val_score(pipe, X, y)\n",
    "    ```\n",
    "    \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to estimate the generalization capability of our model, only the code on the right-hand side will give us the correct solution, and only this result will reflect how well the model will perform on new data. As a matter of fact, **the data in ``X`` and ``y`` was generated completely at random, and there was no relationship between the two**|. Using the procedure on the left-hand side allowed ``SelectPercentile`` to find some of the completely random features that happened to be related to the target, looking at the full dataset, including the validation part in each split. This is where information leaked. Using the procedure on the right-hand side, the feature selection could only select features based on the properties of the training part of the split. Features that had an accidental relationship on the training parts do not necessarily contain any information on the test parts, and so the performance of the model is estimated *correctly* to be at chance level.\n",
    "\n",
    "Hopefully will convince you to use ``Pipeline`` in all your your work, in particular when using cross-validation. However, if we want to use a ``Pipeline`` within ``GridSearchCV`` (which you definitely should!), we have to adjust our code a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline and GridSearchCV\n",
    "Remember that when using ``GridSearchCV`` for tuning hyper-parameters, we pass the estimator together with a dictionary of parameter values.\n",
    "If we pass a ``Pipeline`` as the estimator, we need to ensure that the parameters we want to tune are applied to the correct step of the pipeline. In principle, there could be several steps of the pipeline having identical hyper-parameter names.\n",
    "The way to specify the hyperparmeters within a ``Pipeline`` it to address it by the name of the step of the pipeline, followed by a double underscore (known as 'dunder' in Python), followed by the name of the hyper-parameter. So if we created a pipeline with ``make_pipeline``,\n",
    "and we want to tune the ``n_neighbors`` parameter in ``KNeighbors``, we need to use ``kneighbors__n_neighbors`` as the hyper-parameter name; remember, the when using ``make_pipeline``, the name that is assigned to each step is the lower-cased class name. Tuning the ``n_neighbors`` parameter on the breast cancer dataset could therefore look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kneighborsclassifier__n_neighbors': 8}\n",
      "0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "X, y  = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# create a pipeline\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "# create the search grid.\n",
    "# Pipeline hyper-parameters are specified as <step name>__<hyper-parameter name>\n",
    "param_grid = {'kneighborsclassifier__n_neighbors': range(1, 10)}\n",
    "# Instantiate grid-search\n",
    "grid = GridSearchCV(knn_pipe, param_grid, cv=10)\n",
    "# run the grid-search and report results\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````Note\n",
    "You can always check the available hyper-parameters of any model by calling the ``get_params`` method:\n",
    "\n",
    "```python\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "knn_pipe.get_params()\n",
    "```\n",
    "```\n",
    "{'memory': None,\n",
    " 'steps': [('standardscaler', StandardScaler()),\n",
    "  ('kneighborsclassifier', KNeighborsClassifier())],\n",
    " 'verbose': False,\n",
    " 'standardscaler': StandardScaler(),\n",
    " 'kneighborsclassifier': KNeighborsClassifier(),\n",
    " 'standardscaler__copy': True,\n",
    " 'standardscaler__with_mean': True,\n",
    " 'standardscaler__with_std': True,\n",
    " 'kneighborsclassifier__algorithm': 'auto',\n",
    " 'kneighborsclassifier__leaf_size': 30,\n",
    " 'kneighborsclassifier__metric': 'minkowski',\n",
    " 'kneighborsclassifier__metric_params': None,\n",
    " 'kneighborsclassifier__n_jobs': None,\n",
    " 'kneighborsclassifier__n_neighbors': 5,\n",
    " 'kneighborsclassifier__p': 2,\n",
    " 'kneighborsclassifier__weights': 'uniform'}\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a Pipeline inside GridSearchCV also allows us to tune hyper-parameters of the preprocessing steps. Say we want to tune how many feature we want to select in SelectPercentile, we can do it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kneighborsclassifier__n_neighbors': 8, 'selectpercentile__percentile': 100}\n",
      "0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "# create a pipeline\n",
    "select_pipe = make_pipeline(StandardScaler(), SelectPercentile(), KNeighborsClassifier())\n",
    "# create the search grid.\n",
    "# Pipeline hyper-parameters are specified as <step name>__<hyper-parameter name>\n",
    "param_grid = {'kneighborsclassifier__n_neighbors': range(1, 10),\n",
    "              'selectpercentile__percentile': [1, 2, 5, 10, 50, 100]}\n",
    "# Instantiate grid-search\n",
    "grid = GridSearchCV(select_pipe, param_grid, cv=10)\n",
    "# run the grid-search and report results\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, the when specifying multiple hyper-parameters, ``GridSearchCV`` tries out all possible combinations, so ``9 * 6 = 54 `` different combinations where tried in this code.\n",
    "The result is that keepign all features leads to the best result; this is not very surprising, as our motivation for removing features is usually not improving the accuracy, and if we do feature selection at all, we might be interested in trading off simplicity of the model and generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Estimators with GridSearchCV\n",
    "We can even go one step further and select what preprocessing to include or what model to apply. As a simple example, if we're unsure whether ``MinMaxScaler`` or ``StandardScaler`` is more appropriate for our dataset, we could just have ``GridSearchCV`` figure that out for us.\n",
    "After declaring a ``Pipeline`` object, each step becomes a hyper-parameter to which we can assign an estimator of our choice. It might be more natural in this case to name the steps of our pipeline manually, though you don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__n_neighbors': 8, 'scaler': StandardScaler()}\n",
      "0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# declare a two step pipeline, explicitly giving names to both steps.\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])\n",
    "# The name of the first step is 'scaler' and we can assign different\n",
    "# estimators to this step, such as MinMaxScaler or StandardScaler\n",
    "# There is a special value 'passthrough' which skips the step\n",
    "param_grid = {'scaler': [MinMaxScaler(), StandardScaler(), 'passthrough'],\n",
    "              # we named the second step knn, so we have to use that name here\n",
    "              'knn__n_neighbors': range(1, 10)}\n",
    "# instantiate and run as before:\n",
    "grid = GridSearchCV(pipe, param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "The initial value of 'scaler' from the declaration of `pipe` is not actually used. Scikit-learn requires us to provide a placeholder, though. We could have also used `'passthrough'` instead of StandardScaler and it wouldn't make a difference.\n",
    "```\n",
    "\n",
    "In this case, we didn't win much, but this is a useful tool for automating model selection. However, keep in mind that each option that you add will add a multiplier to your runtime, as all possible combinations are tried. We'll revisit this in chapter TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching Lists of Grids\n",
    "There is a little-known but very useful feature in ``GridSearchCV`` that I want to mention at this point. In fact, ``GridSearchCV`` can not only search over grids, but also over lists of grids, which are specified as lists of dictionaries.\n",
    "This comes in handy when trying to search over different preprocessing steps or models which have different hyper-parameters. For example, say we wanted to tune whether the ``MinMaxScaler`` should scale between ``0`` and ``1`` or between ``-1`` and ``1``, while also considering the case if using ``StandardScaler``. We can't just add ``feature_range`` to the ``param_grid`` dictionary because ``StandardScaler`` doesn't have a ``feature_range`` parameter. Instead we can create a list of two grids: one grid that always uses ``MinMaxScaler`` and one that always uses ``StandardScaler``. This is a bit of a contrived example, but once we know more models and transformers there will be plenty of cases where this comes in handy.\n",
    "\n",
    "The param_grid could then be specified as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [ # list of two dicts\n",
    "    # first dict always uses MinMaxScaler\n",
    "    {'scaler': [MinMaxScaler()],\n",
    "     # two options for feature_range:\n",
    "     'feature_range': [(0, 1), (-1, 1)]},\n",
    "    # second dict always uses StandardScaler\n",
    "    # there are no options that we're tuning\n",
    "    {'scaler': [StandardScaler()]}   \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of points to note here: first, the values for ``scaler`` always need to be a list, even if it's a list with a single element. So we can't specify ``'scaler': MinMaxScaler()``. Second, I left out the tuning of ``n_neightbors`` here. If we want to tune ``n_neighbors`` as well as selecting the preprocessing, we need to specify the range for each of the grids, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'scaler': [MinMaxScaler()],\n",
    "     'feature_range': [(0, 1), (-1, 1)],\n",
    "     'knn__n_neighbors': range(1, 10)},\n",
    "\n",
    "    {'scaler': [StandardScaler()],\n",
    "     'knn__n_neighbors': range(1, 10)}   \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This usage of ``GridSearchCV`` is a bit more advanced and it doesn't come up that often, but it's good to have in your back pocket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing attributes in grid-searched pipeline\n",
    "\n",
    "Finally, I want to walk through how you can get to any attributes of your model if it is in a pipeline in a gridsearch.\n",
    "We have seen all the parts of this already, but it's a bit involved and so I want to unpack it here.\n",
    "We fit a ``grid`` object above, which contained a ``Pipeline`` consisting of a `'scaler'` step and a `'knn'` step.\n",
    "Now let's say we want to find out what the mean of the training data was (again, this is a bit contrived but will come handy later for model inspection).\n",
    "As we learned in chapter TODO, we can get access to the model fitted on the whole training data using the ``best_estimator_`` attribute of ``GridSearchCV``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('knn', KNeighborsClassifier())]),\n",
       "             param_grid={'knn__n_neighbors': range(1, 10),\n",
       "                         'scaler': [MinMaxScaler(), StandardScaler(),\n",
       "                                    'passthrough']})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('knn', KNeighborsClassifier(n_neighbors=8))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see (and might have expected), ``grid.best_estimator_`` is a pipeline. So if we want to access the scaler, we need to extract the step we're interested in, for example using ``[]``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_['scaler']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not immediately obvious from the representation in Jupyter, but this is the scaler that was fitted on the whole training dataset. Now if we want to access the ``mean_`` we can just do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14.12,  19.2 ,  91.89, 654.92,   0.1 ,   0.1 ,   0.09,   0.05,\n",
       "         0.18,   0.06,   0.4 ,   1.21,   2.86,  40.13,   0.01,   0.03,\n",
       "         0.03,   0.01,   0.02,   0.  ,  16.21,  25.51, 106.89, 873.72,\n",
       "         0.13,   0.25,   0.27,   0.11,   0.29,   0.08])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppress scientific notation, only show two decimal points\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "grid.best_estimator_['scaler'].mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO ColumnTransformer also?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this chapter we saw the importance of using pipelines to avoid information leakage, in particular when using cross-validation. We also saw how you can combine ``Pipeline`` and ``GridSearchCV`` to tune your whole workflow with minimal code. Understanding ``Pipeline`` and how it interacts with model validation is critical for working with scikit-learn. Now, you know all of the most important building blocks of scikit-learn, and we have all the tools to start using the different models implemented in scikit-learn."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sklearndev]",
   "language": "python",
   "name": "conda-env-sklearndev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
