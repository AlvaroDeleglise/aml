

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Supervised learning with scikit-learn &#8212; Applied Machine Learning in Python</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Preprocessing and Scaling" href="03-preprocessing.html" />
    <link rel="prev" title="Data Loading" href="01-data-loading.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          

<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Applied Machine Learning in Python</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../index.html">1. Welcome</a>
  </li>
  <li class="">
    <a href="../00-introduction/00-introduction.html">2. Introduction</a>
  </li>
  <li class="active">
    <a href="00-ml-workflow.html">3. The Machine Learning Workflow</a>
  <ul class="nav sidenav_l2">
    <li class="">
      <a href="01-data-loading.html">1. Data Loading</a>
    </li>
    <li class="active">
      <a href="">2. Supervised learning with scikit-learn</a>
    </li>
    <li class="">
      <a href="03-preprocessing.html">3. Preprocessing and Scaling</a>
    </li>
    <li class="">
      <a href="04-categorical-variables.html">4. Categorical Variables</a>
    </li>
    <li class="">
      <a href="05-feature-engineering.html">5. Feature Engineering</a>
    </li>
    <li class="">
      <a href="08-imputation.html">6. Imputation</a>
    </li>
    <li class="">
      <a href="09-pipeline.html">7. Algorithm Chains and Pipelines</a>
    </li>
    <li class="">
      <a href="10-cross-validation-grid-search.html">8. Cross-validation and Grid Search</a>
    </li>
  </ul>
  </li>
  <li class="">
    <a href="../02-supervised-learning/index.html">4. Supervised Learning Algorithms</a>
  </li>
  <li class="">
    <a href="../03-unsupervised-learning/index.html">5. Unsupervised Learning Algorithms</a>
  </li>
  <li class="">
    <a href="../04-model-evaluation/index.html">6. Model Evaluation</a>
  </li>
  <li class="">
    <a href="../05-advanced-topics/index.html">7. Advanced Topics</a>
  </li>
</ul>
</nav>
<p class="navbar_footer">Powered by <a href="https://jupyterbook.org">Jupyter Book</a></p>
</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../_sources/01-ml-workflow/02-supervised-learning.ipynb.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.ipynb</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Edit this page -->
        

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
            <div class="dropdown-buttons">
                
                <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/01-ml-workflow/02-supervised-learning.ipynb"><button type="button" class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip" data-placement="left"><img class="binder-button-logo" src="../_static/images/logo_binder.svg" alt="Interact on binder">Binder</button></a>
                
                
                
            </div>
        </div>
        

        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#supervised-learning" class="nav-link">Supervised Learning</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#nearest-neighbors" class="nav-link">Nearest Neighbors</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#knn-with-scikit-learn" class="nav-link">KNN with scikit-learn</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#influence-of-number-of-neighbors" class="nav-link">Influence of Number of Neighbors</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#model-complexity" class="nav-link">Model complexity</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#overfitting-and-underfitting" class="nav-link">Overfitting and Underfitting</a>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="supervised-learning-with-scikit-learn">
<h1>Supervised learning with scikit-learn<a class="headerlink" href="#supervised-learning-with-scikit-learn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="supervised-learning">
<h2>Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>.larger[
<span class="math notranslate nohighlight">\($ (x_i, y_i) \propto p(x, y) \text{ i.i.d.}\)</span>$
<span class="math notranslate nohighlight">\($ x_i \in \mathbb{R}^p\)</span>$
<span class="math notranslate nohighlight">\($ y_i \in \mathbb{R}\)</span>$
<span class="math notranslate nohighlight">\($\text{learn }f(x_i) \approx y_i\)</span>$
such that
<span class="math notranslate nohighlight">\($f(x) \approx y \quad\)</span>$
for new <span class="math notranslate nohighlight">\((x, y) \propto p(x, y)\)</span>.
]</p>
<p>As a reminder, in supervised learning, the dataset we learn form is
input-output pairs <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>, where <span class="math notranslate nohighlight">\(x_i\)</span> is some n-dimensional input,
or feature vector, and <span class="math notranslate nohighlight">\(y_i\)</span> is the desired output we want to learn to predict.
We assume these samples are drawn from some unknown joint distribution
<span class="math notranslate nohighlight">\(p(x, y)\)</span>.
You can think of this as there being some (not necessarily deterministic)
process that goes from <span class="math notranslate nohighlight">\(x_i\)</span> to <span class="math notranslate nohighlight">\(y_i\)</span>, but that we don’t know.
We try to find a function that approximates the output <span class="math notranslate nohighlight">\(y_i\)</span> for each known
input <span class="math notranslate nohighlight">\(x_i\)</span>. But we also demand that for new inputs <span class="math notranslate nohighlight">\(x\)</span> , and unobserved <span class="math notranslate nohighlight">\(y\)</span>,
<span class="math notranslate nohighlight">\(f(x)\)</span> is approximately <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>I want to go through a simple examples of supervised algorithms today,
nearest neighbors</p>
</div>
<div class="section" id="nearest-neighbors">
<h2>Nearest Neighbors<a class="headerlink" href="#nearest-neighbors" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/02-supervised-learning_3_0.png" src="../_images/02-supervised-learning_3_0.png" />
</div>
</div>
<div class="math notranslate nohighlight">
\[f(x) = y_i, i = \text{argmin}_j || x_j - x||\]</div>
<p>Let’s say we have this two-class classification dataset here, with
two features, one on the x axis and one on the y axis.
And we have three new points as marked by the stars here.
If I make a prediction using a one nearest neighbor classifier, what
will it predict?
It will predict the label of the closest data point in the training set.
That is basically the simplest machine learning algorithm I can come
up with.</p>
<p>Here’s the formula:
the prediction for a new x is the y_i so that x_i is the closest point
in the training set.
Ok, so now how do we find out whether this model is any good?</p>
<p><img alt=":scale 60%" src="01-ml-workflow/images/train_test_set_2d_classification.png" /></p>
<p>The simplest way is to split the data into a training and a test set.
So we take some part of the data set,  let’s say 75% and the
corresponding output, and train the model, and then apply the model on
the remaining 25% to compute the accuracy. This test-set accuracy
will provide an unbiased estimate of future performance.
So if our i.i.d. assumption is correct, and we get a 90% success
rate on the test data, we expect about a 90% success rate on any future
data, for which we don’t have labels.</p>
<p>Let’s dive into how to build and evaluate this model with scikit-learn.</p>
</div>
<div class="section" id="knn-with-scikit-learn">
<h2>KNN with scikit-learn<a class="headerlink" href="#knn-with-scikit-learn" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy: &quot;</span><span class="p">,</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>accuracy:  0.5384615384615384
</pre></div>
</div>
</div>
</div>
<p>We import train_test_split form model selection, which does a
random split into 75%/25%.
We provide it with the data X, which are our two features, and the
labels y.</p>
<p>As you might already know, all the models in scikit-learn are implemented
in python classes, with a single object used to build and store the model.</p>
<p>We start by importing our model, the KneighborsClassifier, and instantiate
it with n_neighbors=1. Instantiating the object is when we set any hyper parameter,
such as here saying that we only want to look at the neirest neighbor.</p>
<p>Then we can call the fit method to build the model, here knn.fit(X_train,
y_train)
All models in scikit-learn have a fit-method, and all the supervised ones take
the data X and the outcomes y. The fit method builds the model and stores any
estimated quantities in the model object, here knn.  In the case of nearest
neighbors, the <code class="docutils literal notranslate"><span class="pre">fit</span></code> methods simply remembers the whole training set.</p>
<p>Then, we can use knn.score to make predictions on the test data, and
compare them against the true labels y_test.</p>
<p>For classification models, the score method will always compute
accuracy.</p>
<p>Just for illustration purposes, I also call the predict method here.
The predict method is what’s used to make predictions on any dataset.
If we use the score method, it calls predict internally and then
compares the outcome to the ground truth that’s provided.</p>
<p>Who here has not seen this before?</p>
</div>
<div class="section" id="influence-of-number-of-neighbors">
<h2>Influence of Number of Neighbors<a class="headerlink" href="#influence-of-number-of-neighbors" title="Permalink to this headline">¶</a></h2>
<p>So this was the predictions as made by one-nearest neighbor.
But we can also consider more neighbors, for example three. Here is the
three nearest neighbors for each of the points and the corresponding
labels.
We can then make a prediction by considering the majority among these
three neighbors.
And as you can see, in this case all the points changed their labels! (I
was actually quite surprised when I saw that, I just picked some points
at random).
Clearly the number of neighbors that we consider matters a lot. But what
is the right number?
The is a problem you’ll encounter a lot in machine learning, the
problem of tuning parameters of the model, also called hyper-parameters,
which can not be learned directly from the data.</p>
<p>So this was the predictions as made by one-nearest neighbor.
But we can also consider more neighbors, for example three. Here is the
three nearest neighbors for each of the points and the corresponding
labels.
We can then make a prediction by considering the majority among these
three neighbors.
And as you can see, in this case all the points changed their labels! (I
was actually quite surprised when I saw that, I just picked some points
at random).
Clearly the number of neighbors that we consider matters a lot. But what
is the right number?
The is a problem you’ll encounter a lot in machine learning, the
problem of tuning parameters of the model, also called hyper-parameters,
which can not be learned directly from the data.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/02-supervised-learning_11_0.png" src="../_images/02-supervised-learning_11_0.png" />
</div>
</div>
<p>Here’s an overview of how the classification changes if we consider
different numbers of neighbors.
You can see as red and blue circles the training data. And the background
is colored according to which class a datapoint would be assigned to
for each location.
For one neighbor, you can see that each point in the training set has
a little area around it that would be classified according to it’s
label. This means all the training points would be classified correctly,
but it leads to a very complex shape of the decision boundary.
If we increase the number of neighbors, the boundary between red and
blue simplifies, and with 40 neighbors we mostly end up with a line.
This also means that now many of the training data points would be
labeled incorrectly.</p>
</div>
<div class="section" id="model-complexity">
<h2>Model complexity<a class="headerlink" href="#model-complexity" title="Permalink to this headline">¶</a></h2>
<p><img alt=":scale 75%" src="01-ml-workflow/images/knn_model_complexity.png" /></p>
<p>We can look at this in more detail by comparing training and test set
scores for the different numbers of neighbors.
Here, I did a random 75%/25% split again. This is a very noisy plot as
the dataset is very small and I only did a random split, but you can
see a trend here.
You can see that for a single neighbor, the training score is 1 so perfect
accuracy, but the test score is only 70%.  If we increase the number of
neighbors we consider, the training score goes down, but the test score
goes up, with an optimum at 19 and 21, but then both go down again.</p>
<p>This is a very typical behavior, that I sketched in a schematic for you.</p>
<p>here is a cartoon version of how this chart looks in general, though
it’s horizontally flipped to the one with saw for knn.
This chart has accuracy on the y axis, and the abstract concept of model
complexity on the x axis.
If we make our machine learning models more complex, we will get better
training set accuracy, as the model will be able to capture more of the
variations in the data.</p>
<p>But if we look at the generalization performance, we get a different
story. If the model complexity is too low, the model will not be able
to capture the main trends, and a more complex model means better
generalization.
However, if we make the model too complex, generalization performance
drops again, because we basically learn to memorize the dataset.</p>
</div>
<div class="section" id="overfitting-and-underfitting">
<h2>Overfitting and Underfitting<a class="headerlink" href="#overfitting-and-underfitting" title="Permalink to this headline">¶</a></h2>
<p><img alt=":scale 80%" src="01-ml-workflow/images/overfitting_underfitting_cartoon_full.png" /></p>
<p>If we use too simple a model, this is often called underfitting, while
if we use to complex a model, this is called overfitting. And somewhere
in the middle is a sweet spot.
Most models have some way to tune model complexity, and we’ll see many
of them in the next couple of weeks.
So going back to nearest neighbors, what parameters correspond to high
model complexity and what to low model complexity? high n_neighbors =
low complexity!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="n">sklearn</span><span class="o">.</span><span class="n">set_config</span><span class="p">(</span><span class="n">print_changed_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">blood</span> <span class="o">=</span> <span class="n">fetch_openml</span><span class="p">(</span><span class="s1">&#39;blood-transfusion-service-center&#39;</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">blood</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">blood</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>(561, 4)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>1    438
2    123
dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-none notranslate"><div class="highlight"><pre><span></span>1    0.780749
2    0.219251
dtype: float64
</pre></div>
</div>
</div>
</div>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="01-data-loading.html" title="previous page">Data Loading</a>
    <a class='right-next' id="next-link" href="03-preprocessing.html" title="next page">Preprocessing and Scaling</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Andreas C. Müller<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>