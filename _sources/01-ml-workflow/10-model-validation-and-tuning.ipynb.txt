{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "So far our model evaluation was relatively simplistic, using a split into training and test data as shown in Figure TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 100%](images/train_test_split_new.png)\n",
    "\n",
    "This is a common scheme, but has several limitations that we'll address now.\n",
    "The first issue is model selection. As we discussed before, many models have hyper-parameters that we need to specify, such as ``k_neighbors`` in the ``KNeighborsClassifier``.\n",
    "We also might want to choose between different families of models or different algorithms. For simplicity, let's focus on the case of a single hyper-parameter, but there's no difference between that and selecting a different algorithm in principle.\n",
    "A natural way to find a good parameter to use would be to try different values, i.e. fit a model for each value of the hyper-parameter on the training set, and evaluate it on the test set.\n",
    "It seems that the hyper-parameters performing best on the test set should be a good choice. That's true, however, with a caveat that's illustrated in figure TODO.\n",
    "![:scale 80%](images/overfitting_validation_set_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo explain figure more; should say test set in the figure?\n",
    "\n",
    "The figure assumes we know what the \"real\" generalization performance of each hyper-parameter setting would be, a quantity that's only theoretically knowable, but the quantity that we're actually interested. What we have is the performance on the test set, which can be understood as a noisy version of the true generalization ability of the model.\n",
    "Now if we pick the best performing hyper-parameters, as indicated by the red dot, it provides a good estimate of what the optimum hyper-parameter value should be, in other words on the x-axis, the red dot is close to the maximum of the idealized generalization performance (the orange line).\n",
    "However, because we took the maximum of a noisy value, the actual accuracy at this point, i.e. the y axis value of the red dot, is overly optimistic. While the test set accuracy is an unbiased estimate of generalization, i.e. it's unbiased on average, taking the maximum over all hyper-parameters results in an optimistic bias.\n",
    "\n",
    "```{note} Bias in validation and multiple testing\n",
    "The issue described above of using the test set both for finding the optimum hyper-parameters and for estimating the accuracy of these parameters can be linked to the concept of multiple testing errors in statistics.\n",
    "The underlying idea is that if I try out many different things (say many hypothesis or many hyper-parameters), then at some point, I'll get a 'good' result by accident. In a process involving randomness and uncertainty, trying 100 times and getting a good result is not the same as trying once and getting a good result. Luckily the fix for our issue is much easier than the fix for multiple hypothesis testing.\n",
    "```\n",
    "\n",
    "So while we now know a good setting for our hyper-parmeter, we have no way of estimating how well the model with this hyper-parameter actually performs. If we want to use this model in production, that's a pretty big issue.\n",
    "However, it has a pretty simple solution: using an additional hold-out set, as shown in Figure todo.\n",
    "\n",
    "## The Threefold-split\n",
    "\n",
    "![:scale 100%](images/train_test_validation_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This use of three separate sets, a training set for model building, a validation set for model selection, and a test set for final model evaluation, is probably the most commonly used method for model selection and evaluation, and (apart from some modifications that we'll discuss below), it's a best practice that you should use whenever possible. The result is illustrated in Figure todo.\n",
    "![:scale 80%](images/overfitting_validation_set_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the validation set to find the value (i.e. x-axis location) of the optimum hyper-parameter, and the test set to find the corresponding y-value. Because we haven't used the test set for estimating the best hyper-parameter, the test set provides an unbiased estimate of generalization performance when following this method.\n",
    "One aspect of this is critically important, though: the test set only provides an unbiased result if we are using it once. We apply this selection procedure several times, compare several test-set results, and pick the best-performing model, we end up with the situation we started with, and our estimate will be biased. Therefore, **you should be really careful in when to use the test set**, and ideally set it aside and **use it only once, after you decided on the final model you want to use**.\n",
    "\n",
    "[Preventing Overfitting in cross-validation - Ng 1997](20http://robotics.stanford.edu/~ang/papers/cv-final.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the threefold split\n",
    "Before we go into more detail of tools for doing model-evaluation with scikit-learn, let's do the procedure *by foot* first, to clarify the process.\n",
    "Here is a simple implementation of using the three-fold split strategy for selecting the number of ``n_neighbors`` in ``KNeighborsClassifier`` on the TODO iris dataset.\n",
    "We split our dataset into three parts, by first talking 25% as the test set, and then taking 25% of the remainder as validation set (so about 19% of the original data).\n",
    "Then we define the candidate values of the parameter we want to adjust. This often requires knowledge of the algorithm and potentially the dataset.\n",
    "Here, we're using a range from 1 to 14 in steps of 2. For ``n_neighbors``, often uneven numbers are used to avoid ties. The upper range is picked somewhat arbitrarily, but we certainly wouldn't want to use more than 50, the number of samples in each class in this dataset.\n",
    "\n",
    "Then, we build a model for each value of ``n_neighbors`` in our list, evaluate it on the validation set, and store the result. Finally, we find the value that gave us the best result.\n",
    "Now we have basically made it to computing the red dot in figure TODO, and we only need to evaluate the purple dot.\n",
    "The easiest way to do this would be to just predict on the test set, which is certainly possible. In partice, we often prefer rebuilding the model using the training data together with the validation data.\n",
    "After we found the best value of ``n_neighbors`` the validation set is no longer useful, and so we can just use it as more data to build our model with. So we train a new model, using the best value of ``n_neighbors`` as determined by the validation set, use as training data both the original training set and the validation set, and evaluate the model on the test set.\n",
    "This provides us with an unbiased estimate of how well this final model will generalize.\n",
    "\n",
    "The step of retraining a model using both the training and validation set is optional, and if model training is very expensive, or we can assume the training dataset is large enough for our model, we might skip this step. Here, we have very little data, however, and we should use as much as we can get. We'll also see more reasons to use this retraining technique later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation score: 1.0\n",
      "best n_neighbors: 3\n",
      "test-set score: 0.974\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Load the data and split it into three parts\n",
    "X, y = load_iris(return_X_y=True)\n",
    "# first split off test set\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=23)\n",
    "# then split of validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=23)\n",
    "\n",
    "# create a list to hold validation scores for each\n",
    "# hyper-parameter setting\n",
    "val_scores = []\n",
    "# Specify a list of values we want to try for n_neighbors\n",
    "# This might require some knowledge of the dataset and the model\n",
    "# or potentially some trial-and-error\n",
    "neighbors = np.arange(1, 15, 2)\n",
    "# for each potential value of n_neighbors\n",
    "for i in neighbors:\n",
    "    # build a model\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    # score validation set accuracy\n",
    "    val_scores.append(knn.score(X_val, y_val))\n",
    "# using max tells us the best score\n",
    "print(f\"best validation score: {np.max(val_scores):.3}\")\n",
    "# with argmax we can find the best value of n_neighbors used\n",
    "best_n_neighbors = neighbors[np.argmax(val_scores)]\n",
    "print(\"best n_neighbors:\", best_n_neighbors)\n",
    "# Now,\n",
    "knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
    "knn.fit(X_trainval, y_trainval)\n",
    "print(f\"test-set score: {knn.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a three-fold split into training, validation and test set is essential whenever you're evaluating more than one model-which is always.\n",
    "However, there's another aspect we might want to improve, which is the reliance on the particular splits.\n",
    "If we change the random state in the splitting (todo?) we might end up with different results. Ideally we want the parameters we pick and our assessment of generalization ability not to be impacted by\n",
    "the initial splitting of the data. In fact, having large variety between outcomes depending on the data split is probably a bad sign and means our model is not very robust.\n",
    "In some cases, when the data is particularly small, such as for the iris dataset that we're using here, this can be hard to overcome, but there's still one tool\n",
    "that's invaluable to have in your toolbox to make model evaluation more robust: cross-validation.\n",
    "\n",
    "## K-Fold Cross-validation\n",
    "\n",
    "\n",
    "\n",
    "Here is an implementation of the three-fold split for selecting the\n",
    "number of neighbors.\n",
    "For each number of neighbors that we want to try, we build a model on\n",
    "the training set, and evaluate it on the validation set.\n",
    "We then pick the best validation set score, here that’s 97.2%, achieved\n",
    "when using three neighbors.\n",
    "We then retrain the model with this parameter, and evaluate on the test set.\n",
    "The retraining step is somewhat optional. We could also just use the best\n",
    "model. But retraining allows us to make better use of all the data.\n",
    "\n",
    "Still, depending on the test-set size we might be using only 70% or 80%\n",
    "of the data, and our results depend on how exactly we split the datasets.\n",
    "So how can we make this more robust?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    ".center[\n",
    "![:scale 80%](images/cross_validation_new.png)\n",
    "]\n",
    "\n",
    "\n",
    "The answer is of course cross-validation. In cross-validation, you split\n",
    "your data into multiple folds, usually 5 or 10, and built multiple models.\n",
    "You start by using fold1 as the test data, and the remaining ones as the\n",
    "training data. You build your model on the training data, and evaluate\n",
    "it on the test fold.\n",
    "For each of the splits of the data, you get a model evaluation and a\n",
    "score. In the end, you can aggregate the scores, for example by taking\n",
    "the mean.\n",
    "What are the pros and cons of this?\n",
    "Each data point is in the test-set exactly once!\n",
    "Takes 5 or 10 times longer!\n",
    "Better data use (larger training sets).\n",
    "Does that solve all problems? No, it replaces only one of the splits,\n",
    "usually the inner one!\n",
    "--\n",
    ".smaller[\n",
    "pro: more stable, more data\n",
    "\n",
    "con: slower\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: center, some-space\n",
    "## Cross-validation + test set\n",
    "\n",
    "![:scale 105%](images/grid_search_cross_validation_new.png)\n",
    "\n",
    "\n",
    "\n",
    "Here is how the workflow looks like when we are using five-fold\n",
    "cross-validation together with a test-set split for adjusting parameters.\n",
    "We start out by splitting of the test data, and then we perform\n",
    "cross-validation on the training set.\n",
    "Once we found the right setting of the parameters, we retrain on the\n",
    "whole training set and evaluate on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-Search with Cross-Validation\n",
    "\n",
    ".smaller[\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "cross_val_scores = []\n",
    "\n",
    "for i in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10)\n",
    "    cross_val_scores.append(np.mean(scores))\n",
    "    \n",
    "print(f\"best cross-validation score: {np.max(cross_val_scores):.3}\")\n",
    "best_n_neighbors = neighbors[np.argmax(cross_val_scores)]\n",
    "print(f\"best n_neighbors: {best_n_neighbors}\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
    "knn.fit(X_train, y_train)\n",
    "print(f\"test-set score: {knn.score(X_test, y_test):.3f}\")\n",
    "```\n",
    "\n",
    "```\n",
    "best cross-validation score: 0.967\n",
    "best n_neighbors: 9\n",
    "test-set score: 0.965\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Here is an implementation of this  for k nearest neighbors.\n",
    "\n",
    "We split the data, then we iterate over all parameters and for each of\n",
    "them we do cross-validation.\n",
    "\n",
    "We had seven different values of n_neighbors, and we are running 10 fold\n",
    "cross-validation. How many models to we train in total?\n",
    "10 * 7 + 1 = 71 (the one is the final model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: center, middle\n",
    "![:scale 80%](images/gridsearch_workflow.png)\n",
    "\n",
    "\n",
    "\n",
    "Here is a conceptual overview of this way of tuning parameters, we start\n",
    "of with the dataset and a candidate set of parameters we want to try,\n",
    "labeled parameter grid, for example the number of neighbors.\n",
    "\n",
    "We split the dataset in to training and test set. We use cross-validation\n",
    "and the parameter grid to find the best parameters.\n",
    "We use the best parameters and the training set to build a model with\n",
    "the best parameters,\n",
    "and finally evaluate it on the test set.\n",
    "\n",
    "\n",
    "Because this is such a common pattern, there is a helper class for this\n",
    "in scikit-learn, called GridSearch CV, which does most of these steps\n",
    "for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    ".smaller[\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "\n",
    "param_grid = {'n_neighbors':  np.arange(1, 30, 2)}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10,\n",
    "                   return_train_score=True)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"best mean cross-validation score: {grid.best_score_}\")\n",
    "print(f\"best parameters: {grid.best_params_}\")\n",
    "print(f\"test-set score: {grid.score(X_test, y_test):.3f}\")\n",
    "```\n",
    "\n",
    "```\n",
    "best mean cross-validation score: 0.967\n",
    "best parameters: {'n_neighbors': 9}\n",
    "test-set score: 0.993\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "Here is an example.\n",
    "We still need to split our data into training and test set.\n",
    "We declare the parameters we want to search over as a dictionary. In\n",
    "this example the parameter is just n_neighbors and the values we want\n",
    "to try out are a range. The keys of the dictionary are the parameter\n",
    "names and the values are the parameter settings we want to try. If you\n",
    "specify multiple parameters, all possible combinations are tried. This\n",
    "is where the name grid-search comes from - it’s an exhaustive search\n",
    "over all possible parameter combinations that you specify.\n",
    "\n",
    "GridSearchCV is a class, and it behaves just like any other model in\n",
    "scikit-learn, with a fit, predict and score method.\n",
    "It’s what we call a meta-estimator, since you give it one estimator,\n",
    "here the KneighborsClassifier, and from that GridSearchCV constructs a\n",
    "new estimator that does the parameter search for you.\n",
    "You also specify the parameters you want to search, and the\n",
    "cross-validation strategy.\n",
    "Then GridSearchCV does all the other things we talked about, it does the\n",
    "cross-validation and parameter selection, and retrains a model with the\n",
    "best parameter settings that were found.\n",
    "We can check out the best cross-validation score and the best parameter\n",
    "setting with the best_score_ and best_params_ attributes.\n",
    "And finally we can compute the accuracy on the test set, simply but\n",
    "using the score method! That will use the retrained model under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: compact\n",
    "\n",
    "## GridSearchCV Results\n",
    ".tiny[\n",
    "```python\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.columns\n",
    "```\n",
    "```\n",
    "Index(['mean_fit_time', 'mean_score_time', 'mean_test_score',\n",
    "       'mean_train_score', 'param_n_neighbors', 'params', 'rank_test_score',\n",
    "       'split0_test_score', 'split0_train_score', 'split1_test_score',\n",
    "       'split1_train_score', 'split2_test_score', 'split2_train_score',\n",
    "       'split3_test_score', 'split3_train_score', 'split4_test_score',\n",
    "       'split4_train_score', 'split5_test_score', 'split5_train_score',\n",
    "       'split6_test_score', 'split6_train_score', 'split7_test_score',\n",
    "       'split7_train_score', 'split8_test_score', 'split8_train_score',\n",
    "       'split9_test_score', 'split9_train_score', 'std_fit_time',\n",
    "       'std_score_time', 'std_test_score', 'std_train_score'],\n",
    "      dtype='object')\n",
    "```\n",
    "\n",
    "```python\n",
    "results.params\n",
    "```\n",
    "```\n",
    "0     {'n_neighbors': 1}\n",
    "1     {'n_neighbors': 3}\n",
    "2     {'n_neighbors': 5}\n",
    "3     {'n_neighbors': 7}\n",
    "4     {'n_neighbors': 9}\n",
    "5    {'n_neighbors': 11}\n",
    "6    {'n_neighbors': 13}\n",
    "Name: params, dtype: object\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "FIXME text size\n",
    "GridSearchCV also computes a lot of interesting statistics for you, which\n",
    "are stored in the cv_results_ attribute. That attribute is a dictionary,\n",
    "but it’s easiest to convert it to a pandas dataframe to look at it.\n",
    "Here you can see the columns. Theres mean fit time, mean score time,\n",
    "mean test scores, mean training scores, standard deviations and scores\n",
    "for each individual split of the data.\n",
    "And there is one row for each setting of the parameters we tried out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: center\n",
    "## n_neighbors Search Results\n",
    "\n",
    "![:scale 70%](images/grid_search_n_neighbors.png)\n",
    "\n",
    "\n",
    "We can use this for example to plot the results of cross-validation over\n",
    "the different parameters.\n",
    "Here are the mean training score and mean test score together with one\n",
    "standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class: spacious\n",
    "## Nested Cross-Validation\n",
    "\n",
    "- Replace outer split by CV loop\n",
    "- Doesn’t yield single model\n",
    "(inner loop might have different best parameter settings)\n",
    "- Takes a long time, not that useful in practice\n",
    "\n",
    "\n",
    "\n",
    "We could additionally replace the outer split of the data by\n",
    "cross-validation. That would yield what’s known as nested\n",
    "cross-validation.\n",
    "This is sometimes interesting when comparing different models, but it will\n",
    "not actually yield one final model. It will yield one model for each loop\n",
    "of the outer fold, which might have different settings of the parameters.\n",
    "Also, this takes a really long time to train, by an additional factor\n",
    "of 5 or 10, so this is not used very commonly in practice.\n",
    "\n",
    "But let’s dive into the cross-validation a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "![:scale 80%](images/cross_validation_new.png)\n",
    "\n",
    "\n",
    "\n",
    "The answer is of course cross-validation. In cross-validation, you split\n",
    "your data into multiple folds, usually 5 or 10, and built multiple models.\n",
    "You start by using fold1 as the test data, and the remaining ones as the\n",
    "training data. You build your model on the training data, and evaluate\n",
    "it on the test fold.\n",
    "For each of the splits of the data, you get a model evaluation and a\n",
    "score. In the end, you can aggregate the scores, for example by taking\n",
    "the mean.\n",
    "What are the pros and cons of this?\n",
    "Each data point is in the test-set exactly once!\n",
    "Takes 5 or 10 times longer!\n",
    "Better data use (larger training sets).\n",
    "Does that solve all problems? No, it replaces only one of the splits,\n",
    "usually the inner one!\n",
    "\n",
    ".smaller[\n",
    "pro: more stable, more data\n",
    "\n",
    "con: slower\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation + test set\n",
    "\n",
    "![:scale 105%](images/grid_search_cross_validation_new.png)\n",
    "\n",
    "\n",
    "\n",
    "Here is how the workflow looks like when we are using five-fold\n",
    "cross-validation together with a test-set split for adjusting parameters.\n",
    "We start out by splitting of the test data, and then we perform\n",
    "cross-validation on the training set.\n",
    "Once we found the right setting of the parameters, we retrain on the\n",
    "whole training set and evaluate on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-Search with Cross-Validation\n",
    "\n",
    ".smaller[\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "cross_val_scores = []\n",
    "\n",
    "for i in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10)\n",
    "    cross_val_scores.append(np.mean(scores))\n",
    "    \n",
    "print(f\"best cross-validation score: {np.max(cross_val_scores):.3}\")\n",
    "best_n_neighbors = neighbors[np.argmax(cross_val_scores)]\n",
    "print(f\"best n_neighbors: {best_n_neighbors}\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
    "knn.fit(X_train, y_train)\n",
    "print(f\"test-set score: {knn.score(X_test, y_test):.3f}\")\n",
    "```\n",
    "\n",
    "```\n",
    "best cross-validation score: 0.967\n",
    "best n_neighbors: 9\n",
    "test-set score: 0.965\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Here is an implementation of this  for k nearest neighbors.\n",
    "\n",
    "We split the data, then we iterate over all parameters and for each of\n",
    "them we do cross-validation.\n",
    "\n",
    "We had seven different values of n_neighbors, and we are running 10 fold\n",
    "cross-validation. How many models to we train in total?\n",
    "10 * 7 + 1 = 71 (the one is the final model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 80%](images/gridsearch_workflow.png)\n",
    "\n",
    "\n",
    "\n",
    "Here is a conceptual overview of this way of tuning parameters, we start\n",
    "of with the dataset and a candidate set of parameters we want to try,\n",
    "labeled parameter grid, for example the number of neighbors.\n",
    "\n",
    "We split the dataset in to training and test set. We use cross-validation\n",
    "and the parameter grid to find the best parameters.\n",
    "We use the best parameters and the training set to build a model with\n",
    "the best parameters,\n",
    "and finally evaluate it on the test set.\n",
    "\n",
    "\n",
    "Because this is such a common pattern, there is a helper class for this\n",
    "in scikit-learn, called GridSearch CV, which does most of these steps\n",
    "for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    ".smaller[\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "\n",
    "param_grid = {'n_neighbors':  np.arange(1, 30, 2)}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10,\n",
    "                   return_train_score=True)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"best mean cross-validation score: {grid.best_score_}\")\n",
    "print(f\"best parameters: {grid.best_params_}\")\n",
    "print(f\"test-set score: {grid.score(X_test, y_test):.3f}\")\n",
    "```\n",
    "\n",
    "```\n",
    "best mean cross-validation score: 0.967\n",
    "best parameters: {'n_neighbors': 9}\n",
    "test-set score: 0.993\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "Here is an example.\n",
    "We still need to split our data into training and test set.\n",
    "We declare the parameters we want to search over as a dictionary. In\n",
    "this example the parameter is just n_neighbors and the values we want\n",
    "to try out are a range. The keys of the dictionary are the parameter\n",
    "names and the values are the parameter settings we want to try. If you\n",
    "specify multiple parameters, all possible combinations are tried. This\n",
    "is where the name grid-search comes from - it’s an exhaustive search\n",
    "over all possible parameter combinations that you specify.\n",
    "\n",
    "GridSearchCV is a class, and it behaves just like any other model in\n",
    "scikit-learn, with a fit, predict and score method.\n",
    "It’s what we call a meta-estimator, since you give it one estimator,\n",
    "here the KneighborsClassifier, and from that GridSearchCV constructs a\n",
    "new estimator that does the parameter search for you.\n",
    "You also specify the parameters you want to search, and the\n",
    "cross-validation strategy.\n",
    "Then GridSearchCV does all the other things we talked about, it does the\n",
    "cross-validation and parameter selection, and retrains a model with the\n",
    "best parameter settings that were found.\n",
    "We can check out the best cross-validation score and the best parameter\n",
    "setting with the best_score_ and best_params_ attributes.\n",
    "And finally we can compute the accuracy on the test set, simply but\n",
    "using the score method! That will use the retrained model under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GridSearchCV Results\n",
    ".tiny[\n",
    "```python\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.columns\n",
    "```\n",
    "```\n",
    "Index(['mean_fit_time', 'mean_score_time', 'mean_test_score',\n",
    "       'mean_train_score', 'param_n_neighbors', 'params', 'rank_test_score',\n",
    "       'split0_test_score', 'split0_train_score', 'split1_test_score',\n",
    "       'split1_train_score', 'split2_test_score', 'split2_train_score',\n",
    "       'split3_test_score', 'split3_train_score', 'split4_test_score',\n",
    "       'split4_train_score', 'split5_test_score', 'split5_train_score',\n",
    "       'split6_test_score', 'split6_train_score', 'split7_test_score',\n",
    "       'split7_train_score', 'split8_test_score', 'split8_train_score',\n",
    "       'split9_test_score', 'split9_train_score', 'std_fit_time',\n",
    "       'std_score_time', 'std_test_score', 'std_train_score'],\n",
    "      dtype='object')\n",
    "```\n",
    "\n",
    "```python\n",
    "results.params\n",
    "```\n",
    "```\n",
    "0     {'n_neighbors': 1}\n",
    "1     {'n_neighbors': 3}\n",
    "2     {'n_neighbors': 5}\n",
    "3     {'n_neighbors': 7}\n",
    "4     {'n_neighbors': 9}\n",
    "5    {'n_neighbors': 11}\n",
    "6    {'n_neighbors': 13}\n",
    "Name: params, dtype: object\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "FIXME text size\n",
    "GridSearchCV also computes a lot of interesting statistics for you, which\n",
    "are stored in the cv_results_ attribute. That attribute is a dictionary,\n",
    "but it’s easiest to convert it to a pandas dataframe to look at it.\n",
    "Here you can see the columns. Theres mean fit time, mean score time,\n",
    "mean test scores, mean training scores, standard deviations and scores\n",
    "for each individual split of the data.\n",
    "And there is one row for each setting of the parameters we tried out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_neighbors Search Results\n",
    "\n",
    "![:scale 70%](images/grid_search_n_neighbors.png)\n",
    "\n",
    "\n",
    "We can use this for example to plot the results of cross-validation over\n",
    "the different parameters.\n",
    "Here are the mean training score and mean test score together with one\n",
    "standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation\n",
    "\n",
    "- Replace outer split by CV loop\n",
    "- Doesn’t yield single model\n",
    "(inner loop might have different best parameter settings)\n",
    "- Takes a long time, not that useful in practice\n",
    "\n",
    "\n",
    "\n",
    "We could additionally replace the outer split of the data by\n",
    "cross-validation. That would yield what’s known as nested\n",
    "cross-validation.\n",
    "This is sometimes interesting when comparing different models, but it will\n",
    "not actually yield one final model. It will yield one model for each loop\n",
    "of the outer fold, which might have different settings of the parameters.\n",
    "Also, this takes a really long time to train, by an additional factor\n",
    "of 5 or 10, so this is not used very commonly in practice.\n",
    "\n",
    "But let’s dive into the cross-validation a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Strategies\n",
    "\n",
    "\n",
    "\n",
    "So I mentioned k-fold cross validation, where k is usually 5 or ten,\n",
    "but there are many other strategies.\n",
    "\n",
    "One of the most commonly ones is stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 90%](images/kfold_cv.png)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 90%](images/stratified_cv.png)\n",
    "]\n",
    ".smallest[\n",
    "Stratified:\n",
    "Ensure relative class frequencies in each fold reflect relative class\n",
    "frequencies on the whole dataset.]\n",
    "\n",
    "\n",
    "\n",
    "The idea behind stratified k-fold cross-validation is that you want the\n",
    "test set to be as representative of the dataset as possible.\n",
    "StratifiedKFold preserves the class frequencies in each fold to be the\n",
    "same as of the overall dataset.\n",
    "Here is and example of a dataset with three classes that are ordered. If\n",
    "you apply standard three-fold to this, the first third of the data would\n",
    "be in the first fold, the second in the second fold and the third in\n",
    "the third fold. Because this data is sorted, that would be particularly\n",
    "bad. If you use stratified cross-validation it would make sure that each\n",
    "fold has exactly 1/3 of the data from each class.\n",
    "\n",
    "This is also helpful if your data is very imbalanced. If some of the\n",
    "classes are very rare, it could otherwise happen that a class is not\n",
    "present at all in a particular fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Stratification\n",
    ".smaller[\n",
    "```python\n",
    "y.value_counts()\n",
    "```\n",
    "```\n",
    "0    60\n",
    "1    40\n",
    "```\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dc = DummyClassifier('most_frequent')\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "res = cross_val_score(dc, X, y, cv=skf)\n",
    "np.mean(res), res.std()\n",
    "```\n",
    "```\n",
    "(0.6, 0.0)\n",
    "```\n",
    "```python\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "res = cross_val_score(dc, X, y, cv=kf)\n",
    "np.mean(res), res.std()\n",
    "```\n",
    "```\n",
    "(0.6, 0.063)\n",
    "```\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated KFold and LeaveOneOut\n",
    "\n",
    "- LeaveOneOut : KFold(n_folds=n_samples) \n",
    "\n",
    "High variance, takes a long time \n",
    "\n",
    ".tiny[(see [Raschka](https://arxiv.org/pdf/1811.12808.pdf) for a review and [Varoquaux](https://hal.inria.fr/hal-01545002/file/paper.pdf) for empirical evaluation)]\n",
    "\n",
    "- Better: ShuffleSplit (aka Monte Carlo) \n",
    "\n",
    "Repeatedly sample a test set with replacement\n",
    "\n",
    "- Even Better: RepeatedKFold. \n",
    "\n",
    "Apply KFold or StratifiedKFold multiple times with shuffled data.\n",
    "\n",
    "\n",
    "\n",
    "If you want even better estimates of the generalization performance,\n",
    "you could try to increase the number of folds, with the extreme\n",
    "of creating one fold per sample. That’s called “LeaveOneOut\n",
    "cross-validation”. However, because the test-set is so small every time,\n",
    "and the training sets all have very large overlap, this method has very\n",
    "high variance.\n",
    "A better way to get a robust estimate is to run 5-fold or 10-fold\n",
    "cross-validation multiple times, while shuffling the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 100%](images/shuffle_split_cv.png)\n",
    "]\n",
    ".smaller[Number of iterations and test set size independent]\n",
    "\n",
    "\n",
    "Another interesting variant is shuffle split and stratified shuffle\n",
    "split. In shuffle split, we repeatedly sample disjoint training and test\n",
    "sets randomly.\n",
    "You only have to specify the number of iterations, the training set size\n",
    "and the test set size. This also allows you to run many iterations with\n",
    "reasonably large test-sets.\n",
    "It’s also great if you have a very large training set and you want to\n",
    "subsample it to get quicker results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 100%](images/repeated_stratified_kfold.png)\n",
    "]\n",
    ".smaller[\n",
    "Potentially less variance than StratifiedShuffleSplit.\n",
    "\n",
    "Five times five fold or at most ten times ten fold is sufficient.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defaults in scikit-learn\n",
    "\n",
    "- 5-fold in 0.22 (used to be 3 fold)\n",
    "- For classification cross-validation is stratified\n",
    "- train_test_split has stratify option:\n",
    "train_test_split(X, y, stratify=y)\n",
    "\n",
    "- No shuffle by default!\n",
    "\n",
    "\n",
    "\n",
    "By default, all cross-validation strategies are five fold.\n",
    "If you do cross-validation for classification, it will be stratified\n",
    "by default.\n",
    "Because of how the interface is done, that’s not true for\n",
    "train_test_split and if you want a stratified train_test_split, which\n",
    "is always a good idea, you should use stratify=y\n",
    "Another thing that’s important to keep in mind is that by default\n",
    "scikit-learn doesn’t shuffle! So if you run cross-validation twice\n",
    "with the default parameters, it will yield exactly the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation with non-iid data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Data\n",
    "### Assume have data (medical, product, user...) from 5 cities\n",
    "- New York, San Francisco, Los Angeles, Chicago, Houston.\n",
    "\n",
    "We can assume data within a city is more correlated then between cities.\n",
    "\n",
    "### Usage Scenarios\n",
    "- Assume all future users will be in one of these cities: i.i.d.\n",
    "- Assume we want to generalize to predict for a new city: not i.i.d.\n",
    "\n",
    "\n",
    "\n",
    "Shipped product in 4 cities. Might ship in another one?\n",
    "States: you have all the states, no new state will start to exist\n",
    "\n",
    "Similar thing for multiple measurements per patient.\n",
    "Or geospacial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 100%](images/group_kfold.png)\n",
    "\n",
    "\n",
    "\n",
    "A somewhat more complicated approach is group k-fold.\n",
    "This is actually for data that doesn’t fulfill our IID assumption and\n",
    "has correlations between samples.\n",
    "The idea is that there are several groups in the data that each contain\n",
    "highly correlated samples.\n",
    "You could think about patient data where you have multiple samples for\n",
    "each patient, then the groups would be which patient a measurement was\n",
    "taken from.\n",
    "If you want to know how well your model generalizes to new patients,\n",
    "you need to ensure that the measurements from each patient are either\n",
    "all in the training set, or all in the test set.\n",
    "And that’s what GroupKFold does.\n",
    "In this example, there are four groups, and we want three folds. The\n",
    "data is divided such that each group is contained in exactly one fold.\n",
    "There are several other cross-validation methods in scikit-learn that\n",
    "use these groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations in time (and/or space)\n",
    "\n",
    "![:scale 70%](images/time_series1.png)\n",
    "\n",
    "\n",
    "\n",
    "Not necessarily obvious that there is a time component!\n",
    "Data collection usually happens over time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations in time (and/or space)\n",
    "\n",
    "![:scale 70%](images/time_series2.png)\n",
    "\n",
    "\n",
    "\n",
    "Not necessarily obvious that there is a time component!\n",
    "Data collection usually happens over time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations in time (and/or space)\n",
    "\n",
    "![:scale 70%](images/time_series3.png)\n",
    "\n",
    "\n",
    "\n",
    "Not necessarily obvious that there is a time component!\n",
    "Data collection usually happens over time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 100%](images/time_series_walk_forward_cv.png)\n",
    "\n",
    "\n",
    "Another common case of data that’s not independent is time\n",
    "series. Usually todays stock price is correlated with yesterdays and\n",
    "tomorrows. If you randomly split time series, this makes predictions\n",
    "deceivingly simple. In applications, you usually have data up to some\n",
    "point, and then try to make predictions for the future, in other words,\n",
    "you’re trying to make a forecast.\n",
    "The TimeSeriesSplit in scikit-learn simulates that, by taking increasing\n",
    "chunks of data from the past and making predictions on the next\n",
    "chunk. This is quite different from the other was to do cross-validation,\n",
    "in that the training sets are all overlapping, but it’s more appropriate\n",
    "for time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 100%](images/time_series_cv.png)\n",
    "\n",
    "\n",
    "\n",
    "Another common case of data that’s not independent is time\n",
    "series. Usually todays stock price is correlated with yesterdays and\n",
    "tomorrows. If you randomly split time series, this makes predictions\n",
    "deceivingly simple. In applications, you usually have data up to some\n",
    "point, and then try to make predictions for the future, in other words,\n",
    "you’re trying to make a forecast.\n",
    "The TimeSeriesSplit in scikit-learn simulates that, by taking increasing\n",
    "chunks of data from the past and making predictions on the next\n",
    "chunk. This is quite different from the other was to do cross-validation,\n",
    "in that the training sets are all overlapping, but it’s more appropriate\n",
    "for time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Cross-Validation Generators\n",
    "\n",
    ".tiny[\n",
    "```python\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit, RepeatedStratifiedKFold\n",
    "kfold = KFold(n_splits=5)\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "ss = ShuffleSplit(n_splits=20, train_size=.4, test_size=.3)\n",
    "rs = RepeatedStratifiedKFold(n_splits=5, n_repeats=10)\n",
    "\n",
    "print(\"KFold:\")\n",
    "print(cross_val_score(KNeighborsClassifier(), X, y, cv=kfold))\n",
    "\n",
    "print(\"StratifiedKFold:\")\n",
    "print(cross_val_score(KNeighborsClassifier(), X, y, cv=skfold))\n",
    "\n",
    "print(\"ShuffleSplit:\")\n",
    "print(cross_val_score(KNeighborsClassifier(), X, y, cv=ss))\n",
    "\n",
    "print(\"RepeatedStratifiedKFold:\")\n",
    "print(cross_val_score(KNeighborsClassifier(), X, y, cv=rs))\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "KFold:\n",
    "[0.93 0.96 0.96 0.98 0.96]\n",
    "StratifiedKFold:\n",
    "[0.98 0.96 0.96 0.97 0.96]\n",
    "ShuffleSplit:\n",
    "[0.98 0.96 0.96 0.98 0.94 0.96 0.95 0.98 0.97 0.92 0.94 0.97 0.95 0.92\n",
    " 0.98 0.98 0.97 0.94 0.97 0.95]\n",
    "RepeatedStratifiedKFold:\n",
    "[0.99 0.96 0.97 0.97 0.95 0.98 0.97 0.98 0.97 0.96 0.97 0.99 0.94 0.96\n",
    " 0.96 0.98 0.97 0.96 0.96 0.97 0.97 0.96 0.96 0.96 0.98 0.96 0.97 0.97\n",
    " 0.97 0.96 0.96 0.95 0.96 0.99 0.98 0.93 0.96 0.98 0.98 0.96 0.96 0.95\n",
    " 0.97 0.97 0.96 0.97 0.97 0.97 0.96 0.96]\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "Ok, so how do we use these cross-validation generators? We can simply\n",
    "pass the object to the cv parameter of the cross_val_score function,\n",
    "instead of passing a number. Then that generator will be used.\n",
    "Here are some examples for k-neighbors classifier.\n",
    "We instantiate a Kfold object with the number of splits equal to 5,\n",
    "and then pass it to cross_val_score.\n",
    "We can do the same with StratifiedKFold, and we can also shuffle if we\n",
    "like, or we can use Shuffle split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_validate function\n",
    ".smaller[\n",
    "```python\n",
    "from sklearn.model_selection import cross_validate\n",
    "res = cross_validate(KNeighborsClassifier(), X, y, return_train_score=True,\n",
    "                     scoring=[\"accuracy\", \"roc_auc\"])\n",
    "res_df = pd.DataFrame(res)\n",
    "```\n",
    "\n",
    "```\n",
    "fit_time\tscore_time\ttest_accuracy\ttest_roc_auc\ttrain_accuracy\ttrain_roc_auc\n",
    "0.000839\t0.010204    0.965217\t    0.996609\t    0.980176\t    0.997654\n",
    "0.000870\t0.014424    0.956522\t    0.983689\t    0.975771\t    0.998650\n",
    "0.000603\t0.009298    0.982301\t    0.999329\t    0.971491\t    0.996977\n",
    "0.000698\t0.006670    0.955752\t    0.984071\t    0.978070\t    0.997820\n",
    "0.000611\t0.006559    0.964602\t    0.994634\t    0.978070\t    0.998026\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "FIXME alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Questions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "sklearn.set_config(print_changed_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(KNeighborsClassifier(),\n",
    "                X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(KNeighborsClassifier(),\n",
    "                X_train, y_train, cv=KFold(n_splits=10, shuffle=True, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(KNeighborsClassifier(),\n",
    "                X_train, y_train,\n",
    "                cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Searches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid-Search with build-in cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameter grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "param_grid = {'C': 10. ** np.arange(-3, 3),\n",
    "              'gamma' : 10. ** np.arange(-5, 0)}\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(SVC(), param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GridSearchCV object behaves just like a normal classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract just the scores\n",
    "\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "scores = np.array(scores).reshape(6, 5)\n",
    "\n",
    "plt.matshow(scores)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(5), param_grid['gamma'])\n",
    "plt.yticks(np.arange(6), param_grid['C']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Use GridSearchCV to adjust n_neighbors of KNeighborsClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Model complexity\n",
    "\n",
    "![:scale 75%](images/knn_model_complexity.png)\n",
    "\n",
    "\n",
    "We can look at this in more detail by comparing training and test set\n",
    "scores for the different numbers of neighbors.\n",
    "Here, I did a random 75%/25% split again. This is a very noisy plot as\n",
    "the dataset is very small and I only did a random split, but you can\n",
    "see a trend here.\n",
    "You can see that for a single neighbor, the training score is 1 so perfect\n",
    "accuracy, but the test score is only 70%.  If we increase the number of\n",
    "neighbors we consider, the training score goes down, but the test score\n",
    "goes up, with an optimum at 19 and 21, but then both go down again.\n",
    "\n",
    "This is a very typical behavior, that I sketched in a schematic for you.\n",
    "\n",
    "\n",
    "here is a cartoon version of how this chart looks in general, though\n",
    "it's horizontally flipped to the one with saw for knn.\n",
    "This chart has accuracy on the y axis, and the abstract concept of model\n",
    "complexity on the x axis.\n",
    "If we make our machine learning models more complex, we will get better\n",
    "training set accuracy, as the model will be able to capture more of the\n",
    "variations in the data.\n",
    "\n",
    "\n",
    "But if we look at the generalization performance, we get a different\n",
    "story. If the model complexity is too low, the model will not be able\n",
    "to capture the main trends, and a more complex model means better\n",
    "generalization.\n",
    "However, if we make the model too complex, generalization performance\n",
    "drops again, because we basically learn to memorize the dataset.\n",
    "\n",
    "\n",
    "\n",
    "## Overfitting and Underfitting\n",
    "\n",
    "![:scale 80%](images/overfitting_underfitting_cartoon_full.png)\n",
    "\n",
    "\n",
    "If we use too simple a model, this is often called underfitting, while\n",
    "if we use to complex a model, this is called overfitting. And somewhere\n",
    "in the middle is a sweet spot.\n",
    "Most models have some way to tune model complexity, and we’ll see many\n",
    "of them in the next couple of weeks.\n",
    "So going back to nearest neighbors, what parameters correspond to high\n",
    "model complexity and what to low model complexity? high n_neighbors =\n",
    "low complexity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/grid_search_k_neighbors.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not using Pipelines vs feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(seed=0)\n",
    "X = rnd.normal(size=(100, 10000))\n",
    "X_test = rnd.normal(size=(100, 10000))\n",
    "y = rnd.normal(size=(100,))\n",
    "y_test = rnd.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "\n",
    "select = SelectPercentile(score_func=f_regression,\n",
    "                          percentile=5)\n",
    "select.fit(X, y)\n",
    "X_selected = select.transform(X)\n",
    "print(X_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "np.mean(cross_val_score(Ridge(), X_selected, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge().fit(X_selected, y)\n",
    "X_test_selected = select.transform(X_test)\n",
    "ridge.score(X_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to house price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X, y = df, target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "ridge = Ridge().fit(X_train_scaled, y_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "ridge.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(StandardScaler(), Ridge())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(seed=0)\n",
    "X = rnd.normal(size=(100, 10000))\n",
    "X_test = rnd.normal(size=(100, 10000))\n",
    "y = rnd.normal(size=(100,))\n",
    "y_test = rnd.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([(\"select\", select),\n",
    "                 (\"ridge\", Ridge())])\n",
    "np.mean(cross_val_score(pipe, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X, y = df, target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline(((\"scaler\", StandardScaler()),\n",
    "                 (\"regressor\", KNeighborsRegressor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor())\n",
    "param_grid = {'kneighborsregressor__n_neighbors': range(1, 10)}\n",
    "grid = GridSearchCV(knn_pipe, param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    diabetes.data, diabetes.target, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(),\n",
    "    Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                    n_jobs=-1, return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('regressor', Ridge())])\n",
    "\n",
    "param_grid = {'scaler': [StandardScaler(), MinMaxScaler(), 'passthrough'],\n",
    "              'regressor': [Ridge(), Lasso()],\n",
    "              'regressor__alpha': np.logspace(-3, 3, 7)}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36901969445308325"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('regressor', Ridge())])\n",
    "\n",
    "param_grid = [{'regressor': [DecisionTreeRegressor()],\n",
    "               'regressor__max_depth': [2, 3, 4],\n",
    "               'scaler': ['passthrough']},\n",
    "              {'regressor': [Ridge()],\n",
    "               'regressor__alpha': [0.1, 1],\n",
    "               'scaler': [StandardScaler(), MinMaxScaler(), 'passthrough']}\n",
    "             ]\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
