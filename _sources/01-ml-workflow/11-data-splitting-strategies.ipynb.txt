{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting Strategies\n",
    "\n",
    "## Do importance of pipelines here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Strategies\n",
    "\n",
    "\n",
    "\n",
    "So I mentioned k-fold cross validation, where k is usually 5 or ten,\n",
    "but there are many other strategies.\n",
    "\n",
    "One of the most commonly ones is stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 90%](images/kfold_cv.png)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 90%](images/stratified_cv.png)\n",
    "]\n",
    ".smallest[\n",
    "Stratified:\n",
    "Ensure relative class frequencies in each fold reflect relative class\n",
    "frequencies on the whole dataset.]\n",
    "\n",
    "\n",
    "\n",
    "The idea behind stratified k-fold cross-validation is that you want the\n",
    "test set to be as representative of the dataset as possible.\n",
    "StratifiedKFold preserves the class frequencies in each fold to be the\n",
    "same as of the overall dataset.\n",
    "Here is and example of a dataset with three classes that are ordered. If\n",
    "you apply standard three-fold to this, the first third of the data would\n",
    "be in the first fold, the second in the second fold and the third in\n",
    "the third fold. Because this data is sorted, that would be particularly\n",
    "bad. If you use stratified cross-validation it would make sure that each\n",
    "fold has exactly 1/3 of the data from each class.\n",
    "\n",
    "This is also helpful if your data is very imbalanced. If some of the\n",
    "classes are very rare, it could otherwise happen that a class is not\n",
    "present at all in a particular fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Stratification\n",
    ".smaller[\n",
    "```python\n",
    "y.value_counts()\n",
    "```\n",
    "```\n",
    "0    60\n",
    "1    40\n",
    "```\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dc = DummyClassifier('most_frequent')\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "res = cross_val_score(dc, X, y, cv=skf)\n",
    "np.mean(res), res.std()\n",
    "```\n",
    "```\n",
    "(0.6, 0.0)\n",
    "```\n",
    "```python\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "res = cross_val_score(dc, X, y, cv=kf)\n",
    "np.mean(res), res.std()\n",
    "```\n",
    "```\n",
    "(0.6, 0.063)\n",
    "```\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated KFold and LeaveOneOut\n",
    "\n",
    "- LeaveOneOut : KFold(n_folds=n_samples) \n",
    "\n",
    "High variance, takes a long time \n",
    "\n",
    ".tiny[(see [Raschka](https://arxiv.org/pdf/1811.12808.pdf) for a review and [Varoquaux](https://hal.inria.fr/hal-01545002/file/paper.pdf) for empirical evaluation)]\n",
    "\n",
    "- Better: ShuffleSplit (aka Monte Carlo) \n",
    "\n",
    "Repeatedly sample a test set with replacement\n",
    "\n",
    "- Even Better: RepeatedKFold. \n",
    "\n",
    "Apply KFold or StratifiedKFold multiple times with shuffled data.\n",
    "\n",
    "\n",
    "\n",
    "If you want even better estimates of the generalization performance,\n",
    "you could try to increase the number of folds, with the extreme\n",
    "of creating one fold per sample. That’s called “LeaveOneOut\n",
    "cross-validation”. However, because the test-set is so small every time,\n",
    "and the training sets all have very large overlap, this method has very\n",
    "high variance.\n",
    "A better way to get a robust estimate is to run 5-fold or 10-fold\n",
    "cross-validation multiple times, while shuffling the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 100%](images/shuffle_split_cv.png)\n",
    "]\n",
    ".smaller[Number of iterations and test set size independent]\n",
    "\n",
    "\n",
    "Another interesting variant is shuffle split and stratified shuffle\n",
    "split. In shuffle split, we repeatedly sample disjoint training and test\n",
    "sets randomly.\n",
    "You only have to specify the number of iterations, the training set size\n",
    "and the test set size. This also allows you to run many iterations with\n",
    "reasonably large test-sets.\n",
    "It’s also great if you have a very large training set and you want to\n",
    "subsample it to get quicker results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".center[\n",
    "![:scale 100%](images/repeated_stratified_kfold.png)\n",
    "]\n",
    ".smaller[\n",
    "Potentially less variance than StratifiedShuffleSplit.\n",
    "\n",
    "Five times five fold or at most ten times ten fold is sufficient.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defaults in scikit-learn\n",
    "\n",
    "- 5-fold in 0.22 (used to be 3 fold)\n",
    "- For classification cross-validation is stratified\n",
    "- train_test_split has stratify option:\n",
    "train_test_split(X, y, stratify=y)\n",
    "\n",
    "- No shuffle by default!\n",
    "\n",
    "\n",
    "\n",
    "By default, all cross-validation strategies are five fold.\n",
    "If you do cross-validation for classification, it will be stratified\n",
    "by default.\n",
    "Because of how the interface is done, that’s not true for\n",
    "train_test_split and if you want a stratified train_test_split, which\n",
    "is always a good idea, you should use stratify=y\n",
    "Another thing that’s important to keep in mind is that by default\n",
    "scikit-learn doesn’t shuffle! So if you run cross-validation twice\n",
    "with the default parameters, it will yield exactly the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation with non-iid data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Data\n",
    "### Assume have data (medical, product, user...) from 5 cities\n",
    "- New York, San Francisco, Los Angeles, Chicago, Houston.\n",
    "\n",
    "We can assume data within a city is more correlated then between cities.\n",
    "\n",
    "### Usage Scenarios\n",
    "- Assume all future users will be in one of these cities: i.i.d.\n",
    "- Assume we want to generalize to predict for a new city: not i.i.d.\n",
    "\n",
    "\n",
    "\n",
    "Shipped product in 4 cities. Might ship in another one?\n",
    "States: you have all the states, no new state will start to exist\n",
    "\n",
    "Similar thing for multiple measurements per patient.\n",
    "Or geospacial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 100%](images/group_kfold.png)\n",
    "\n",
    "\n",
    "\n",
    "A somewhat more complicated approach is group k-fold.\n",
    "This is actually for data that doesn’t fulfill our IID assumption and\n",
    "has correlations between samples.\n",
    "The idea is that there are several groups in the data that each contain\n",
    "highly correlated samples.\n",
    "You could think about patient data where you have multiple samples for\n",
    "each patient, then the groups would be which patient a measurement was\n",
    "taken from.\n",
    "If you want to know how well your model generalizes to new patients,\n",
    "you need to ensure that the measurements from each patient are either\n",
    "all in the training set, or all in the test set.\n",
    "And that’s what GroupKFold does.\n",
    "In this example, there are four groups, and we want three folds. The\n",
    "data is divided such that each group is contained in exactly one fold.\n",
    "There are several other cross-validation methods in scikit-learn that\n",
    "use these groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations in time (and/or space)\n",
    "\n",
    "![:scale 70%](images/time_series1.png)\n",
    "\n",
    "\n",
    "\n",
    "Not necessarily obvious that there is a time component!\n",
    "Data collection usually happens over time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations in time (and/or space)\n",
    "\n",
    "![:scale 70%](images/time_series2.png)\n",
    "\n",
    "\n",
    "\n",
    "Not necessarily obvious that there is a time component!\n",
    "Data collection usually happens over time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations in time (and/or space)\n",
    "\n",
    "![:scale 70%](images/time_series3.png)\n",
    "\n",
    "\n",
    "\n",
    "Not necessarily obvious that there is a time component!\n",
    "Data collection usually happens over time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 100%](images/time_series_walk_forward_cv.png)\n",
    "\n",
    "\n",
    "Another common case of data that’s not independent is time\n",
    "series. Usually todays stock price is correlated with yesterdays and\n",
    "tomorrows. If you randomly split time series, this makes predictions\n",
    "deceivingly simple. In applications, you usually have data up to some\n",
    "point, and then try to make predictions for the future, in other words,\n",
    "you’re trying to make a forecast.\n",
    "The TimeSeriesSplit in scikit-learn simulates that, by taking increasing\n",
    "chunks of data from the past and making predictions on the next\n",
    "chunk. This is quite different from the other was to do cross-validation,\n",
    "in that the training sets are all overlapping, but it’s more appropriate\n",
    "for time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![:scale 100%](images/time_series_cv.png)\n",
    "\n",
    "\n",
    "\n",
    "Another common case of data that’s not independent is time\n",
    "series. Usually todays stock price is correlated with yesterdays and\n",
    "tomorrows. If you randomly split time series, this makes predictions\n",
    "deceivingly simple. In applications, you usually have data up to some\n",
    "point, and then try to make predictions for the future, in other words,\n",
    "you’re trying to make a forecast.\n",
    "The TimeSeriesSplit in scikit-learn simulates that, by taking increasing\n",
    "chunks of data from the past and making predictions on the next\n",
    "chunk. This is quite different from the other was to do cross-validation,\n",
    "in that the training sets are all overlapping, but it’s more appropriate\n",
    "for time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Cross-Validation Generators\n",
    "\n",
    ".tiny[\n",
    "```python\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit, RepeatedStratifiedKFold\n",
    "kfold = KFold(n_splits=5)\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "ss = ShuffleSplit(n_splits=20, train_size=.4, test_size=.3)\n",
    "rs = RepeatedStratifiedKFold(n_splits=5, n_repeats=10)\n",
    "\n",
    "print(\"KFold:\")\n",
    "print(cross_val_score(KNeighborsClassifier(), X, y, cv=kfold))\n",
    "\n",
    "print(\"StratifiedKFold:\")\n",
    "print(cross_val_score(KNeighborsClassifier(), X, y, cv=skfold))\n",
    "\n",
    "print(\"ShuffleSplit:\")\n",
    "print(cross_val_score(KNeighborsClassifier(), X, y, cv=ss))\n",
    "\n",
    "print(\"RepeatedStratifiedKFold:\")\n",
    "print(cross_val_score(KNeighborsClassifier(), X, y, cv=rs))\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "KFold:\n",
    "[0.93 0.96 0.96 0.98 0.96]\n",
    "StratifiedKFold:\n",
    "[0.98 0.96 0.96 0.97 0.96]\n",
    "ShuffleSplit:\n",
    "[0.98 0.96 0.96 0.98 0.94 0.96 0.95 0.98 0.97 0.92 0.94 0.97 0.95 0.92\n",
    " 0.98 0.98 0.97 0.94 0.97 0.95]\n",
    "RepeatedStratifiedKFold:\n",
    "[0.99 0.96 0.97 0.97 0.95 0.98 0.97 0.98 0.97 0.96 0.97 0.99 0.94 0.96\n",
    " 0.96 0.98 0.97 0.96 0.96 0.97 0.97 0.96 0.96 0.96 0.98 0.96 0.97 0.97\n",
    " 0.97 0.96 0.96 0.95 0.96 0.99 0.98 0.93 0.96 0.98 0.98 0.96 0.96 0.95\n",
    " 0.97 0.97 0.96 0.97 0.97 0.97 0.96 0.96]\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "Ok, so how do we use these cross-validation generators? We can simply\n",
    "pass the object to the cv parameter of the cross_val_score function,\n",
    "instead of passing a number. Then that generator will be used.\n",
    "Here are some examples for k-neighbors classifier.\n",
    "We instantiate a Kfold object with the number of splits equal to 5,\n",
    "and then pass it to cross_val_score.\n",
    "We can do the same with StratifiedKFold, and we can also shuffle if we\n",
    "like, or we can use Shuffle split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_validate function\n",
    ".smaller[\n",
    "```python\n",
    "from sklearn.model_selection import cross_validate\n",
    "res = cross_validate(KNeighborsClassifier(), X, y, return_train_score=True,\n",
    "                     scoring=[\"accuracy\", \"roc_auc\"])\n",
    "res_df = pd.DataFrame(res)\n",
    "```\n",
    "\n",
    "```\n",
    "fit_time\tscore_time\ttest_accuracy\ttest_roc_auc\ttrain_accuracy\ttrain_roc_auc\n",
    "0.000839\t0.010204    0.965217\t    0.996609\t    0.980176\t    0.997654\n",
    "0.000870\t0.014424    0.956522\t    0.983689\t    0.975771\t    0.998650\n",
    "0.000603\t0.009298    0.982301\t    0.999329\t    0.971491\t    0.996977\n",
    "0.000698\t0.006670    0.955752\t    0.984071\t    0.978070\t    0.997820\n",
    "0.000611\t0.006559    0.964602\t    0.994634\t    0.978070\t    0.998026\n",
    "```\n",
    "]\n",
    "\n",
    "\n",
    "FIXME alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Questions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "sklearn.set_config(print_changed_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(KNeighborsClassifier(),\n",
    "                X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(KNeighborsClassifier(),\n",
    "                X_train, y_train, cv=KFold(n_splits=10, shuffle=True, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(KNeighborsClassifier(),\n",
    "                X_train, y_train,\n",
    "                cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Searches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid-Search with build-in cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameter grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "param_grid = {'C': 10. ** np.arange(-3, 3),\n",
    "              'gamma' : 10. ** np.arange(-5, 0)}\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(SVC(), param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GridSearchCV object behaves just like a normal classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract just the scores\n",
    "\n",
    "scores = grid_search.cv_results_['mean_test_score']\n",
    "scores = np.array(scores).reshape(6, 5)\n",
    "\n",
    "plt.matshow(scores)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(5), param_grid['gamma'])\n",
    "plt.yticks(np.arange(6), param_grid['C']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Use GridSearchCV to adjust n_neighbors of KNeighborsClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Model complexity\n",
    "\n",
    "![:scale 75%](images/knn_model_complexity.png)\n",
    "\n",
    "\n",
    "We can look at this in more detail by comparing training and test set\n",
    "scores for the different numbers of neighbors.\n",
    "Here, I did a random 75%/25% split again. This is a very noisy plot as\n",
    "the dataset is very small and I only did a random split, but you can\n",
    "see a trend here.\n",
    "You can see that for a single neighbor, the training score is 1 so perfect\n",
    "accuracy, but the test score is only 70%.  If we increase the number of\n",
    "neighbors we consider, the training score goes down, but the test score\n",
    "goes up, with an optimum at 19 and 21, but then both go down again.\n",
    "\n",
    "This is a very typical behavior, that I sketched in a schematic for you.\n",
    "\n",
    "\n",
    "here is a cartoon version of how this chart looks in general, though\n",
    "it's horizontally flipped to the one with saw for knn.\n",
    "This chart has accuracy on the y axis, and the abstract concept of model\n",
    "complexity on the x axis.\n",
    "If we make our machine learning models more complex, we will get better\n",
    "training set accuracy, as the model will be able to capture more of the\n",
    "variations in the data.\n",
    "\n",
    "\n",
    "But if we look at the generalization performance, we get a different\n",
    "story. If the model complexity is too low, the model will not be able\n",
    "to capture the main trends, and a more complex model means better\n",
    "generalization.\n",
    "However, if we make the model too complex, generalization performance\n",
    "drops again, because we basically learn to memorize the dataset.\n",
    "\n",
    "\n",
    "\n",
    "## Overfitting and Underfitting\n",
    "\n",
    "![:scale 80%](images/overfitting_underfitting_cartoon_full.png)\n",
    "\n",
    "\n",
    "If we use too simple a model, this is often called underfitting, while\n",
    "if we use to complex a model, this is called overfitting. And somewhere\n",
    "in the middle is a sweet spot.\n",
    "Most models have some way to tune model complexity, and we’ll see many\n",
    "of them in the next couple of weeks.\n",
    "So going back to nearest neighbors, what parameters correspond to high\n",
    "model complexity and what to low model complexity? high n_neighbors =\n",
    "low complexity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/grid_search_k_neighbors.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not using Pipelines vs feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(seed=0)\n",
    "X = rnd.normal(size=(100, 10000))\n",
    "X_test = rnd.normal(size=(100, 10000))\n",
    "y = rnd.normal(size=(100,))\n",
    "y_test = rnd.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "\n",
    "select = SelectPercentile(score_func=f_regression,\n",
    "                          percentile=5)\n",
    "select.fit(X, y)\n",
    "X_selected = select.transform(X)\n",
    "print(X_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "np.mean(cross_val_score(Ridge(), X_selected, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge().fit(X_selected, y)\n",
    "X_test_selected = select.transform(X_test)\n",
    "ridge.score(X_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to house price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X, y = df, target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "ridge = Ridge().fit(X_train_scaled, y_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "ridge.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(StandardScaler(), Ridge())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(seed=0)\n",
    "X = rnd.normal(size=(100, 10000))\n",
    "X_test = rnd.normal(size=(100, 10000))\n",
    "y = rnd.normal(size=(100,))\n",
    "y_test = rnd.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([(\"select\", select),\n",
    "                 (\"ridge\", Ridge())])\n",
    "np.mean(cross_val_score(pipe, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X, y = df, target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline(((\"scaler\", StandardScaler()),\n",
    "                 (\"regressor\", KNeighborsRegressor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor())\n",
    "param_grid = {'kneighborsregressor__n_neighbors': range(1, 10)}\n",
    "grid = GridSearchCV(knn_pipe, param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    diabetes.data, diabetes.target, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(),\n",
    "    Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                    n_jobs=-1, return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('regressor', Ridge())])\n",
    "\n",
    "param_grid = {'scaler': [StandardScaler(), MinMaxScaler(), 'passthrough'],\n",
    "              'regressor': [Ridge(), Lasso()],\n",
    "              'regressor__alpha': np.logspace(-3, 3, 7)}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36901969445308325"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('regressor', Ridge())])\n",
    "\n",
    "param_grid = [{'regressor': [DecisionTreeRegressor()],\n",
    "               'regressor__max_depth': [2, 3, 4],\n",
    "               'scaler': ['passthrough']},\n",
    "              {'regressor': [Ridge()],\n",
    "               'regressor__alpha': [0.1, 1],\n",
    "               'scaler': [StandardScaler(), MinMaxScaler(), 'passthrough']}\n",
    "             ]\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
